-DOCSTART- -X- O
In -X- _ O
comparison -X- _ O
with -X- _ O
the -X- _ O
other -X- _ O
widely -X- _ O
used -X- _ O
strategies -X- _ O
for -X- _ O
selecting -X- _ O
important -X- _ O
tokens, -X- _ O
such -X- _ O
as -X- _ O
salinecy -X- _ O
and -X- _ O
attention, -X- _ O
our -X- _ O
CPs -X- _ O
have -X- _ O
significantly -X- _ O
less -X- _ O
false -X- _ B-MetricName
positive -X- _ I-MetricName
rate -X- _ I-MetricName
in -X- _ O
preserving -X- _ O
ratio -X- _ O

Figure -X- _ O
5 -X- _ O
shows -X- _ O
the -X- _ O
agreement -X- _ O
between -X- _ O
human -X- _ B-MethodName
rationales -X- _ I-MethodName
and -X- _ O
the -X- _ O
selected -X- _ O
tokens -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
two -X- _ O
metrics. -X- _ O

We -X- _ O
used -X- _ O
soft -X- _ O
scores -X- _ O
for -X- _ O
computing -X- _ O
AP -X- _ B-MetricName
and -X- _ O
binary -X- _ O
scores -X- _ O
for -X- _ O
computing -X- _ O
FPR. -X- _ B-MetricName

Whereas, -X- _ O
the -X- _ O
intuition -X- _ O
behind -X- _ O
the -X- _ O
second -X- _ O
metric -X- _ O
is -X- _ O
how -X- _ O
many -X- _ O
irrelevant -X- _ O
tokens -X- _ O
are -X- _ O
selected -X- _ O
by -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
be -X- _ O
passed -X- _ O
to -X- _ O
subsequent -X- _ O
layers. -X- _ O

The -X- _ O
first -X- _ O
metric -X- _ O
measures -X- _ O
whether -X- _ O
the -X- _ O
model -X- _ O
assigns -X- _ O
higher -X- _ O
continuous -X- _ O
scores -X- _ O
to -X- _ O
those -X- _ O
tokens -X- _ O
that -X- _ O
are -X- _ O
annotated -X- _ O
by -X- _ O
humans -X- _ B-MethodName
as -X- _ O
rationales. -X- _ O

As -X- _ O
for -X- _ O
evaluation, -X- _ O
we -X- _ O
used -X- _ O
the -X- _ O
Average -X- _ B-MetricName
Precision -X- _ I-MetricName
(AP) -X- _ O
and -X- _ O
False -X- _ B-MetricName
Positive -X- _ I-MetricName
Rate -X- _ I-MetricName
(FPR) -X- _ B-MetricName
metrics -X- _ O
by -X- _ O
comparing -X- _ O
the -X- _ O
remaining -X- _ O
tokens -X- _ O
to -X- _ O
the -X- _ O
human -X- _ B-MethodName
rationale -X- _ I-MethodName
annotations. -X- _ I-MethodName

Since -X- _ O
human -X- _ O
rationales -X- _ O
are -X- _ O
annotated -X- _ O
at -X- _ O
the -X- _ O
word -X- _ O
level, -X- _ O
we -X- _ O
sum -X- _ O
the -X- _ O
scores -X- _ O
across -X- _ O
tokens -X- _ O
corresponding -X- _ O
to -X- _ O
each -X- _ O
word -X- _ O
to -X- _ O
arrive -X- _ O
at -X- _ O
word-level -X- _ O
importance -X- _ O
scores. -X- _ O

We -X- _ O
also -X- _ O
fine-tuned -X- _ O
a -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
on -X- _ O
the -X- _ O
Movie -X- _ B-DatasetName
Review -X- _ I-DatasetName
dataset -X- _ O
and -X- _ O
computed -X- _ O
layer-wise -X- _ O
raw -X- _ O
attention, -X- _ O
attention -X- _ O
rollout, -X- _ O
and -X- _ O
saliency -X- _ O
scores -X- _ O
for -X- _ O
each -X- _ O
token -X- _ O
representation. -X- _ O

To -X- _ O
investigate -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
trained -X- _ O
CPs -X- _ O
in -X- _ O
predicting -X- _ O
human -X- _ O
rationales -X- _ O
we -X- _ O
computed -X- _ O
the -X- _ O
output -X- _ O
scores -X- _ O
of -X- _ O
CPs -X- _ O
in -X- _ O
AdapLeR -X- _ B-MethodName
for -X- _ O
each -X- _ O
token -X- _ O
representation -X- _ O
in -X- _ O
each -X- _ O
layer. -X- _ O

Figure -X- _ O
5: -X- _ O
Agreement -X- _ O
with -X- _ O
human -X- _ O
rationales -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
mean -X- _ O
Average -X- _ O
Precision -X- _ O
and -X- _ O
False -X- _ O
Positive -X- _ O
Rate -X- _ O
for -X- _ O
Contribution -X- _ O
Predictor -X- _ O
(CP) -X- _ O
and -X- _ O
three -X- _ O
alternative -X- _ O
techniques. -X- _ O

Using -X- _ O
our -X- _ O
proposed -X- _ O
method, -X- _ O
pre-trained -X- _ O
language -X- _ O
models -X- _ O
can -X- _ O
use -X- _ O
fewer -X- _ O
FLOPs, -X- _ B-MetricName
reducing -X- _ O
energy -X- _ O
use -X- _ O
and -X- _ O
carbon -X- _ O
emissions -X- _ O
(Schwartz -X- _ O
et -X- _ O
al., -X- _ O
2020a). -X- _ O

Additionally, -X- _ O
combining -X- _ O
our -X- _ O
width-based -X- _ O
strategy -X- _ O
with -X- _ O
a -X- _ O
depthbased -X- _ O
one -X- _ O
(e.g., -X- _ O
early -X- _ O
exiting) -X- _ O
might -X- _ O
potentially -X- _ O
yield -X- _ O
greater -X- _ O
efficiency, -X- _ O
something -X- _ O
we -X- _ O
plan -X- _ O
to -X- _ O
pursue -X- _ O
as -X- _ O
future -X- _ O
work. -X- _ O

As -X- _ O
future -X- _ O
work, -X- _ O
we -X- _ O
aim -X- _ O
to -X- _ O
apply -X- _ O
our -X- _ O
technique -X- _ O
to -X- _ O
more -X- _ O
tasks -X- _ O
and -X- _ O
see -X- _ O
whether -X- _ O
it -X- _ O
can -X- _ O
be -X- _ O
adapted -X- _ O
to -X- _ O
those -X- _ O
tasks -X- _ O
that -X- _ O
require -X- _ O
all -X- _ O
token -X- _ O
representations -X- _ O
to -X- _ O
be -X- _ O
present -X- _ O
in -X- _ O
the -X- _ O
final -X- _ O
layer -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
(e.g., -X- _ O
question -X- _ O
answering). -X- _ O

Furthermore, -X- _ O
we -X- _ O
demonstrated -X- _ O
that -X- _ O
contribution -X- _ O
predictors -X- _ O
generate -X- _ O
rationales -X- _ O
that -X- _ O
are -X- _ O
highly -X- _ O
in -X- _ O
line -X- _ O
with -X- _ O
those -X- _ O
manually -X- _ O
specified -X- _ O
by -X- _ O
humans. -X- _ O

Empirical -X- _ O
results -X- _ O
on -X- _ O
eight -X- _ O
diverse -X- _ O
text -X- _ O
classification -X- _ O
tasks -X- _ O
show -X- _ O
considerable -X- _ O
improvements -X- _ O
over -X- _ O
existing -X- _ O
methods. -X- _ O

Specifically, -X- _ O
AdapLeR -X- _ B-MethodName
accomplishes -X- _ O
this -X- _ O
by -X- _ O
training -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
Contribution -X- _ O
Predictors -X- _ O
based -X- _ O
on -X- _ O
saliencies -X- _ O
extracted -X- _ O
from -X- _ O
a -X- _ O
fine-tuned -X- _ O
model -X- _ O
and -X- _ O
applying -X- _ O
a -X- _ O
gradual -X- _ O
masking -X- _ O
technique -X- _ O
to -X- _ O
simulate -X- _ O
input-adaptive -X- _ O
token -X- _ O
removal -X- _ O
during -X- _ O
training. -X- _ O

To -X- _ O
assign -X- _ O
an -X- _ O
importance -X- _ O
score -X- _ O
to -X- _ O
each -X- _ O
token, -X- _ O
we -X- _ O
examined -X- _ O
two -X- _ O
different -X- _ O
interpretation -X- _ O
of -X- _ O
attention -X- _ O
weights. -X- _ O

For -X- _ O
a -X- _ O
fair -X- _ O
comparison, -X- _ O
for -X- _ O
each -X- _ O
sentence -X- _ O
in -X- _ O
the -X- _ O
test -X- _ O
set, -X- _ O
we -X- _ O
selected -X- _ O
the -X- _ O
top-k -X- _ O
salient -X- _ O
and -X- _ O
attended -X- _ O
words, -X- _ O
with -X- _ O
k -X- _ O
being -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
words -X- _ O
that -X- _ O
are -X- _ O
annotated -X- _ O
as -X- _ O
rationales. -X- _ O

Results -X- _ O
in -X- _ O
Table -X- _ O
2 -X- _ O
show -X- _ O
that -X- _ O
fine-tuning -X- _ O
on -X- _ O
the -X- _ O
most -X- _ O
salient -X- _ O
tokens -X- _ O
outperforms -X- _ O
that -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
most -X- _ O
attended -X- _ O
tokens. -X- _ O

This -X- _ O
denotes -X- _ O
that -X- _ O
saliency -X- _ O
is -X- _ O
a -X- _ O
better -X- _ O
indicator -X- _ O
for -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
tokens. -X- _ O

Note -X- _ O
that -X- _ O
in -X- _ O
our -X- _ O
approach, -X- _ O
saliency -X- _ O
scores -X- _ O
are -X- _ O
easily -X- _ O
estimated -X- _ O
within -X- _ O
inference -X- _ O
time -X- _ O
by -X- _ O
the -X- _ O
pre-trained -X- _ O
CPs. -X- _ O

In -X- _ O
this -X- _ O
paper, -X- _ O
we -X- _ O
introduced -X- _ O
AdapLeR, -X- _ B-MethodName
a -X- _ O
novel -X- _ O
method -X- _ O
that -X- _ O
accelerates -X- _ O
inference -X- _ O
by -X- _ O
dynamically -X- _ O
identifying -X- _ O
and -X- _ O
dropping -X- _ O
less -X- _ O
contributing -X- _ O
token -X- _ O
representations -X- _ O
through -X- _ O
layers -X- _ O
of -X- _ O
BERT-based -X- _ O
models. -X- _ O

As -X- _ O
a -X- _ O
by-product -X- _ O
of -X- _ O
the -X- _ O
contribution -X- _ O
estimation -X- _ O
process, -X- _ O
our -X- _ O
trained -X- _ O
CPs -X- _ O
are -X- _ O
able -X- _ O
to -X- _ O
generate -X- _ O
these -X- _ O
rationales -X- _ O
at -X- _ O
inference -X- _ O
without -X- _ O
the -X- _ O
need -X- _ O
for -X- _ O
human-generated -X- _ O
annotations. -X- _ O

to -X- _ O
which -X- _ O
to -X- _ O
quantify -X- _ O
the -X- _ O
extent -X- _ O
AdapLeR’s -X- _ B-MethodName
CPs -X- _ O
can -X- _ O
preserve -X- _ O
rationales -X- _ O
without -X- _ O
requiring -X- _ O
direct -X- _ O
human -X- _ O
annotations -X- _ O
in -X- _ O
an -X- _ O
unsuper -X- _ O

Figure -X- _ O
4 -X- _ O
illustrates -X- _ O
two -X- _ O
examples -X- _ O
from -X- _ O
the -X- _ O
SST-2 -X- _ B-DatasetName
and -X- _ O
QNLI -X- _ B-DatasetName
datasets -X- _ O
in -X- _ O
which -X- _ O
CPs -X- _ O
identify -X- _ O
and -X- _ O
gradually -X- _ O
drop -X- _ O
the -X- _ O
irrelevant -X- _ O
tokens -X- _ O
through -X- _ O
layers, -X- _ O
finally -X- _ O
focusing -X- _ O
mostly -X- _ O
on -X- _ O
the -X- _ O
most -X- _ O
important -X- _ O
token -X- _ O
representations; -X- _ O
pedestrian -X- _ O
(adjective) -X- _ O
in -X- _ O
SST-2 -X- _ B-DatasetName
and -X- _ O
tesla -X- _ O
coil -X- _ O
in -X- _ O
the -X- _ O
passage -X- _ O
part -X- _ O
of -X- _ O
QNLI -X- _ B-DatasetName
(both -X- _ O
of -X- _ O
which -X- _ O
are -X- _ O
highly -X- _ O
aligned -X- _ O
with -X- _ O
human -X- _ O
rationale). -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
we -X- _ O
validate -X- _ O
our -X- _ O
Contribution -X- _ O
Predictors -X- _ O
in -X- _ O
selecting -X- _ O
the -X- _ O
most -X- _ O
contributed -X- _ O
tokens. -X- _ O

Computing -X- _ O
these -X- _ O
weights -X- _ O
is -X- _ O
convenient -X- _ O
as -X- _ O
they -X- _ O
are -X- _ O
already -X- _ O
computed -X- _ O
during -X- _ O
the -X- _ O
forward -X- _ O
pass, -X- _ O
whereas -X- _ O
computing -X- _ O
saliency -X- _ O
requires -X- _ O
an -X- _ O
additional -X- _ O
backpropagation -X- _ O
step. -X- _ O

Nonetheless, -X- _ O
recent -X- _ O
length -X- _ O
reduction -X- _ O
techniques -X- _ O
(Goyal -X- _ O
et -X- _ O
al., -X- _ O
2020; -X- _ O
Kim -X- _ O
and -X- _ O
Cho, -X- _ O
2021; -X- _ O
Wang -X- _ O
et -X- _ O
al., -X- _ O
2021) -X- _ O
have -X- _ O
mostly -X- _ O
adopted -X- _ O
attention -X- _ O
weights -X- _ O
as -X- _ O
their -X- _ O
criterion -X- _ O
for -X- _ O
selecting -X- _ O
important -X- _ O
tokens. -X- _ O

Additionally, -X- _ O
we -X- _ O
measured -X- _ O
how -X- _ O
much -X- _ O
the -X- _ O
[CLS] -X- _ O
token -X- _ O
attends -X- _ O
to -X- _ O
each -X- _ O
token -X- _ O
in -X- _ O
the -X- _ O
sentence, -X- _ O
a -X- _ O
strategy -X- _ O
which -X- _ O
has -X- _ O
been -X- _ O
widely -X- _ O
used -X- _ O
in -X- _ O
interpretability -X- _ O
studies -X- _ O
around -X- _ O
BERT -X- _ B-MethodName
(Abnar -X- _ O
and -X- _ O
Zuidema, -X- _ O
2020; -X- _ O
Chrysostomou -X- _ O
and -X- _ O
Aletras, -X- _ O
2021; -X- _ O
Jain -X- _ O
et -X- _ O
al., -X- _ O
2020, -X- _ O
inter -X- _ O
alia). -X- _ O

The -X- _ O
first -X- _ O
strategy -X- _ O
is -X- _ O
the -X- _ O
one -X- _ O
adopted -X- _ O
by -X- _ O
PoWER-BERT -X- _ B-MethodName
(Goyal -X- _ O
et -X- _ O
al., -X- _ O
2020) -X- _ O
in -X- _ O
which -X- _ O
for -X- _ O
each -X- _ O
token -X- _ O
we -X- _ O
accumulate -X- _ O
attention -X- _ O
values -X- _ O
from -X- _ O

Similarly, -X- _ O
we -X- _ O
aggregated -X- _ O
the -X- _ O
selfattention -X- _ O
weights -X- _ O
across -X- _ O
all -X- _ O
layers -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
using -X- _ O
a -X- _ O
post-processed -X- _ O
variant -X- _ O
of -X- _ O
attentions -X- _ O
called -X- _ O
attention -X- _ O
rollout -X- _ O
(Abnar -X- _ O
and -X- _ O
Zuidema, -X- _ O
2020), -X- _ O
a -X- _ O
popular -X- _ O
technique -X- _ O
in -X- _ O
which -X- _ O
the -X- _ O
attention -X- _ O
weight -X- _ O
matrix -X- _ O
in -X- _ O
each -X- _ O
layer -X- _ O
is -X- _ O
multiplied -X- _ O
with -X- _ O
the -X- _ O
preceding -X- _ O
ones -X- _ O
to -X- _ O
form -X- _ O
aggregated -X- _ O
attention -X- _ O
values. -X- _ O

Secondly, -X- _ O
the -X- _ O
backpropagation -X- _ O
of -X- _ O
gradients -X- _ O
through -X- _ O
layers -X- _ O
to -X- _ O
the -X- _ O
beginning -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
provides -X- _ O
us -X- _ O
with -X- _ O
aggregated -X- _ O
values -X- _ O
for -X- _ O
the -X- _ O
relative -X- _ O
importance -X- _ O
of -X- _ O
each -X- _ O
token -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
entire -X- _ O
model. -X- _ O

Firstly, -X- _ O
representations -X- _ O
in -X- _ O
the -X- _ O
embedding -X- _ O
layer -X- _ O
are -X- _ O
non-contextualized, -X- _ O
allowing -X- _ O
us -X- _ O
to -X- _ O
measure -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
each -X- _ O
token -X- _ O
independently -X- _ O
from -X- _ O
the -X- _ O
others. -X- _ O

This -X- _ O
brings -X- _ O
about -X- _ O
two -X- _ O
advantages. -X- _ O

We -X- _ O
then -X- _ O
obtained -X- _ O
saliency -X- _ B-MetricName
scores -X- _ I-MetricName
with -X- _ O
respect -X- _ O
to -X- _ O
the -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
embedding -X- _ O
layer. -X- _ O

To -X- _ O
compute -X- _ O
these, -X- _ O
we -X- _ O
first -X- _ O
fine-tuned -X- _ O
BERT -X- _ B-MethodName
with -X- _ O
all -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
for -X- _ O
a -X- _ O
given -X- _ O
target -X- _ O
task. -X- _ O

We -X- _ O
investigated -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
saliency -X- _ O
and -X- _ O
self-attention -X- _ O
weights -X- _ O
as -X- _ O
two -X- _ O
commonly -X- _ O
used -X- _ O
strategies -X- _ O
for -X- _ O
measuring -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
tokens -X- _ O
in -X- _ O
pre-trained -X- _ O
language -X- _ O
models. -X- _ O

Obviously, -X- _ O
human -X- _ O
annotations -X- _ O
are -X- _ O
not -X- _ O
available -X- _ O
for -X- _ O
a -X- _ O
whole -X- _ O
range -X- _ O
of -X- _ O
downstream -X- _ O
NLP -X- _ O
tasks; -X- _ O
therefore, -X- _ O
this -X- _ O
criterion -X- _ O
is -X- _ O
infeasible -X- _ O
in -X- _ O
practice -X- _ O
and -X- _ O
can -X- _ O
only -X- _ O
be -X- _ O
viewed -X- _ O
as -X- _ O
an -X- _ O
upper -X- _ O
bound -X- _ O
for -X- _ O
evaluating -X- _ O
different -X- _ O
strategies -X- _ O
in -X- _ O
measuring -X- _ O
token -X- _ O
importance. -X- _ O

See -X- _ O
more -X- _ O
full-layer -X- _ O
plots -X- _ O
in -X- _ O
the -X- _ O
appendix -X- _ O
6. -X- _ O

Only -X- _ O
the -X- _ O
highlighted -X- _ O
token -X- _ O
representations -X- _ O
are -X- _ O
processed -X- _ O
in -X- _ O
each -X- _ O
layer. -X- _ O

This -X- _ O
can -X- _ O
be -X- _ O
attributed -X- _ O
to -X- _ O
the -X- _ O
input-adaptive -X- _ O
procedure -X- _ O
employed -X- _ O
by -X- _ O
the -X- _ O
former -X- _ O
two -X- _ O
methods -X- _ O
for -X- _ O
determining -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
reduced -X- _ O
tokens -X- _ O
(whereas -X- _ O
PoWER-BERT -X- _ B-MethodName
adopts -X- _ O
a -X- _ O
fixed -X- _ O
retention -X- _ O
configuration -X- _ O
in -X- _ O
token -X- _ O
elimination). -X- _ O

An -X- _ O
interesting -X- _ O
observation -X- _ O
here -X- _ O
is -X- _ O
that -X- _ O
the -X- _ O
curves -X- _ O
for -X- _ O
TR-BERT -X- _ B-MethodName
and -X- _ O
AdapLeR -X- _ B-MethodName
are -X- _ O
much -X- _ O
higher -X- _ O
than -X- _ O
that -X- _ O
of -X- _ O
PoWER-BERT. -X- _ B-MethodName

As -X- _ O
we -X- _ O
can -X- _ O
see, -X- _ O
AdapLeR -X- _ B-MethodName
significantly -X- _ O
outperforms -X- _ O
the -X- _ O
other -X- _ O
two -X- _ O
approaches -X- _ O
in -X- _ O
all -X- _ O
two -X- _ O
tasks. -X- _ O

The -X- _ O
bold -X- _ O
values -X- _ O
indicate -X- _ O
the -X- _ O
best -X- _ O
corresponding -X- _ O
strategy -X- _ O
for -X- _ O
each -X- _ O
task -X- _ O
(the -X- _ O
closest -X- _ O
performance -X- _ O
to -X- _ O
the -X- _ O
upper -X- _ O
bound). -X- _ O

Table -X- _ O
2: -X- _ O
Accuracy -X- _ B-MetricName
and -X- _ O
speedup -X- _ B-MetricName
when -X- _ O
the -X- _ O
most -X- _ O
important -X- _ O
input -X- _ O
tokens -X- _ O
during -X- _ O
fine-tuning -X- _ O
are -X- _ O
computed -X- _ O
based -X- _ O
on -X- _ O
attention -X- _ O
and -X- _ O
saliency -X- _ O
strategies -X- _ O
and -X- _ O
human -X- _ O
rationale -X- _ O
(the -X- _ O
upper -X- _ O
bound). -X- _ O

PoWER-BERT -X- _ B-MethodName
adopts -X- _ O
attention -X- _ O
weights -X- _ O
in -X- _ O
its -X- _ O
token -X- _ O
selection -X- _ O
which -X- _ O
requires -X- _ O
at -X- _ O
least -X- _ O
one -X- _ O
layer -X- _ O
of -X- _ O
computation -X- _ O
to -X- _ O
be -X- _ O
determined, -X- _ O
and -X- _ O
TR-BERT -X- _ B-MethodName
ap -X- _ O

For -X- _ O
instance, -X- _ O
in -X- _ O
AG’s -X- _ B-DatasetName
News, -X- _ I-DatasetName
the -X- _ O
topic -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
might -X- _ O
be -X- _ O
identifiable -X- _ O
with -X- _ O
a -X- _ O
single -X- _ O
token -X- _ O
(e.g., -X- _ O
soccer -X- _ O
→ -X- _ O
Topic: -X- _ O
Sports, -X- _ O
see -X- _ O
Figure -X- _ O
6 -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
for -X- _ O
an -X- _ O
example). -X- _ O

Some -X- _ O
tasks -X- _ O
may -X- _ O
need -X- _ O
less -X- _ O
amount -X- _ O
of -X- _ O
contextualism -X- _ O
during -X- _ O
inference -X- _ O
and -X- _ O
could -X- _ O
be -X- _ O
classified -X- _ O
by -X- _ O
using -X- _ O
only -X- _ O
a -X- _ O
fraction -X- _ O
of -X- _ O
input -X- _ O
tokens. -X- _ O

It -X- _ O
is -X- _ O
noteworthy -X- _ O
that -X- _ O
the -X- _ O
results -X- _ O
also -X- _ O
reveal -X- _ O
some -X- _ O
form -X- _ O
of -X- _ O
dependency -X- _ O
on -X- _ O
the -X- _ O
type -X- _ O
of -X- _ O
the -X- _ O
tasks. -X- _ O

While -X- _ O
preserving -X- _ O
the -X- _ O
same -X- _ O
level -X- _ O
of -X- _ O
performance, -X- _ O
AdapLeR -X- _ B-MethodName
outperforms -X- _ O
other -X- _ O
techniques -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
speedup -X- _ B-MetricValue
across -X- _ O
all -X- _ O
tasks -X- _ O
(ranging -X- _ O
from -X- _ O
+0.2x -X- _ B-MetricValue
to -X- _ O
+7.4x -X- _ B-MetricValue
compared -X- _ O
to -X- _ O
the -X- _ O
best -X- _ O
model -X- _ O
in -X- _ O
each -X- _ O
dataset). -X- _ O

Table -X- _ O
1 -X- _ O
shows -X- _ O
performance -X- _ O
and -X- _ O
speedup -X- _ O
for -X- _ O
AdapLeR -X- _ B-MethodName
and -X- _ O
other -X- _ O
comparison -X- _ O
models -X- _ O
across -X- _ O
eight -X- _ O
different -X- _ O
datasets. -X- _ O

To -X- _ O
have -X- _ O
a -X- _ O
fair -X- _ O
comparison, -X- _ O
we -X- _ O
also -X- _ O
computed -X- _ O
FLOPs -X- _ B-MetricName
for -X- _ O
PoWERBERT -X- _ B-MethodName
in -X- _ O
a -X- _ O
single -X- _ O
instance -X- _ O
mode, -X- _ O
described -X- _ O
in -X- _ O
Section -X- _ O
C. -X- _ O

Then, -X- _ O
a -X- _ O
model’s -X- _ O
speedup -X- _ O
can -X- _ O
be -X- _ O
defined -X- _ O
as -X- _ O
the -X- _ O
total -X- _ B-MetricName
FLOPs -X- _ I-MetricName
measured -X- _ O
on -X- _ O
BERT -X- _ B-MethodName
(our -X- _ O
baseline) -X- _ O
divided -X- _ O
by -X- _ O
the -X- _ O
corresponding -X- _ O
model’s -X- _ O
total -X- _ B-MetricName
FLOPs. -X- _ I-MetricName

The -X- _ O
total -X- _ O
FLOPs -X- _ B-MetricName
of -X- _ O
a -X- _ O
given -X- _ O
model -X- _ O
is -X- _ O
a -X- _ O
summation -X- _ O
of -X- _ O
the -X- _ O
measured -X- _ O
FLOPs -X- _ B-MetricName
over -X- _ O
all -X- _ O
test -X- _ O
examples. -X- _ O

5Note -X- _ O
that -X- _ O
θ -X- _ O
and -X- _ O
η -X- _ O
are -X- _ O
trainable -X- _ O
terms -X- _ O
that -X- _ O
are -X- _ O
tuned -X- _ O
by -X- _ O
the -X- _ O

4Since -X- _ O
some -X- _ O
of -X- _ O
the -X- _ O
datasets -X- _ O
were -X- _ O
not -X- _ O
used -X- _ O
originally, -X- _ O
we -X- _ O
had -X- _ O
to -X- _ O
search -X- _ O
the -X- _ O
hyperparameters -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
given -X- _ O
ranges. -X- _ O

Ye -X- _ O
et -X- _ O
al., -X- _ O
2021) -X- _ O
which -X- _ O
usually -X- _ O
have -X- _ O
two -X- _ O
or -X- _ O
three -X- _ O
hyperparameters -X- _ O
adjusted -X- _ O
per -X- _ O
task. -X- _ O

This -X- _ O
makes -X- _ O
our -X- _ O
approach -X- _ O
comparable -X- _ O
to -X- _ O
existing -X- _ O
techniques -X- _ O
(Goyal -X- _ O
et -X- _ O
al., -X- _ O
2020; -X- _ O

Among -X- _ O
these, -X- _ O
ϕ -X- _ B-HyperparameterName
and -X- _ O
γ -X- _ B-HyperparameterName
are -X- _ O
the -X- _ O
primary -X- _ O
terms -X- _ O
that -X- _ O
have -X- _ O
considerable -X- _ O
effects -X- _ O
on -X- _ O
AdapLeR’s -X- _ B-MethodName
downstream -X- _ O
performance -X- _ O
and -X- _ O
speedup. -X- _ O

Overall, -X- _ O
we -X- _ O
introduced -X- _ O
four -X- _ O
hyperparameters -X- _ O
(γ, -X- _ B-HyperparameterName
ϕ, -X- _ B-HyperparameterName
λ, -X- _ B-HyperparameterName
β)5 -X- _ B-HyperparameterName
which -X- _ O
are -X- _ O
involved -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
process. -X- _ O

Trainings -X- _ O
and -X- _ O
evaluations -X- _ O
were -X- _ O
conducted -X- _ O
on -X- _ O
a -X- _ O
dual -X- _ O
2080Ti -X- _ O
11GB -X- _ O
GPU -X- _ O
machine -X- _ O
with -X- _ O
multiple -X- _ O
runs. -X- _ O

The -X- _ O
backbone -X- _ O
model -X- _ O
and -X- _ O
our -X- _ O
model -X- _ O
implementation -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
HuggingFace’s -X- _ O
Transformers -X- _ O
library -X- _ O
(Wolf -X- _ O
et -X- _ O
al., -X- _ O
2020). -X- _ O

To -X- _ O
fine-tune -X- _ O
the -X- _ O
backbone -X- _ O
model, -X- _ O
we -X- _ O
used -X- _ O
same -X- _ O
hyperparameters -X- _ O
over -X- _ O
all -X- _ O
tasks -X- _ O
(see -X- _ O
Section -X- _ O
D -X- _ O
for -X- _ O
details). -X- _ O

We -X- _ O
used -X- _ O
the -X- _ O
provided -X- _ O
implementations -X- _ O
and -X- _ O
suggested -X- _ O
hyperparameters4 -X- _ O
to -X- _ O
train -X- _ O
these -X- _ O
baselines. -X- _ O

We -X- _ O
also -X- _ O
compare -X- _ O
against -X- _ O
three -X- _ O
other -X- _ O
approaches: -X- _ O
DistilBERT -X- _ B-MethodName
(uncased) -X- _ I-MethodName
(Sanh -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
as -X- _ O
a -X- _ O
static -X- _ O
compression -X- _ O
method, -X- _ O
PoWER-BERT -X- _ B-MethodName
and -X- _ O
TR-BERT -X- _ B-MethodName
as -X- _ O
two -X- _ O
strong -X- _ O
length -X- _ O
reduction -X- _ O
methods -X- _ O
(cf. -X- _ O
Sec. -X- _ O
1). -X- _ O

As -X- _ O
our -X- _ O
baseline, -X- _ O
we -X- _ O
report -X- _ O
results -X- _ O
for -X- _ O
the -X- _ O
pretrained -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
(base-uncased) -X- _ O
(Devlin -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
which -X- _ O
is -X- _ O
also -X- _ O
the -X- _ O
backbone -X- _ O
of -X- _ O
AdapLeR. -X- _ B-MethodName

In -X- _ O
the -X- _ O
MNLI -X- _ B-TaskName
task, -X- _ O
the -X- _ O
speedup -X- _ O
and -X- _ O
performance -X- _ O
values -X- _ O
are -X- _ O
the -X- _ O
average -X- _ O
of -X- _ O
the -X- _ O
evaluations -X- _ O
on -X- _ O
the -X- _ O
matched -X- _ O
and -X- _ O
mismatched -X- _ O
test -X- _ O
sets. -X- _ O

For -X- _ O
each -X- _ O
dataset -X- _ O
the -X- _ O
corresponding -X- _ O
metric -X- _ O
has -X- _ O
been -X- _ O
reported -X- _ O
(Accuracy: -X- _ B-MetricName
Acc., -X- _ B-MetricName
F1: -X- _ B-MetricName
F-1 -X- _ B-MetricName
Score). -X- _ I-MetricName

Table -X- _ O
1: -X- _ O
Comparison -X- _ O
of -X- _ O
our -X- _ O
proposed -X- _ O
method -X- _ O
(AdapLeR) -X- _ B-MethodName
with -X- _ O
other -X- _ O
baselines -X- _ O
in -X- _ O
eight -X- _ O
classification -X- _ O
tasks -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
performance -X- _ B-MetricName
and -X- _ O
speedup. -X- _ B-MetricName

In -X- _ O
order -X- _ O
to -X- _ O
incorporate -X- _ O
a -X- _ O
variety -X- _ O
of -X- _ O
tasks, -X- _ O
we -X- _ O
utilized -X- _ O
SST-2 -X- _ B-DatasetName
(Socher -X- _ O
et -X- _ O
al., -X- _ O
2013) -X- _ O
and -X- _ O
IMDB -X- _ B-DatasetName
(Maas -X- _ O
et -X- _ O
al., -X- _ O
2011) -X- _ O
for -X- _ O
sentiment, -X- _ B-TaskName
MRPC -X- _ B-DatasetName
(Dolan -X- _ O
and -X- _ O
Brockett, -X- _ O
2005) -X- _ O
for -X- _ O
paraphrase, -X- _ B-TaskName
AG’s -X- _ B-DatasetName
News -X- _ I-DatasetName
(Zhang -X- _ O
et -X- _ O
al., -X- _ O
2015) -X- _ O
for -X- _ O
topic -X- _ B-TaskName
classification, -X- _ I-TaskName
DBpedia -X- _ B-DatasetName
(Lehmann -X- _ O
et -X- _ O
al., -X- _ O
2015) -X- _ O
for -X- _ O
knowledge -X- _ B-TaskName
extraction, -X- _ I-TaskName
MNLI -X- _ B-DatasetName
(Williams -X- _ O
et -X- _ O
al., -X- _ O
2018) -X- _ O
for -X- _ O
NLI, -X- _ B-TaskName

To -X- _ O
verify -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
AdapLeR -X- _ B-MethodName
on -X- _ O
inference -X- _ B-TaskName
speedup, -X- _ I-TaskName
we -X- _ O
selected -X- _ O
eight -X- _ O
various -X- _ O
text -X- _ O
classification -X- _ O
datasets. -X- _ O

So -X- _ O
in -X- _ O
other -X- _ O
words, -X- _ O
the -X- _ O
first -X- _ O
objective -X- _ O
is -X- _ O
to -X- _ O
solve -X- _ O
the -X- _ O
task -X- _ O
and -X- _ O
make -X- _ O
it -X- _ O
explainable -X- _ O
with -X- _ O
the -X- _ O
CPs, -X- _ O
and -X- _ O
the -X- _ O
secondary -X- _ O
objective -X- _ O
builds -X- _ O
the -X- _ O
speedup -X- _ O
using -X- _ O
tuning -X- _ O
the -X- _ O
threshold -X- _ O
levels -X- _ O
and -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
pooling -X- _ O
in -X- _ O
each -X- _ O
layer. -X- _ O

The -X- _ O
reason -X- _ O
that -X- _ O
this -X- _ O
objective -X- _ O
is -X- _ O
treated -X- _ O
as -X- _ O
a -X- _ O
separate -X- _ O
problem -X- _ O
instead -X- _ O
of -X- _ O
merging -X- _ O
it -X- _ O
with -X- _ O
the -X- _ O
previous -X- _ O
one, -X- _ O
is -X- _ O
because -X- _ O
in -X- _ O
the -X- _ O
latter -X- _ O
case -X- _ O
the -X- _ O
CPs -X- _ O
could -X- _ O
be -X- _ O
influenced -X- _ O
by -X- _ O
the -X- _ O
length -X- _ O
loss -X- _ O
and -X- _ O
try -X- _ O
to -X- _ O
manipulate -X- _ O
the -X- _ O
contribution -X- _ O
scores -X- _ O
for -X- _ O
some -X- _ O
tokens -X- _ O
regardless -X- _ O
of -X- _ O
their -X- _ O
real -X- _ O
influence. -X- _ O

During -X- _ O
training, -X- _ O
we -X- _ O
assign -X- _ O
a -X- _ O
separate -X- _ O
optimization -X- _ O
process -X- _ O
which -X- _ O
tunes -X- _ O
η -X- _ O
and -X- _ O
θ -X- _ O
to -X- _ O
adjust -X- _ O
the -X- _ O
thresholds -X- _ O
and -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
[CLS] -X- _ O
pooling2 -X- _ O
alongside -X- _ O
with -X- _ O
the -X- _ O
CP -X- _ O
training. -X- _ O

Considering -X- _ O
that -X- _ O
we -X- _ O
have -X- _ O
a -X- _ O
non-positive -X- _ O
and -X- _ O
continuous -X- _ O
attention -X- _ O
mask -X- _ O
M -X- _ O
, -X- _ O
the -X- _ O
length -X- _ O
loss -X- _ O
of -X- _ O
a -X- _ O
single -X- _ O
layer -X- _ O
would -X- _ O
be -X- _ O
the -X- _ O
summation -X- _ O
over -X- _ O
the -X- _ O
exponential -X- _ O
of -X- _ O
the -X- _ O
mask -X- _ O
values -X- _ O
exp(mi) -X- _ O
to -X- _ O
map -X- _ O
the -X- _ O
masking -X- _ O
range -X- _ O
[−∞, -X- _ O
0] -X- _ O
to -X- _ O
a -X- _ O
[0 -X- _ O
(fully -X- _ O
masked/removed), -X- _ O
1 -X- _ O
(fully -X- _ O
retained)] -X- _ O
bound. -X- _ O

In -X- _ O
the -X- _ O
speedup -X- _ O
tuning -X- _ O
process, -X- _ O
we -X- _ O
combine -X- _ O
the -X- _ O
cross-entropy -X- _ O
loss -X- _ O
of -X- _ O
the -X- _ O
target -X- _ B-TaskName
classification -X- _ I-TaskName
task -X- _ O
with -X- _ O
a -X- _ O
length -X- _ O
loss -X- _ O
which -X- _ O
is -X- _ O
the -X- _ O
expected -X- _ O
number -X- _ O
of -X- _ O
unmasked -X- _ O
token -X- _ O
representations -X- _ O
in -X- _ O
all -X- _ O
layers. -X- _ O

CPs -X- _ O
to -X- _ O
shift -X- _ O
the -X- _ O
contribution -X- _ O
towards -X- _ O
the -X- _ O
[CLS] -X- _ O
token -X- _ O
to -X- _ O
gather -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
task-specific -X- _ O
information -X- _ O
and -X- _ O
avoids -X- _ O
carrying -X- _ O
redundant -X- _ O
tokens -X- _ O
through -X- _ O
the -X- _ O
model. -X- _ O

As -X- _ O
we -X- _ O
leverage -X- _ O
the -X- _ O
representation -X- _ O
of -X- _ O
the -X- _ O
[CLS] -X- _ O
token -X- _ O
in -X- _ O
the -X- _ O
last -X- _ O
layer -X- _ O
for -X- _ O
classification, -X- _ O
this -X- _ O
token -X- _ O
acts -X- _ O
as -X- _ O
a -X- _ O
pooler -X- _ O
and -X- _ O
gathers -X- _ O
information -X- _ O
about -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
the -X- _ O
input. -X- _ O

Since -X- _ O
S -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
input -X- _ O
embeddings, -X- _ O
the -X- _ O
[CLS] -X- _ O
token -X- _ O
usually -X- _ O
shows -X- _ O
a -X- _ O
low -X- _ O
amount -X- _ O
of -X- _ O
contribution -X- _ O
due -X- _ O
to -X- _ O
not -X- _ O
having -X- _ O
any -X- _ O
contextualism -X- _ O
in -X- _ O
the -X- _ O
input. -X- _ O

Where -X- _ O
γ -X- _ B-HyperparameterName
is -X- _ O
a -X- _ O
hyperparameter -X- _ O
which -X- _ O
that -X- _ O
specifies -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
emphasis -X- _ O
on -X- _ O
the -X- _ O
CP -X- _ O
training -X- _ O
loss: -X- _ O

The -X- _ O
CPs -X- _ O
are -X- _ O
trained -X- _ O
by -X- _ O
an -X- _ O
additional -X- _ O
term -X- _ O
which -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
KL-divergence1 -X- _ O
of -X- _ O
each -X- _ O
layer’s -X- _ O
CP -X- _ O
output -X- _ O
with -X- _ O
the -X- _ O
extracted -X- _ O
saliencies. -X- _ O

In -X- _ O
this -X- _ O
case, -X- _ O
the -X- _ O
non-zero -X- _ O
value -X- _ O
in -X- _ O
the -X- _ O
second -X- _ O
term -X- _ O
of -X- _ O
Equation -X- _ O
4, -X- _ O
which -X- _ O
facilitates -X- _ O
optimizing -X- _ O
ηℓ. -X- _ O

Consider -X- _ O
a -X- _ O
scenario -X- _ O
in -X- _ O
which -X- _ O
ηℓ -X- _ O
sharply -X- _ O
drops, -X- _ O
causing -X- _ O
most -X- _ O
of -X- _ O
˜Sℓ -X- _ O
i -X- _ O
get -X- _ O
over -X- _ O
the -X- _ O
δℓ -X- _ O
threshold. -X- _ O

β -X- _ B-HyperparameterName
< -X- _ O
0.1 -X- _ O
to -X- _ O
construct -X- _ O
a -X- _ O
small -X- _ O
negative -X- _ O
slope -X- _ O
(similar -X- _ O
to -X- _ O
the -X- _ O
well -X- _ O
known -X- _ O
Leaky-ReLU -X- _ O
of -X- _ O
Maas -X- _ O
et -X- _ O
al. -X- _ O
2013) -X- _ O
for -X- _ O
those -X- _ O
tokens -X- _ O
with -X- _ O
higher -X- _ O
contributing -X- _ O
scores -X- _ O
than -X- _ O
δℓ -X- _ O
threshold. -X- _ O

To -X- _ O
avoid -X- _ O
undergoing -X- _ O
zero -X- _ O
gradients -X- _ O
during -X- _ O
training, -X- _ O
we -X- _ O
define -X- _ O
0 -X- _ O

Increasing -X- _ O
λ -X- _ B-HyperparameterName
makes -X- _ O
the -X- _ O
soft-removal -X- _ O
function -X- _ O
stronger -X- _ O
and -X- _ O
more -X- _ O
decisive -X- _ O
in -X- _ O
masking -X- _ O
the -X- _ O
representations. -X- _ O

In -X- _ O
the -X- _ O
first -X- _ O
term, -X- _ O
the -X- _ O
less -X- _ O
important -X- _ O
tokens -X- _ O
with -X- _ O
scores -X- _ O
lower -X- _ O
than -X- _ O
the -X- _ O
threshold -X- _ O
(δℓ) -X- _ O
are -X- _ O
assigned -X- _ O
higher -X- _ O
negative -X- _ O
masking -X- _ O
as -X- _ O
they -X- _ O
get -X- _ O
more -X- _ O
distant -X- _ O

The -X- _ O
slope -X- _ O
is -X- _ O
determined -X- _ O
by -X- _ O
λadj -X- _ O
= -X- _ O
λ/δ, -X- _ B-HyperparameterName
where -X- _ O
λ -X- _ B-HyperparameterName
is -X- _ O
a -X- _ O
hyperparameter -X- _ O
that -X- _ O
is -X- _ O
increased -X- _ O
exponentially -X- _ O
after -X- _ O
each -X- _ O
epoch -X- _ O
(e.g., -X- _ O
λ -X- _ B-HyperparameterName
← -X- _ O
10 -X- _ O
× -X- _ O
λ -X- _ B-HyperparameterName
after -X- _ O
finishing -X- _ O
each -X- _ O
epoch). -X- _ O

This -X- _ O
function -X- _ O
consists -X- _ O
of -X- _ O
two -X- _ O
main -X- _ O
zones -X- _ O
(Figure -X- _ O
2). -X- _ O

In -X- _ O
each -X- _ O
layer, -X- _ O
after -X- _ O
predicting -X- _ O
contribution -X- _ O
scores, -X- _ O
instead -X- _ O
of -X- _ O
instantly -X- _ O
removing -X- _ O
the -X- _ O
token -X- _ O
representations, -X- _ O
we -X- _ O
accumulate -X- _ O
a -X- _ O
negative -X- _ O
mask -X- _ O
to -X- _ O
the -X- _ O
attention -X- _ O
mask -X- _ O
vector -X- _ O
M -X- _ O
using -X- _ O
a -X- _ O
soft-removal -X- _ O
function: -X- _ O

Using -X- _ O
batch-wise -X- _ O
training -X- _ O
in -X- _ O
this -X- _ O
scenario -X- _ O
will -X- _ O
also -X- _ O
be -X- _ O
problematic -X- _ O
as -X- _ O
the -X- _ O
structure -X- _ O
will -X- _ O
vary -X- _ O
with -X- _ O
each -X- _ O
example. -X- _ O

During -X- _ O
training, -X- _ O
if -X- _ O
tokens -X- _ O
are -X- _ O
immediately -X- _ O
dropped -X- _ O
similarly -X- _ O
to -X- _ O
the -X- _ O
inference -X- _ O
mode, -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
dropping -X- _ O
tokens -X- _ O
cannot -X- _ O
be -X- _ O
captured -X- _ O
using -X- _ O
a -X- _ O
gradient -X- _ O
backpropagation -X- _ O
procedure. -X- _ O

Hence, -X- _ O
inspired -X- _ O
by -X- _ O
the -X- _ O
padding -X- _ O
mechanism -X- _ O
of -X- _ O
self-attention -X- _ O
models -X- _ O
(Vaswani -X- _ O
et -X- _ O
al., -X- _ O
2017) -X- _ O
we -X- _ O
introduce -X- _ O
a -X- _ O
new -X- _ O
procedure -X- _ O
that -X- _ O
gradually -X- _ O
masks -X- _ O
out -X- _ O
less -X- _ O
contributing -X- _ O
token -X- _ O
representations. -X- _ O

We -X- _ O
also -X- _ O
define -X- _ O
a -X- _ O
speedup -X- _ O
tuning -X- _ O
objective -X- _ O
to -X- _ O
determine -X- _ O
the -X- _ O
thresholds -X- _ O
(via -X- _ O
tuning -X- _ O
η) -X- _ O
to -X- _ O
control -X- _ O
the -X- _ O
performance-speedup -X- _ O
trade-off. -X- _ O

Each -X- _ O
CP -X- _ O
is -X- _ O
jointly -X- _ O
trained -X- _ O
with -X- _ O
the -X- _ O
rest -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
using -X- _ O
the -X- _ O
saliencies -X- _ O
extracted -X- _ O
in -X- _ O
the -X- _ O
previous -X- _ O
phase -X- _ O
alongside -X- _ O
with -X- _ O
the -X- _ O
target -X- _ O
task -X- _ O
labels. -X- _ O

In -X- _ O
this -X- _ O
phase, -X- _ O
a -X- _ O
non-linear -X- _ O
function -X- _ O
gradually -X- _ O
fades -X- _ O
out -X- _ O
the -X- _ O
representations -X- _ O
throughout -X- _ O
the -X- _ O
training -X- _ O
process. -X- _ O

The -X- _ O
final -X- _ O
step -X- _ O
is -X- _ O
to -X- _ O
train -X- _ O
a -X- _ O
pre-trained -X- _ O
model -X- _ O
using -X- _ O
an -X- _ O
adaptive -X- _ O
length -X- _ O
reduction -X- _ O
procedure. -X- _ O

Also, -X- _ O
since -X- _ O
each -X- _ O
input -X- _ O
instance -X- _ O
has -X- _ O
a -X- _ O
different -X- _ O
computational -X- _ O
path -X- _ O
during -X- _ O
token -X- _ O
removal -X- _ O
process, -X- _ O
it -X- _ O
is -X- _ O
obvious -X- _ O
that -X- _ O
at -X- _ O
inference -X- _ O
time, -X- _ O
the -X- _ O
batch -X- _ O
size -X- _ O
should -X- _ O
be -X- _ O
equal -X- _ O
to -X- _ O
one -X- _ O
(single -X- _ O
instance -X- _ O
usage), -X- _ O
similarly -X- _ O
to -X- _ O
other -X- _ O
dynamic -X- _ O
approaches -X- _ O
(Zhou -X- _ O
et -X- _ O

Tokens -X- _ O
are -X- _ O
considered -X- _ O
important -X- _ O
if -X- _ O
their -X- _ O
contribution -X- _ O
score -X- _ O
exceeds -X- _ O
δ -X- _ O
(which -X- _ O
is -X- _ O
a -X- _ O
value -X- _ O
equal -X- _ O
or -X- _ O
smaller -X- _ O
than -X- _ O
the -X- _ O
uniform -X- _ O
score). -X- _ O

Based -X- _ O
on -X- _ O
this -X- _ O
intuition, -X- _ O
we -X- _ O
define -X- _ O
a -X- _ O
cutoff -X- _ O
threshold -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
uniform -X- _ O
level -X- _ O
as: -X- _ O
δℓ -X- _ O
= -X- _ O
ηℓ -X- _ O
· -X- _ O
1/n -X- _ O
with -X- _ O
0 -X- _ O

Despite -X- _ O
preserving -X- _ O
this, -X- _ O
other -X- _ O
tokens -X- _ O
might -X- _ O
be -X- _ O
removed -X- _ O
from -X- _ O
a -X- _ O
layer -X- _ O
when -X- _ O
[CLS] -X- _ O
has -X- _ O
a -X- _ O
significantly -X- _ O
high -X- _ O
estimated -X- _ O
contribution -X- _ O
score -X- _ O
than -X- _ O
others. -X- _ O

Given -X- _ O
that -X- _ O
the -X- _ O
final -X- _ O
classification -X- _ O
head -X- _ O
uses -X- _ O
the -X- _ O
last -X- _ O
hidden -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
[CLS] -X- _ O
token, -X- _ O
we -X- _ O
preserve -X- _ O
this -X- _ O
token’s -X- _ O
representation -X- _ O
in -X- _ O
all -X- _ O
layers. -X- _ O

On -X- _ O
the -X- _ O
other -X- _ O
hand, -X- _ O
the -X- _ O
lower-scoring -X- _ O
tokens -X- _ O
could -X- _ O
be -X- _ O
viewed -X- _ O
as -X- _ O
unnecessary -X- _ O
tokens -X- _ O
if -X- _ O
the -X- _ O
contribution -X- _ O
scores -X- _ O
are -X- _ O
concentrated -X- _ O
only -X- _ O
on -X- _ O
a -X- _ O
subset -X- _ O
of -X- _ O
tokens. -X- _ O

Tokens -X- _ O
are -X- _ O
selected -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
contribution -X- _ O
scores -X- _ O
˜Sℓ -X- _ O
obtained -X- _ O
from -X- _ O
the -X- _ O
CP -X- _ O
of -X- _ O
the -X- _ O
corresponding -X- _ O
layer -X- _ O
ℓ. -X- _ O
As -X- _ O
the -X- _ O
sum -X- _ O
of -X- _ O
these -X- _ O
scores -X- _ O
is -X- _ O
equal -X- _ O
to -X- _ O
one, -X- _ O
a -X- _ O
uniform -X- _ O
level -X- _ O
indicates -X- _ O
that -X- _ O
all -X- _ O
tokens -X- _ O
contribute -X- _ O
equally -X- _ O
to -X- _ O
the -X- _ O
prediction -X- _ O
and -X- _ O
should -X- _ O
be -X- _ O
retained. -X- _ O

In -X- _ O
our -X- _ O
approach, -X- _ O
we -X- _ O
eliminate -X- _ O
less -X- _ O
contributing -X- _ O
token -X- _ O
representations -X- _ O
before -X- _ O
delivering -X- _ O
hidden -X- _ O
states -X- _ O
to -X- _ O
the -X- _ O
next -X- _ O
encoder. -X- _ O

The -X- _ O
input -X- _ O
sequence -X- _ O
of -X- _ O
n -X- _ O
tokens -X- _ O
is -X- _ O
usually -X- _ O
passed -X- _ O
through -X- _ O
an -X- _ O
embedding -X- _ O
layer -X- _ O
to -X- _ O
build -X- _ O
the -X- _ O
initial -X- _ O
hidden -X- _ O
states -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
h0. -X- _ O

We -X- _ O
argue -X- _ O
that, -X- _ O
despite -X- _ O
being -X- _ O
limited -X- _ O
in -X- _ O
learning -X- _ O
capacity, -X- _ O
the -X- _ O
MLP -X- _ O
is -X- _ O
sufficient -X- _ O
for -X- _ O
estimating -X- _ O
scores -X- _ O
that -X- _ O
are -X- _ O
more -X- _ O
generalized -X- _ O
and -X- _ O
relevant -X- _ O
than -X- _ O
vanilla -X- _ O
saliency -X- _ O
values. -X- _ O

To -X- _ O
circumvent -X- _ O
this, -X- _ O
we -X- _ O
simply -X- _ O
add -X- _ O
a -X- _ O
CP -X- _ O
after -X- _ O
each -X- _ O
layer -X- _ O
ℓ -X- _ O
in -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
estimate -X- _ O
contribution -X- _ O
score -X- _ O
for -X- _ O
each -X- _ O
token -X- _ O
representation, -X- _ O
i.e., -X- _ O
˜Sℓ -X- _ O
i -X- _ O
. -X- _ O

Computing -X- _ O
gradients -X- _ O
during -X- _ O
inference -X- _ O
is -X- _ O
problematic -X- _ O
as -X- _ O
backpropagation -X- _ O
computation -X- _ O
prolongs -X- _ O
inference -X- _ O
time, -X- _ O
which -X- _ O
is -X- _ O
contrary -X- _ O
to -X- _ O
our -X- _ O
main -X- _ O
goal. -X- _ O

To -X- _ O
this -X- _ O
end, -X- _ O
we -X- _ O
opted -X- _ O
for -X- _ O
saliency -X- _ O
scores -X- _ O
which -X- _ O
have -X- _ O
been -X- _ O
recently -X- _ O
shown -X- _ O

Therefore, -X- _ O
one -X- _ O
important -X- _ O
step -X- _ O
is -X- _ O
to -X- _ O
measure -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
each -X- _ O
token. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1, -X- _ O
our -X- _ O
approach -X- _ O
relies -X- _ O
on -X- _ O
dropping -X- _ O
low -X- _ O
contributing -X- _ O
tokens -X- _ O
in -X- _ O
each -X- _ O
layer -X- _ O
and -X- _ O
passing -X- _ O
only -X- _ O
the -X- _ O
more -X- _ O
important -X- _ O
ones -X- _ O
to -X- _ O
the -X- _ O
next. -X- _ O

While -X- _ O
self-attention -X- _ O
is -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
most -X- _ O
white-box -X- _ O
components -X- _ O
in -X- _ O
transformer-based -X- _ O
models, -X- _ O
relying -X- _ O
on -X- _ O
raw -X- _ O
attention -X- _ O
weights -X- _ O
as -X- _ O
an -X- _ O
explanation -X- _ O
could -X- _ O
be -X- _ O
misleading -X- _ O
given -X- _ O
that -X- _ O
they -X- _ O
are -X- _ O
not -X- _ O
necessarily -X- _ O
responsible -X- _ O
for -X- _ O
determining -X- _ O
the -X- _ O
contribution -X- _ O
of -X- _ O
each -X- _ O
token -X- _ O
in -X- _ O
the -X- _ O
final -X- _ O
classifier’s -X- _ O
decision -X- _ O
(Jain -X- _ O
and -X- _ O
Wallace, -X- _ O
2019; -X- _ O
Serrano -X- _ O
and -X- _ O
Smith, -X- _ O
2019; -X- _ O
Abnar -X- _ O
and -X- _ O
Zuidema, -X- _ O
2020). -X- _ O

This -X- _ O
impedes -X- _ O
the -X- _ O
usage -X- _ O
of -X- _ O
self-attention -X- _ O
based -X- _ O
models -X- _ O
in -X- _ O
low-resource -X- _ O
settings. -X- _ O

Before -X- _ O
applying -X- _ O
a -X- _ O
softmax -X- _ O
function, -X- _ O
these -X- _ O
values -X- _ O
are -X- _ O
divided -X- _ O
by -X- _ O
a -X- _ O
scaling -X- _ O
factor -X- _ O
and -X- _ O
then -X- _ O
added -X- _ O
to -X- _ O
an -X- _ O
attention -X- _ O
mask -X- _ O
vector -X- _ O
m, -X- _ O
which -X- _ O
is -X- _ O
zero -X- _ O
for -X- _ O
positions -X- _ O
we -X- _ O
wish -X- _ O
to -X- _ O
attend -X- _ O
and -X- _ O
−∞ -X- _ O
(in -X- _ O
practice, -X- _ O
−10000) -X- _ O
for -X- _ O
padded -X- _ O
tokens -X- _ O
(Vaswani -X- _ O
et -X- _ O
al., -X- _ O
2017). -X- _ O

To -X- _ O
this -X- _ O
end, -X- _ O
each -X- _ O
input -X- _ O
vector -X- _ O
xi -X- _ O
is -X- _ O
multiplied -X- _ O
by -X- _ O
the -X- _ O
corresponding -X- _ O
trainable -X- _ O
matrices -X- _ O
Q, -X- _ O
K, -X- _ O
and -X- _ O
V -X- _ O
to -X- _ O
respectively -X- _ O
produce -X- _ O
query -X- _ O
(qi), -X- _ O
key -X- _ O
(ki), -X- _ O
and -X- _ O
value -X- _ O
(vi) -X- _ O
vectors. -X- _ O

al., -X- _ O
2017) -X- _ O
which -X- _ O
looks -X- _ O
for -X- _ O
the -X- _ O
relation -X- _ O
between -X- _ O
different -X- _ O
positions -X- _ O
of -X- _ O
a -X- _ O
single -X- _ O
sequence -X- _ O
of -X- _ O
token -X- _ O
representations -X- _ O
(x1, -X- _ O
..., -X- _ O
xn) -X- _ O
to -X- _ O
build -X- _ O
contextualized -X- _ O
representations. -X- _ O

• -X- _ O
We -X- _ O
also -X- _ O
show -X- _ O
the -X- _ O
superiority -X- _ O
of -X- _ O
our -X- _ O
token -X- _ O
selection -X- _ O
strategy -X- _ O
over -X- _ O
the -X- _ O
other -X- _ O
widely -X- _ O
used -X- _ O
strategies -X- _ O
by -X- _ O
using -X- _ O
human -X- _ O
rationales. -X- _ O

sentations -X- _ O
by -X- _ O
employing -X- _ O
a -X- _ O
novel -X- _ O
soft-removal -X- _ O
function. -X- _ O

• -X- _ O
We -X- _ O
couple -X- _ O
a -X- _ O
simple -X- _ O
Contribution -X- _ O
Predictor -X- _ O
(CP) -X- _ O
with -X- _ O
each -X- _ O
layer -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
estimate -X- _ O
tokens’ -X- _ O
contribution -X- _ O
scores -X- _ O
to -X- _ O
eliminate -X- _ O
redundant -X- _ O
representations. -X- _ O

In -X- _ O
summary, -X- _ O
our -X- _ O
contributions -X- _ O
are -X- _ O
threefold: -X- _ O

It -X- _ O
is -X- _ O
also -X- _ O
worth -X- _ O
noting -X- _ O
in -X- _ O
contrast -X- _ O
to -X- _ O
our -X- _ O
approach -X- _ O
above -X- _ O
studies -X- _ O
are -X- _ O
based -X- _ O
on -X- _ O
top-k -X- _ O
operations -X- _ O
for -X- _ O
identifying -X- _ O
the -X- _ O
k -X- _ O
most -X- _ O
important -X- _ O
tokens -X- _ O
during -X- _ O
training -X- _ O
or -X- _ O
inference, -X- _ O
which -X- _ O
can -X- _ O
be -X- _ O
expensive -X- _ O
without -X- _ O
a -X- _ O
specific -X- _ O
hardware -X- _ O
architecture -X- _ O
(Wang -X- _ O
et -X- _ O
al., -X- _ O
2021). -X- _ O

In -X- _ O
contrast, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
simple -X- _ O
but -X- _ O
effective -X- _ O
method -X- _ O
to -X- _ O
gradually -X- _ O
eliminate -X- _ O
tokens -X- _ O
in -X- _ O
each -X- _ O
layer -X- _ O
throughout -X- _ O
the -X- _ O
training -X- _ O
phase -X- _ O
using -X- _ O
a -X- _ O
soft-removal -X- _ O
function -X- _ O
which -X- _ O
allows -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
be -X- _ O
adaptable -X- _ O
to -X- _ O
various -X- _ O
inputs -X- _ O
in -X- _ O
a -X- _ O
batch-wise -X- _ O
mode. -X- _ O

Hence, -X- _ O
they -X- _ O
only -X- _ O
perform -X- _ O
token -X- _ O
selection -X- _ O
at -X- _ O
two -X- _ O
layers. -X- _ O

All -X- _ O
of -X- _ O
these -X- _ O
steps -X- _ O
significantly -X- _ O
increase -X- _ O
the -X- _ O
training -X- _ O
cost. -X- _ O

To -X- _ O
mitigate -X- _ O
this, -X- _ O
they -X- _ O
resorted -X- _ O
to -X- _ O
extra -X- _ O
heuristics -X- _ O
such -X- _ O
as -X- _ O
imitation -X- _ O
learning -X- _ O
(Hussein -X- _ O
et -X- _ O
al., -X- _ O
2017) -X- _ O
for -X- _ O
warming -X- _ O
up -X- _ O
the -X- _ O
training -X- _ O
of -X- _ O
the -X- _ O
policy -X- _ O
network, -X- _ O
action -X- _ O
sampling -X- _ O
for -X- _ O
limiting -X- _ O
the -X- _ O
search -X- _ O
space, -X- _ O
and -X- _ O
knowledge -X- _ O
distillation -X- _ O
for -X- _ O
transferring -X- _ O
knowledge -X- _ O
from -X- _ O
the -X- _ O
intact -X- _ O
backbone -X- _ O
fine-tuned -X- _ O
model. -X- _ O

However, -X- _ O
as -X- _ O
pointed -X- _ O
out -X- _ O
by -X- _ O
the -X- _ O
authors, -X- _ O
the -X- _ O
problem -X- _ O
has -X- _ O
a -X- _ O
large -X- _ O
search -X- _ O
space, -X- _ O
making -X- _ O
it -X- _ O
difficult -X- _ O
for -X- _ O
RL -X- _ O
to -X- _ O
solve. -X- _ O

The -X- _ O
most -X- _ O
related -X- _ O
study -X- _ O
to -X- _ O
ours -X- _ O
is -X- _ O
TR-BERT -X- _ B-MethodName
(Ye -X- _ O
et -X- _ O
al., -X- _ O
2021) -X- _ O
which -X- _ O
leverages -X- _ O
reinforcement -X- _ O
learning -X- _ O
to -X- _ O
develop -X- _ O
an -X- _ O
input-adaptive -X- _ O
token -X- _ O
selection -X- _ O
policy -X- _ O
network. -X- _ O

We -X- _ O
show -X- _ O
that -X- _ O
this -X- _ O
choice -X- _ O
results -X- _ O
in -X- _ O
more -X- _ O
reliable -X- _ O
scores -X- _ O
than -X- _ O
attention -X- _ O
weights -X- _ O
in -X- _ O
measuring -X- _ O
tokens’ -X- _ O
contributions. -X- _ O

Instead -X- _ O
of -X- _ O
relying -X- _ O
on -X- _ O
attention -X- _ O
weights, -X- _ O
our -X- _ O
method -X- _ O
trains -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
Contribution -X- _ O
Predictors -X- _ O
(CP) -X- _ O
to -X- _ O
estimate -X- _ O
tokens’ -X- _ O
saliency -X- _ O
scores -X- _ O
at -X- _ O
inference. -X- _ O

In -X- _ O
this -X- _ O
work, -X- _ O
we -X- _ O
introduce -X- _ O
Adaptive -X- _ B-MethodName
Length -X- _ I-MethodName
Reduction -X- _ I-MethodName
(AdapLeR). -X- _ B-MethodName

In -X- _ O
addition, -X- _ O
their -X- _ O
token -X- _ O
selection -X- _ O
strategies -X- _ O
are -X- _ O
based -X- _ O
on -X- _ O
attention -X- _ O
weights -X- _ O
which -X- _ O
can -X- _ O
result -X- _ O
in -X- _ O
a -X- _ O
suboptimal -X- _ O
solution -X- _ O
(Ye -X- _ O
et -X- _ O
al., -X- _ O
2021). -X- _ O

Several -X- _ O
studies -X- _ O
have -X- _ O
followed -X- _ O
(Kim -X- _ O
and -X- _ O
Cho, -X- _ O
2021; -X- _ O
Wang -X- _ O
et -X- _ O
al., -X- _ O
2021); -X- _ O
However, -X- _ O
they -X- _ O
usually -X- _ O
optimize -X- _ O
a -X- _ O
single -X- _ O
token -X- _ O
elimination -X- _ O
configuration -X- _ O
across -X- _ O
the -X- _ O
entire -X- _ O
dataset, -X- _ O
resulting -X- _ O
in -X- _ O
a -X- _ O
static -X- _ O
model. -X- _ O

PoWER-BERT -X- _ B-MethodName
(Goyal -X- _ O
et -X- _ O
al., -X- _ O
2020) -X- _ O
is -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
first -X- _ O
such -X- _ O
techniques -X- _ O
which -X- _ O
reduces -X- _ O
inference -X- _ O
time -X- _ O
by -X- _ O
eliminating -X- _ O
redundant -X- _ O
token -X- _ O
representations -X- _ O
through -X- _ O
layers -X- _ O
based -X- _ O
on -X- _ O
self-attention -X- _ O
weights. -X- _ O

come -X- _ O
at -X- _ O
the -X- _ O
cost -X- _ O
of -X- _ O
reducing -X- _ O
model’s -X- _ O
capacity -X- _ O
in -X- _ O
complex -X- _ O
reasoning -X- _ O
(Sanh -X- _ O
et -X- _ O
al., -X- _ O
2019; -X- _ O
Sun -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O

Moreover, -X- _ O
in -X- _ O
contrast -X- _ O
to -X- _ O
layer-wise -X- _ O
pruning, -X- _ O
token-level -X- _ O
pruning -X- _ O
does -X- _ O
not -X- _ O

Among -X- _ O
these -X- _ O
redundancies, -X- _ O
some -X- _ O
tokens -X- _ O
carry -X- _ O
more -X- _ O
task-specific -X- _ O
information -X- _ O
than -X- _ O
others -X- _ O
(Mohebbi -X- _ O
et -X- _ O
al., -X- _ O
2021), -X- _ O
suggesting -X- _ O
that -X- _ O
only -X- _ O
these -X- _ O
tokens -X- _ O
could -X- _ O
be -X- _ O
considered -X- _ O
through -X- _ O
the -X- _ O
model. -X- _ O

Liu -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
softmax -X- _ O
outputs -X- _ O
with -X- _ O
temperature -X- _ O
calibration -X- _ O
(Schwartz -X- _ O
et -X- _ O
al., -X- _ O
2020b), -X- _ O
trained -X- _ O
confidence -X- _ O
predictors -X- _ O
(Xin -X- _ O
et -X- _ O
al., -X- _ O
2021), -X- _ O
or -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
agreements -X- _ O
between -X- _ O
predictions -X- _ O
of -X- _ O
intermediate -X- _ O
classifiers -X- _ O
(Zhou -X- _ O
et -X- _ O
al., -X- _ O
2020). -X- _ O

Early -X- _ O
exit -X- _ O
mechanism -X- _ O
(Schwartz -X- _ O
et -X- _ O
al., -X- _ O
2020b; -X- _ O
Liao -X- _ O
et -X- _ O
al., -X- _ O
2021; -X- _ O
Xin -X- _ O
et -X- _ O
al., -X- _ O
2020; -X- _ O
Liu -X- _ O
et -X- _ O
al., -X- _ O
2020; -X- _ O
Xin -X- _ O
et -X- _ O
al., -X- _ O
2021; -X- _ O
Sun -X- _ O
et -X- _ O
al., -X- _ O
2021; -X- _ O
Eyzaguirre -X- _ O
et -X- _ O
al., -X- _ O
2021) -X- _ O
is -X- _ O
a -X- _ O
commonly -X- _ O
used -X- _ O
method -X- _ O
in -X- _ O
which -X- _ O
each -X- _ O
layer -X- _ O
in -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
coupled -X- _ O
with -X- _ O
an -X- _ O
intermediate -X- _ O
classifier -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
target -X- _ O
label. -X- _ O

A -X- _ O
number -X- _ O
of -X- _ O
techniques -X- _ O
have -X- _ O
been -X- _ O
also -X- _ O
proposed -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
make -X- _ O
efficiency -X- _ O
enhancement -X- _ O
sensitive -X- _ O
to -X- _ O
inputs. -X- _ O

This -X- _ O
is -X- _ O
particularly -X- _ O
promising -X- _ O
as -X- _ O
recent -X- _ O
analytical -X- _ O
studies -X- _ O
showed -X- _ O
that -X- _ O
there -X- _ O
are -X- _ O
redundant -X- _ O
encoded -X- _ O
information -X- _ O
in -X- _ O
token -X- _ O
representations -X- _ O
(Klafka -X- _ O
and -X- _ O
Ettinger, -X- _ O
2020; -X- _ O
Ethayarajh, -X- _ O
2019). -X- _ O

al., -X- _ O
2021), -X- _ O
i.e., -X- _ O
reducing -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
hidden -X- _ O
states. -X- _ O

However, -X- _ O
one -X- _ O
can -X- _ O
view -X- _ O
compression -X- _ O
from -X- _ O
the -X- _ O
width -X- _ O
perspective -X- _ O
(Goyal -X- _ O
et -X- _ O
al., -X- _ O
2020; -X- _ O
Ye -X- _ O
et -X- _ O

Most -X- _ O
of -X- _ O
these -X- _ O
input-adaptive -X- _ O
techniques -X- _ O
compress -X- _ O
the -X- _ O
model -X- _ O
from -X- _ O
the -X- _ O
depth -X- _ O
perspective -X- _ O
(i.e., -X- _ O
reducing -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
involved -X- _ O
encoder -X- _ O
layers). -X- _ O

In -X- _ O
this -X- _ O
regard, -X- _ O
there -X- _ O
have -X- _ O
been -X- _ O
various -X- _ O
attempts -X- _ O
at -X- _ O
improving -X- _ O
the -X- _ O
efficiency -X- _ O
of -X- _ O
BERT-based -X- _ O
models -X- _ O
(Devlin -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
including -X- _ O
knowledge -X- _ O
distilation -X- _ O
(Hinton -X- _ O
et -X- _ O
al., -X- _ O
2015; -X- _ O

Various -X- _ O
halting -X- _ O
conditions -X- _ O
have -X- _ O
been -X- _ O
proposed, -X- _ O
including -X- _ O
Shannon’s -X- _ O
entropy -X- _ O
(Xin -X- _ O
et -X- _ O
al., -X- _ O
2020; -X- _ O

At -X- _ O
inference, -X- _ O
a -X- _ O
halting -X- _ O
condition -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
determine -X- _ O
whether -X- _ O
the -X- _ O
model -X- _ O
allows -X- _ O
an -X- _ O
example -X- _ O
to -X- _ O
exit -X- _ O
without -X- _ O
passing -X- _ O
through -X- _ O
all -X- _ O
layers. -X- _ O

Despite -X- _ O
providing -X- _ O
significant -X- _ O
reduction -X- _ O
in -X- _ O
model -X- _ O
size, -X- _ O
these -X- _ O
techniques -X- _ O
are -X- _ O
generally -X- _ O
static -X- _ O
at -X- _ O
inference -X- _ O
time, -X- _ O
i.e., -X- _ O
they -X- _ O
dedicate -X- _ O
the -X- _ O
same -X- _ O
amount -X- _ O
of -X- _ O
computation -X- _ O
to -X- _ O
all -X- _ O
inputs, -X- _ O
irrespective -X- _ O
of -X- _ O
their -X- _ O
difficulty. -X- _ O

While -X- _ O
large-scale -X- _ O
pre-trained -X- _ O
language -X- _ O
models -X- _ O
exhibit -X- _ O
remarkable -X- _ O
performances -X- _ O
on -X- _ O
various -X- _ O
NLP -X- _ B-TaskName
benchmarks, -X- _ O
their -X- _ O
excessive -X- _ O
computational -X- _ O
costs -X- _ O
and -X- _ O
high -X- _ O
inference -X- _ O
latency -X- _ O
have -X- _ O
limited -X- _ O
their -X- _ O
usage -X- _ O
in -X- _ O
resource-limited -X- _ O
settings. -X- _ O

Introduction -X- _ O

In -X- _ O
comparison -X- _ O
to -X- _ O
other -X- _ O
widely -X- _ O
used -X- _ O
strategies -X- _ O
for -X- _ O
selecting -X- _ O
important -X- _ O
tokens, -X- _ O
such -X- _ O
as -X- _ O
saliency -X- _ O
and -X- _ O
attention, -X- _ O
our -X- _ O
proposed -X- _ O
method -X- _ O
has -X- _ O
a -X- _ O
significantly -X- _ O
lower -X- _ O
false -X- _ O
positive -X- _ O
rate -X- _ O
in -X- _ O
generating -X- _ O
rationales. -X- _ O

We -X- _ O
also -X- _ O
validate -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
selected -X- _ O
tokens -X- _ O
in -X- _ O
our -X- _ O
method -X- _ O
using -X- _ O
human -X- _ O
annotations -X- _ O
in -X- _ O
the -X- _ O
ERASER -X- _ B-DatasetName
benchmark. -X- _ O

Our -X- _ O
experiments -X- _ O
on -X- _ O
several -X- _ O
diverse -X- _ O
classification -X- _ O
tasks -X- _ O
show -X- _ O
speedups -X- _ B-MetricName
up -X- _ I-MetricName
to -X- _ O
22x -X- _ B-MetricValue
during -X- _ O
inference -X- _ O
time -X- _ O
without -X- _ O
much -X- _ O
sacrifice -X- _ O
in -X- _ O
performance. -X- _ O

In -X- _ O
this -X- _ O
work, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
novel -X- _ O
approach -X- _ O
for -X- _ O
reducing -X- _ O
the -X- _ O
computational -X- _ O
cost -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
with -X- _ O
minimal -X- _ O
loss -X- _ O
in -X- _ O
downstream -X- _ O
performance. -X- _ O

But, -X- _ O
this -X- _ O
usually -X- _ O
comes -X- _ O
at -X- _ O
the -X- _ O
cost -X- _ O
of -X- _ O
high -X- _ O
latency -X- _ O
and -X- _ O
computation, -X- _ O
hindering -X- _ O
their -X- _ O
usage -X- _ O
in -X- _ O
resource-limited -X- _ O
settings. -X- _ O

Pre-trained -X- _ O
language -X- _ O
models -X- _ O
have -X- _ O
shown -X- _ O
stellar -X- _ O
performance -X- _ O
in -X- _ O
various -X- _ O
downstream -X- _ O
tasks. -X- _ O

Tehran -X- _ O
Institute -X- _ O
for -X- _ O
Advanced -X- _ O
Studies, -X- _ O
Khatam -X- _ O
University, -X- _ O
Iran -X- _ O

m_modarressi@comp.iust.ac.ir -X- _ O
h.mohebbi@uvt.nl -X- _ O
mp792@cam.ac.uk -X- _ O

Abstract -X- _ O

Speeding -X- _ O
up -X- _ O
Inference -X- _ O
by -X- _ O
Adaptive -X- _ B-MethodName
Length -X- _ I-MethodName
Reduction -X- _ I-MethodName

For -X- _ O
the -X- _ O
ensemble, -X- _ O
multi-label, -X- _ B-MethodName
and -X- _ I-MethodName
multi-task -X- _ I-MethodName
models, -X- _ O
we -X- _ O
conduct -X- _ O
two -X- _ O
types -X- _ O
of -X- _ O
evaluation: -X- _ O
First, -X- _ O
we -X- _ O
test -X- _ O
how -X- _ O
well -X- _ O
the -X- _ O
majority -X- _ O
vote -X- _ O
of -X- _ O
predicted -X- _ O
labels -X- _ O
match -X- _ O
the -X- _ O
majority -X- _ O
vote -X- _ O
of -X- _ O
annotations -X- _ O
(columns -X- _ O
2-4 -X- _ O
in -X- _ O
Table -X- _ O
1); -X- _ O
second, -X- _ O
we -X- _ O
report -X- _ O
how -X- _ O
well -X- _ O
the -X- _ O
individual -X- _ O
predicted -X- _ O
labels -X- _ O
97 -X- _ O

Multi-label -X- _ B-MetricName
Multi-task -X- _ I-MetricName
Precision -X- _ I-MetricName
49.53±3.8 -X- _ B-MetricValue

In -X- _ O
other -X- _ O
words, -X- _ O
modeling -X- _ O
each -X- _ O
annotator, -X- _ O
and -X- _ O
their -X- _ O
presumable -X- _ O
internal -X- _ O
consistency, -X- _ O
could -X- _ O
lead -X- _ O
to -X- _ O
more -X- _ O
stable -X- _ O
prediction -X- _ O
results. -X- _ O

We -X- _ O
now -X- _ O
evaluate -X- _ O
the -X- _ O
individual -X- _ O
predictions -X- _ O
made -X- _ O
by -X- _ O
the -X- _ O
multi-annotator -X- _ B-MethodName
model -X- _ I-MethodName
(prior -X- _ O
to -X- _ O
majority -X- _ O
vote) -X- _ O
on -X- _ O
how -X- _ O
well -X- _ O
they -X- _ O
match -X- _ O
individual -X- _ O
three -X- _ O
multiannotators’ -X- _ O
labels -X- _ O
(Table -X- _ O
1). -X- _ O

We -X- _ O
compare -X- _ O
uncertainty -X- _ O
in -X- _ O
predictions -X- _ O
with -X- _ O
annotator -X- _ O
disagreement, -X- _ O
measured -X- _ O
as -X- _ O
the -X- _ O
variance -X- _ B-MetricName
of -X- _ I-MetricName
the -X- _ I-MetricName
annotations. -X- _ I-MetricName

Being -X- _ O
able -X- _ O
to -X- _ O
model -X- _ O
uncertainty -X- _ O
is -X- _ O
especially -X- _ O
useful -X- _ O
in -X- _ O
deployment -X- _ O
scenarios -X- _ O
where -X- _ O
knowing -X- _ O
when -X- _ O
not -X- _ O
to -X- _ O
make -X- _ O
a -X- _ O
prediction -X- _ O
is -X- _ O
important. -X- _ O

Our -X- _ O
approach -X- _ O
also -X- _ O
provides -X- _ O
a -X- _ O
way -X- _ O
to -X- _ O
estimate -X- _ O
uncertainty -X- _ O
in -X- _ O
predictions, -X- _ O
which -X- _ O
we -X- _ O
demonstrate -X- _ O
better -X- _ O
correlate -X- _ O
with -X- _ O
annotation -X- _ O
disagreements -X- _ O
than -X- _ O
traditional -X- _ O
methods. -X- _ O

We -X- _ O
show -X- _ O
that -X- _ O
this -X- _ O
approach -X- _ O
yields -X- _ O
same -X- _ O
or -X- _ O
better -X- _ O
performance -X- _ O
than -X- _ O
aggregating -X- _ O
labels -X- _ O
in -X- _ O
the -X- _ O
data -X- _ O
prior -X- _ O
to -X- _ O
training -X- _ O
across -X- _ O
seven -X- _ O
different -X- _ O
binary -X- _ O
classification -X- _ O
tasks. -X- _ O

In -X- _ O
particular, -X- _ O
our -X- _ O
multi-task -X- _ B-MethodName
based -X- _ I-MethodName
approach -X- _ I-MethodName
treats -X- _ O
predicting -X- _ O
each -X- _ O
annotators’ -X- _ O
judgements -X- _ O
as -X- _ O
separate -X- _ O
subtasks, -X- _ O
while -X- _ O
sharing -X- _ O
a -X- _ O
common -X- _ O
learned -X- _ O
representation -X- _ O
of -X- _ O
the -X- _ O
task. -X- _ O

In -X- _ O
order -X- _ O
to -X- _ O
address -X- _ O
this, -X- _ O
we -X- _ O
investigate -X- _ O
the -X- _ O
efficacy -X- _ O
of -X- _ O
multi-annotator -X- _ B-MethodName
models. -X- _ O

Majority -X- _ B-MethodName
voting -X- _ I-MethodName
and -X- _ O
averaging -X- _ O
are -X- _ O
common -X- _ O
approaches -X- _ O
used -X- _ O
to -X- _ O
resolve -X- _ B-TaskName
annotator -X- _ I-TaskName
disagreements -X- _ I-TaskName
and -X- _ O
derive -X- _ B-TaskName
single -X- _ I-TaskName
ground -X- _ I-TaskName
truth -X- _ I-TaskName
labels -X- _ I-TaskName
from -X- _ I-TaskName
multiple -X- _ I-TaskName
annotations. -X- _ I-TaskName

Annotator -X- _ O
disagreements -X- _ O
may -X- _ O
capture -X- _ O
important -X- _ O
nuances -X- _ O
in -X- _ O
such -X- _ O
tasks -X- _ O
that -X- _ O
are -X- _ O
often -X- _ O
ignored -X- _ O
while -X- _ O
aggregating -X- _ B-MethodName
annotations -X- _ I-MethodName
to -X- _ O
a -X- _ O
single -X- _ O
ground -X- _ O
truth. -X- _ O

However, -X- _ O
annotators -X- _ O
may -X- _ O
systematically -X- _ O
disagree -X- _ O
with -X- _ O
one -X- _ O
another, -X- _ O
often -X- _ O
reflecting -X- _ O
their -X- _ O
individual -X- _ O
biases -X- _ O
and -X- _ O
values, -X- _ O
especially -X- _ O
in -X- _ O
the -X- _ O
case -X- _ O
of -X- _ O
subjective -X- _ O
tasks -X- _ O
such -X- _ O
as -X- _ O
detecting -X- _ O
affect, -X- _ O
aggression, -X- _ O
and -X- _ O
hate -X- _ O
speech. -X- _ O

Vinodkumar -X- _ O
Prabhakaran -X- _ O
Google -X- _ O
Research, -X- _ O
USA -X- _ O
vinodkpg@google.com -X- _ O

Dealing -X- _ O
with -X- _ O
Disagreements: -X- _ O
Looking -X- _ O
Beyond -X- _ O
the -X- _ O
Majority -X- _ O
Vote -X- _ O
in -X- _ O
Subjective -X- _ B-TaskName
Annotations -X- _ I-TaskName
Aida -X- _ O
Mostafazadeh -X- _ O
Davani -X- _ O
University -X- _ O
of -X- _ O
Southern -X- _ O
California, -X- _ O
USA -X- _ O
mostafaz@usc.edu -X- _ O
Mark -X- _ O
D´ıaz -X- _ O
Google -X- _ O
Research, -X- _ O
USA -X- _ O
markdiaz@google.com -X- _ O

Hence, -X- _ O
while -X- _ O
it -X- _ O
is -X- _ O
possible -X- _ O
to -X- _ O
combine -X- _ O
local -X- _ O
attention -X- _ O
and -X- _ O
targeted -X- _ O
upsampling, -X- _ O
this -X- _ O
is -X- _ O
left -X- _ O
as -X- _ O
future -X- _ O
work. -X- _ O

Minimal -X- _ B-TaskName
Answer -X- _ I-TaskName
Span -X- _ I-TaskName
Task -X- _ I-TaskName
(MINSPAN) -X- _ O
Given -X- _ O
a -X- _ O
full -X- _ O
Wikipedia -X- _ O
article, -X- _ O
return -X- _ O
the -X- _ O
start -X- _ O
and -X- _ O
end -X- _ O
byte -X- _ O
indices -X- _ O
of -X- _ O
the -X- _ O
minimal -X- _ O
span -X- _ O
that -X- _ O
completely -X- _ O
answers -X- _ O
the -X- _ O
question. -X- _ O

We -X- _ O
evaluate -X- _ O
on -X- _ O
the -X- _ O
primary -X- _ O
tasks.24 -X- _ O
Passage -X- _ B-TaskName
Selection -X- _ I-TaskName
Task -X- _ I-TaskName
(SELECTP) -X- _ O
Given -X- _ O
a -X- _ O
list -X- _ O
of -X- _ O
the -X- _ O
passages -X- _ O
in -X- _ O
a -X- _ O
Wikipedia -X- _ O
article, -X- _ O
return -X- _ O
either -X- _ O
the -X- _ O
index -X- _ O
of -X- _ O
the -X- _ O
passage -X- _ O
that -X- _ O
answers -X- _ O
the -X- _ O
question, -X- _ O
or -X- _ O
return -X- _ O
NULL -X- _ O
if -X- _ O
the -X- _ O
article -X- _ O
contains -X- _ O
no -X- _ O
acceptable -X- _ O
answer. -X- _ O

90 -X- _ O
Language -X- _ O
mBERT -X- _ B-MethodName
CANINE-C -X- _ B-MethodName
Dutch -X- _ O
English -X- _ O
German -X- _ O
Spanish -X- _ O
Macro -X- _ O
Avg -X- _ O
Amharic -X- _ O
Hausa -X- _ O
Igbo -X- _ O
Kinyarwanda -X- _ O
Luganda -X- _ O
Luo -X- _ O
Nigerian -X- _ O
Pidgin -X- _ O
Swahili -X- _ O
Wolof -X- _ O
Yor`ub´a -X- _ O
Macro -X- _ O
Avg -X- _ O
CONLL -X- _ O
90.2 -X- _ O
91.1 -X- _ O
82.5 -X- _ O
87.6 -X- _ O
87.8 -X- _ O
74.7 -X- _ O
(–15.5) -X- _ O
79.8 -X- _ O
(–11.3) -X- _ O
64.1 -X- _ O
(–18.4) -X- _ O
77.4 -X- _ O
(–10.2) -X- _ O
74.0 -X- _ O
(–13.8) -X- _ O
MASAKHANER -X- _ O
0.0 -X- _ O
89.3 -X- _ O
84.6 -X- _ O
73.9 -X- _ O
80.2 -X- _ O
75.8 -X- _ O
89.8 -X- _ O
87.1 -X- _ O
64.9 -X- _ O
78.7 -X- _ O
72.4 -X- _ O
44.6 -X- _ O
(+44.6) -X- _ O
76.1 -X- _ O
(–13.2) -X- _ O
75.6 -X- _ O
(–9.0) -X- _ O
58.3 -X- _ O
(–15.6) -X- _ O
69.4 -X- _ O
(–10.8) -X- _ O
63.4 -X- _ O
(–12.4) -X- _ O
66.6 -X- _ O
(–23.2) -X- _ O
72.7 -X- _ O
(–14.4) -X- _ O
60.7 -X- _ O
(–4.2) -X- _ O
67.9 -X- _ O
(–10.8) -X- _ O
65.5 -X- _ O
(–6.9) -X- _ O
CANINE-C -X- _ O
+ -X- _ O
n-grams -X- _ O
88.5 -X- _ O
(–1.7) -X- _ O
89.8 -X- _ O
(–1.3) -X- _ O
82.1 -X- _ O
(–0.4) -X- _ O
86.5 -X- _ O
(–1.1) -X- _ O
86.7 -X- _ O
(–1.1) -X- _ O
50.0 -X- _ O
(+50.0) -X- _ O
88.0 -X- _ O
(–1.3) -X- _ O
85.0 -X- _ O
(+0.4) -X- _ O
72.8 -X- _ O
(–1.1) -X- _ O
79.6 -X- _ O
(–0.6) -X- _ O
74.2 -X- _ O
(–1.6) -X- _ O
88.7 -X- _ O
(–1.1) -X- _ O
83.7 -X- _ O
(–3.4) -X- _ O
66.5 -X- _ O
(+1.6) -X- _ O
79.1 -X- _ O
(+0.4) -X- _ O
76.8 -X- _ O
(+4.3) -X- _ O
Table -X- _ O
8: -X- _ O
Language-wise -X- _ O
breakdown -X- _ O
for -X- _ O
Named -X- _ B-TaskName
Entity -X- _ I-TaskName
Recognition -X- _ I-TaskName
for -X- _ O
the -X- _ O
CoNLL -X- _ B-DatasetName
and -X- _ O
MasakhaNER -X- _ B-DatasetName
datasets -X- _ O
(labeled -X- _ B-MetricName
F1). -X- _ I-MetricName

CANINE -X- _ B-MethodName
eliminates -X- _ O
many -X- _ O
engineering -X- _ O
pitfalls -X- _ O
for -X- _ O
practitioners -X- _ O
and -X- _ O
opens -X- _ O
up -X- _ O
new -X- _ O
research -X- _ O
directions -X- _ O
for -X- _ O
the -X- _ O
community. -X- _ O

6 -X- _ O
In -X- _ O
this -X- _ O
article, -X- _ O
we -X- _ O
described -X- _ O
CANINE, -X- _ B-MethodName
which -X- _ O
is, -X- _ O
to -X- _ O
our -X- _ O
knowledge, -X- _ O
the -X- _ O
first -X- _ O
pre-trained -X- _ O
deep -X- _ O
encoder -X- _ O
for -X- _ O
language -X- _ B-TaskName
understanding -X- _ I-TaskName
that -X- _ O
uses -X- _ O
a -X- _ O
tokenization-free, -X- _ O
vocabulary-free -X- _ O
model, -X- _ O
while -X- _ O
surpassing -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
models -X- _ O
built -X- _ O
on -X- _ O
top -X- _ O
of -X- _ O
heuristic -X- _ O
tokenizers. -X- _ O

Such -X- _ O
models -X- _ O
borrow -X- _ O
their -X- _ O
architectures -X- _ O
from -X- _ O
monolingual -X- _ O
predecessors -X- _ O
and -X- _ O
apply -X- _ O
joint -X- _ O
training -X- _ O
in -X- _ O
100+ -X- _ O
languages, -X- _ O
either -X- _ O
with -X- _ O
unsupervised -X- _ O
LM -X- _ O
losses: -X- _ O
mBERT, -X- _ B-MethodName
mT5 -X- _ O
(Xue -X- _ O
et -X- _ O
al., -X- _ O
2021), -X- _ O
or -X- _ O
with -X- _ O
additional -X- _ O
translation -X- _ O
losses: -X- _ O
XLM -X- _ O
(Lample -X- _ O
and -X- _ O
Conneau, -X- _ O
2019), -X- _ O
XLM-R -X- _ O
(Conneau -X- _ O
et -X- _ O
al., -X- _ O
2020). -X- _ O

5.3 -X- _ O
Multilingual -X- _ O
Models -X- _ O
Multilingual -X- _ O
NLP -X- _ O
has -X- _ O
been -X- _ O
dominated -X- _ O
by -X- _ O
deep -X- _ O
pre-trained -X- _ O
multilingual -X- _ O
models -X- _ O
whose -X- _ O
subword -X- _ O
vocabularies -X- _ O
are -X- _ O
shared -X- _ O
across -X- _ O
languages. -X- _ O

Consistent -X- _ O
with -X- _ O
previous -X- _ O
observations -X- _ O
that -X- _ O
feeding -X- _ O
characters -X- _ O
into -X- _ O
a -X- _ O
transformer -X- _ O
stack -X- _ O
comes -X- _ O
with -X- _ O
a -X- _ O
huge -X- _ O
computational -X- _ O
cost -X- _ O
while -X- _ O
not -X- _ O
improving -X- _ O
over -X- _ O
tokenization-based -X- _ O
approaches -X- _ O
(Al-Rfou -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
a -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
fine-tuned -X- _ O
for -X- _ O
semantic -X- _ O
parsing -X- _ O
achieved -X- _ O
gains -X- _ O
only -X- _ O
when -X- _ O
characters -X- _ O
complemented -X- _ O
subwords -X- _ O
(van -X- _ O
Noord -X- _ O
et -X- _ O
al., -X- _ O
2020). -X- _ O

CharacterBERT -X- _ O
(Boukkouri -X- _ O
et -X- _ O
al., -X- _ O
2020) -X- _ O
ported -X- _ O
84 -X- _ O
this -X- _ O
technique -X- _ O
to -X- _ O
BERT, -X- _ O
augmenting -X- _ O
its -X- _ O
existing -X- _ O
WordPiece-tokenized -X- _ O
input. -X- _ O

ELMo -X- _ O
(Peters -X- _ O
et -X- _ O
al., -X- _ O
2018), -X- _ O
a -X- _ O
bidirectional -X- _ O
LSTM -X- _ O
model, -X- _ O
applied -X- _ O
character -X- _ O
convolutions -X- _ O
to -X- _ O
its -X- _ O
whitespace-separated -X- _ O
input -X- _ O
tokens. -X- _ O

For -X- _ O
Transfer -X- _ O
Learning -X- _ O
Token-based -X- _ O
models -X- _ O
have -X- _ O
also -X- _ O
been -X- _ O
augmented -X- _ O
with -X- _ O
character-level -X- _ O
information -X- _ O
in -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
transfer -X- _ O
learning, -X- _ O
where -X- _ O
encoders -X- _ O
trained -X- _ O
with -X- _ O
unsupervised -X- _ O
objectives -X- _ O
are -X- _ O
repurposed -X- _ O
to -X- _ O
solve -X- _ O
downstream -X- _ O
tasks. -X- _ O

Character -X- _ O
information -X- _ O
proved -X- _ O
particularly -X- _ O
useful -X- _ O
for -X- _ O
low-resource -X- _ O
languages -X- _ O
(Xie -X- _ O
et -X- _ O
al., -X- _ O
2018), -X- _ O
phenomena -X- _ O
such -X- _ O
as -X- _ O
code-switching -X- _ O
and -X- _ O
transliteration -X- _ O
(Ball -X- _ O
and -X- _ O
Garrette, -X- _ O
2018), -X- _ O
and -X- _ O
rich -X- _ O
morphology -X- _ O
(Vania -X- _ O
and -X- _ O
Lopez, -X- _ O
2017), -X- _ O
previously -X- _ O
receiving -X- _ O
special -X- _ O
modeling -X- _ O
including -X- _ O
adaptor -X- _ O
grammars -X- _ O
(Botha -X- _ O
and -X- _ O
Blunsom, -X- _ O
2013). -X- _ O

Character -X- _ O
information -X- _ O
has -X- _ O
been -X- _ O
leveraged -X- _ O
for -X- _ O
many -X- _ O
other -X- _ O
end -X- _ O
tasks -X- _ O
as -X- _ O
well, -X- _ O
including: -X- _ O
text -X- _ O
classification -X- _ O
(Zhang -X- _ O
et -X- _ O
al., -X- _ O
2015; -X- _ O
Zhang -X- _ O
and -X- _ O
LeCun, -X- _ O
2017), -X- _ O
part-of-speech -X- _ O
tagging -X- _ O
and -X- _ O
NER -X- _ O
(Gillick -X- _ O
et -X- _ O
al., -X- _ O
2016; -X- _ O
Akbik -X- _ O
et -X- _ O
al., -X- _ O
2018; -X- _ O
Pinter -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
named -X- _ O
entity -X- _ O
detection -X- _ O
(Yu -X- _ O
et -X- _ O
al., -X- _ O
2018), -X- _ O
dependency -X- _ O
parsing -X- _ O
(Vania -X- _ O
et -X- _ O
al., -X- _ O
2018), -X- _ O
and -X- _ O
machine -X- _ O
reading -X- _ O
comprehension -X- _ O
(Hewlett -X- _ O
et -X- _ O
al., -X- _ O
2018). -X- _ O

CANINE -X- _ O
has -X- _ O
a -X- _ O
similar -X- _ O
motivation, -X- _ O
but -X- _ O
operates -X- _ O
in -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
pre-trained -X- _ O
transformers; -X- _ O
training -X- _ B-MetricName
is -X- _ O
7x -X- _ B-MetricValue
faster -X- _ O
compared -X- _ O
to -X- _ O
a -X- _ O
char-to-char -X- _ B-MethodName
baseline -X- _ I-MethodName
(on -X- _ O
TPU -X- _ O
v3), -X- _ O
and -X- _ O
has -X- _ O
a -X- _ O
28% -X- _ O
increase -X- _ O
in -X- _ O
training -X- _ O
time -X- _ O
over -X- _ O
mBERT -X- _ B-MethodName
(Table -X- _ O
2). -X- _ O

77 -X- _ O
Residual -X- _ O
Connections -X- _ O
While -X- _ O
the -X- _ O
initial -X- _ O
character -X- _ O
encoder -X- _ O
(before -X- _ O
downsampling) -X- _ O
and -X- _ O
final -X- _ O
character -X- _ O
encoder -X- _ O
(after -X- _ O
upsampling) -X- _ O
both -X- _ O
represent -X- _ O
character -X- _ O
positions, -X- _ O
they -X- _ O
conceptually -X- _ O
have -X- _ O
very -X- _ O
different -X- _ O
purposes -X- _ O
in -X- _ O
the -X- _ O
network. -X- _ O

19We -X- _ O
use -X- _ O
w -X- _ B-HyperparameterName
= -X- _ O
4 -X- _ B-HyperparameterValue
in -X- _ O
our -X- _ O
experiments. -X- _ O

where -X- _ O
⊕ -X- _ O
indicates -X- _ O
vector -X- _ O
concatenation -X- _ O
of -X- _ O
the -X- _ O
representations -X- _ O
(i.e., -X- _ O
not -X- _ O
sequences) -X- _ O
such -X- _ O
that -X- _ O
CONV -X- _ O
projects -X- _ O
from -X- _ O
Rn×2d -X- _ B-HyperparameterValue
back -X- _ O
to -X- _ O
Rn×d -X- _ B-HyperparameterValue
across -X- _ O
a -X- _ O
window -X- _ O
of -X- _ O
w -X- _ O
characters.19 -X- _ O
Applying -X- _ O
a -X- _ O
final -X- _ O
transformer -X- _ O
layer -X- _ O
(standard, -X- _ O
not -X- _ O
local) -X- _ O
yields -X- _ O
a -X- _ O
final -X- _ O
sequence -X- _ O
representation -X- _ O
yseq -X- _ O
∈ -X- _ O
Rn×d. -X- _ O

18In -X- _ O
our -X- _ O
experiments, -X- _ O
we -X- _ O
found -X- _ O
a -X- _ O
downsampling -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
of -X- _ O
4X -X- _ B-HyperparameterValue
to -X- _ O
in -X- _ O
high -X- _ O
quality -X- _ O
with -X- _ O
a -X- _ O
speed -X- _ O
comparable -X- _ O
to -X- _ O
BERT. -X- _ B-MethodName

In -X- _ O
our -X- _ O
experiments, -X- _ O
we -X- _ O
use -X- _ O
r -X- _ B-HyperparameterName
= -X- _ O
4 -X- _ B-HyperparameterValue
and -X- _ O
n -X- _ B-HyperparameterName
= -X- _ O
2048 -X- _ B-HyperparameterValue
such -X- _ O
that -X- _ O
16We -X- _ O
use -X- _ O
B -X- _ B-HyperparameterName
= -X- _ O
15k -X- _ B-HyperparameterValue
and -X- _ O
N -X- _ B-HyperparameterName
= -X- _ O
4 -X- _ B-HyperparameterValue
for -X- _ O
our -X- _ O
n-grams. -X- _ O

More -X- _ O
formally, -X- _ O
hinit -X- _ O
← -X- _ O
LOCALTRANSFORMER1(e) -X- _ O
hdown -X- _ O
← -X- _ O
STRIDEDCONV(hinit, -X- _ O
r) -X- _ O
hup -X- _ O
← -X- _ O
CONV -X- _ O
(hinit -X- _ O
⊕ -X- _ O
h(cid:6) -X- _ O
yseq -X- _ O
← -X- _ O
TRANSFORMER1(hup) -X- _ O
down, -X- _ O
w) -X- _ O
We -X- _ O
refer -X- _ O
to -X- _ O
this -X- _ O
output -X- _ O
as -X- _ O
the -X- _ O
downsampled -X- _ O
positions: -X- _ O
hdown -X- _ B-HyperparameterValue
∈ -X- _ O
Rm×d -X- _ O
where -X- _ O
m -X- _ B-HyperparameterValue
= -X- _ O
n/r -X- _ B-HyperparameterValue
is -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
downsampled -X- _ I-HyperparameterName
positions. -X- _ I-HyperparameterName

(Note -X- _ O
that -X- _ O
since -X- _ O
each -X- _ O
downsampled -X- _ O
position -X- _ O
is -X- _ O
associated -X- _ O
with -X- _ O
exactly -X- _ O
r -X- _ O
characters -X- _ O
for -X- _ O
a -X- _ O
downsampling -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
of -X- _ O
r, -X- _ B-HyperparameterValue
each -X- _ O
position -X- _ O
of -X- _ O
downsampled -X- _ O
representation -X- _ O
is -X- _ O
replicated -X- _ B-HyperparameterName
r -X- _ B-HyperparameterValue
times -X- _ O
before -X- _ O
concatenation.) -X- _ O

We -X- _ O
reconstruct -X- _ O
a -X- _ O
character-wise -X- _ O
output -X- _ O
representation -X- _ O
by -X- _ O
first -X- _ O
concatenating -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
original -X- _ O
character -X- _ O
transformer -X- _ O
(above) -X- _ O
with -X- _ O
the -X- _ O
downsampled -X- _ O
representation -X- _ O
produced -X- _ O
by -X- _ O
the -X- _ O
deep -X- _ O
transformer -X- _ O
stack. -X- _ O

Upsampling -X- _ O
While -X- _ O
the -X- _ O
above -X- _ O
architecture -X- _ O
is -X- _ O
sufficient -X- _ O
for -X- _ O
classification -X- _ O
tasks, -X- _ O
sequence -X- _ O
prediction -X- _ O
tasks -X- _ O
require -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
expose -X- _ O
an -X- _ O
output -X- _ O
layer -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
sequence -X- _ O
length -X- _ O
as -X- _ O
the -X- _ O
input -X- _ O
(i.e., -X- _ O
characters -X- _ O
are -X- _ O
the -X- _ O
model’s -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
‘‘API’’ -X- _ O
for -X- _ O
tasks -X- _ O
like -X- _ O
tagging -X- _ O
and -X- _ O
span -X- _ O
prediction). -X- _ O

9For -X- _ O
example, -X- _ O
should -X- _ O
a -X- _ O
subword -X- _ O
containing -X- _ O
an -X- _ O
unknown -X- _ O
character -X- _ O
be -X- _ O
a -X- _ O
separate -X- _ O
token, -X- _ O
or -X- _ O
should -X- _ O
the -X- _ O
unknown -X- _ O
character -X- _ O
be -X- _ O
separated -X- _ O
as -X- _ O
its -X- _ O
own -X- _ O
token? -X- _ O

8Vietnamese -X- _ O
uses -X- _ O
diacritics -X- _ O
to -X- _ O
indicate -X- _ O
tones—often -X- _ O
the -X- _ O
only -X- _ O
difference -X- _ O
among -X- _ O
several -X- _ O
unrelated -X- _ O
content -X- _ O
words. -X- _ O

7Spanish -X- _ O
past -X- _ O
tense -X- _ O
uses -X- _ O
an -X- _ O
accented -X- _ O
final -X- _ O
vowel. -X- _ O

However, -X- _ O
this -X- _ O
approach -X- _ O
would -X- _ O
in -X- _ O
far -X- _ O
more -X- _ O
sequence -X- _ O
positions -X- _ O
given -X- _ O
the -X- _ O
same -X- _ O
input -X- _ O
text, -X- _ O
leading -X- _ O
to -X- _ O
linearly -X- _ O
more -X- _ O
compute -X- _ O
in -X- _ O
feed -X- _ O
forward -X- _ O
layers -X- _ O
and -X- _ O
quadratically -X- _ O
more -X- _ O
compute -X- _ O
in -X- _ O
self-attention -X- _ O
layers. -X- _ O

down]0 -X- _ O
We -X- _ O
used -X- _ O
L -X- _ B-HyperparameterName
= -X- _ O
12 -X- _ B-HyperparameterValue
to -X- _ O
match -X- _ O
mBERT. -X- _ B-MethodName

This -X- _ O
is -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
the -X- _ O
core -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
derivative -X- _ O
models, -X- _ O
and -X- _ O
remains -X- _ O
the -X- _ O
core -X- _ O
of -X- _ O
CANINE -X- _ B-MethodName
in -X- _ O
that -X- _ O
it -X- _ O
accounts -X- _ O
for -X- _ O
the -X- _ O
vast -X- _ O
majority -X- _ O
of -X- _ O
its -X- _ O
compute -X- _ O
and -X- _ O
parameters, -X- _ O
though -X- _ O
we -X- _ O
note -X- _ O
that -X- _ O
this -X- _ O
middle -X- _ O
portion -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
could -X- _ O
easily -X- _ O
be -X- _ O
replaced -X- _ O
with -X- _ O
any -X- _ O
other -X- _ O
sequence-to-sequence -X- _ O
model -X- _ O
including -X- _ O
those -X- _ O
with -X- _ O
better -X- _ O
compute -X- _ O
performance -X- _ O
such -X- _ O
as -X- _ O
Performer -X- _ O
(Choromanski -X- _ O
et -X- _ O
al., -X- _ O
2021), -X- _ O
Big -X- _ O
Bird -X- _ O
(Zaheer -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
RFA -X- _ O
(Peng -X- _ O
et -X- _ O
al., -X- _ O
2021), -X- _ O
ETC -X- _ O
(Ainslie -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
and -X- _ O
so -X- _ O
on. -X- _ O

Deep -X- _ O
Transformer -X- _ O
Stack -X- _ O
After -X- _ O
downsampling, -X- _ O
CANINE -X- _ B-MethodName
applies -X- _ O
a -X- _ O
deep -X- _ O
transformer -X- _ O
stack -X- _ O
with -X- _ O
L -X- _ B-HyperparameterValue
layers -X- _ O
to -X- _ O
the -X- _ O
resulting -X- _ O
downsampled -X- _ O
positions. -X- _ O

Next, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
strided -X- _ O
convolution -X- _ O
to -X- _ O
reduce -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
sequence -X- _ O
positions -X- _ O
to -X- _ O
be -X- _ O
similar -X- _ O
to -X- _ O
that -X- _ O
of -X- _ O
a -X- _ O
word -X- _ O
piece -X- _ O
model.18 -X- _ O
Given -X- _ O
character -X- _ O
embeddings -X- _ O
e -X- _ O
∈ -X- _ O
Rn×d -X- _ O
with -X- _ O
a -X- _ O
sequence -X- _ B-HyperparameterName
length -X- _ I-HyperparameterName
of -X- _ O
n -X- _ B-HyperparameterValue
characters -X- _ O
and -X- _ B-HyperparameterName
dimensionality -X- _ I-HyperparameterName
d, -X- _ B-HyperparameterValue
we -X- _ O
use -X- _ O
a -X- _ O
convolution -X- _ O
with -X- _ O
a -X- _ O
stride -X- _ B-HyperparameterName
of -X- _ O
r -X- _ B-HyperparameterValue
to -X- _ O
downsample -X- _ O
the -X- _ O
sequence: -X- _ O
m -X- _ B-HyperparameterName
= -X- _ O
512, -X- _ B-HyperparameterValue
giving -X- _ O
CANINE’s -X- _ B-MethodName
primary -X- _ O
encoder—the -X- _ O
transformer -X- _ O
stack—the -X- _ O
same -X- _ O
length -X- _ O
as -X- _ O
in -X- _ O
mBERT. -X- _ B-MethodName

This -X- _ O
model -X- _ O
performs -X- _ O
self-attention -X- _ O
only -X- _ O
within -X- _ O
each -X- _ O
block -X- _ O
of -X- _ O
a -X- _ O
pre-defined -X- _ O
size,17 -X- _ O
saving -X- _ O
the -X- _ O
quadratic -X- _ O
cost -X- _ O
of -X- _ O
attention -X- _ O
while -X- _ O
leveraging -X- _ O
the -X- _ O
linguistic -X- _ O
intuition -X- _ O
that -X- _ O
word -X- _ O
composition—that -X- _ O
is, -X- _ O
the -X- _ O
kind -X- _ O
of -X- _ O
composition -X- _ O
relevant -X- _ O
in -X- _ O
the -X- _ O
lowest -X- _ O
layers -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
(Tenney -X- _ O
et -X- _ O
al., -X- _ O
2019)—tends -X- _ O
to -X- _ O
happen -X- _ O
at -X- _ O
a -X- _ O
very -X- _ O
local -X- _ O
level. -X- _ O

First, -X- _ O
we -X- _ O
encode -X- _ O
characters -X- _ O
using -X- _ O
a -X- _ O
single-layer -X- _ O
block-wise -X- _ O
local -X- _ O
attention -X- _ O
transformer. -X- _ O

Downsampling -X- _ O
To -X- _ O
make -X- _ O
CANINE -X- _ B-MethodName
efficient, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
multi-part -X- _ O
downsampling -X- _ O
strategy. -X- _ O

Notably, -X- _ O
it -X- _ O
also -X- _ O
allows -X- _ O
the -X- _ O
model’s -X- _ O
input -X- _ O
signature -X- _ O
to -X- _ O
remain -X- _ O
a -X- _ O
simple -X- _ O
sequence -X- _ O
of -X- _ O
codepoints. -X- _ O

This -X- _ O
formulation -X- _ O
still -X- _ O
admits -X- _ O
tokenization-free -X- _ B-TaskName
modeling, -X- _ I-TaskName
but -X- _ O
provides -X- _ O
the -X- _ O
model -X- _ O
with -X- _ O
an -X- _ O
inductive -X- _ O
bias -X- _ O
that -X- _ O
favors -X- _ O
slightly -X- _ O
more -X- _ O
memorization -X- _ O
via -X- _ O
a -X- _ O
compute-cheap -X- _ O
means -X- _ O
of -X- _ O
adding -X- _ O
parameters. -X- _ O

The -X- _ O
simplest -X- _ O
implementation -X- _ O
of -X- _ O
such -X- _ O
a -X- _ O
character -X- _ O
model -X- _ O
would -X- _ O
be -X- _ O
to -X- _ O
feed -X- _ O
characters -X- _ O
at -X- _ O
each -X- _ O
position -X- _ O
in -X- _ O
place -X- _ O
of -X- _ O
subwords. -X- _ O

3.1 -X- _ O
Model -X- _ O
CANINE -X- _ B-MethodName
is -X- _ O
designed -X- _ O
to -X- _ O
be -X- _ O
a -X- _ O
minimally -X- _ O
modified -X- _ O
variant -X- _ O
of -X- _ O
the -X- _ O
deep -X- _ O
transformer -X- _ O
stack -X- _ O
found -X- _ O
in -X- _ O
modern -X- _ O
encoders -X- _ O
such -X- _ O
as -X- _ O
GPT, -X- _ O
(m)BERT, -X- _ O
XLM, -X- _ O
and -X- _ O
XLM-R -X- _ O
such -X- _ O
that -X- _ O
its -X- _ O
architecture -X- _ O
is -X- _ O
easily -X- _ O
adoptable -X- _ O
by -X- _ O
other -X- _ O
models -X- _ O
in -X- _ O
this -X- _ O
family. -X- _ O

3 -X- _ O
CANINE -X- _ B-MethodName
CANINE -X- _ B-MethodName
consists -X- _ O
of -X- _ O
three -X- _ O
primary -X- _ O
components: -X- _ O
(1) -X- _ O
a -X- _ O
vocabulary-free -X- _ O
technique -X- _ O
for -X- _ O
embedding -X- _ O
text; -X- _ O
(2) -X- _ O
a -X- _ O
character-level -X- _ O
model -X- _ O
that -X- _ O
is -X- _ O
efficient -X- _ O
by -X- _ O
means -X- _ O
of -X- _ O
downsampling -X- _ O
and -X- _ O
upsampling; -X- _ O
and -X- _ O
(3) -X- _ O
an -X- _ O
effective -X- _ O
means -X- _ O
of -X- _ O
performing -X- _ O
masked -X- _ B-TaskName
language -X- _ I-TaskName
modeling -X- _ I-TaskName
on -X- _ O
a -X- _ O
character-level -X- _ O
model. -X- _ O

Yet -X- _ O
this -X- _ O
same -X- _ O
pre-training -X- _ O
paradigm -X- _ O
presents -X- _ O
an -X- _ O
advantage -X- _ O
for -X- _ O
character -X- _ O
models: -X- _ O
access -X- _ O
to -X- _ O
far -X- _ O
more -X- _ O
(unsupervised) -X- _ O
data -X- _ O
to -X- _ O
learn -X- _ O
word -X- _ O
composition -X- _ O
from -X- _ O
characters; -X- _ O
without -X- _ O
transfer -X- _ O
learning, -X- _ O
this -X- _ O
has -X- _ O
historically -X- _ O
been -X- _ O
impractical -X- _ O
for -X- _ O
many -X- _ O
tasks -X- _ O
having -X- _ O
little -X- _ O
supervised -X- _ O
data. -X- _ O

2.3 -X- _ O
Reducing -X- _ B-TaskName
Engineering -X- _ I-TaskName
Effort -X- _ I-TaskName
Mature -X- _ I-TaskName
tokenizers -X- _ I-TaskName
often -X- _ O
include -X- _ O
years -X- _ O
of -X- _ O
handengineered -X- _ O
rules -X- _ O
around -X- _ O
special -X- _ O
cases -X- _ O
such -X- _ O
as -X- _ O
email -X- _ O
addresses, -X- _ O
URLs, -X- _ O
and -X- _ O
handling -X- _ O
unknown -X- _ O
words;9 -X- _ O
even -X- _ O
fairly -X- _ O
minimal -X- _ O
modern -X- _ O
tokenizers -X- _ O
include -X- _ O
initial -X- _ O
word-splitting -X- _ O
heuristics -X- _ O
followed -X- _ O
by -X- _ O
a -X- _ O
specific -X- _ O
algorithm -X- _ O
and -X- _ O
vocabulary -X- _ O
for -X- _ O
further -X- _ O
breaking -X- _ O
these -X- _ O
tokens -X- _ O
into -X- _ O
subwords. -X- _ O

All -X- _ O
of -X- _ O
these -X- _ O
things -X- _ O
introduce -X- _ O
ample -X- _ O
opportunities -X- _ O
for -X- _ O
mismatches -X- _ O
to -X- _ O
arise -X- _ O
between -X- _ O
tokenization -X- _ B-TaskName
and -X- _ O
the -X- _ O
vocabulary -X- _ O
from -X- _ O
pre-training. -X- _ O

Modern -X- _ O
pre-trained -X- _ O
models -X- _ O
also -X- _ O
have -X- _ O
many -X- _ O
requirements -X- _ O
throughout -X- _ O
their -X- _ O
lifecycle: -X- _ O
Between -X- _ O
the -X- _ O
time -X- _ O
a -X- _ O
model -X- _ O
is -X- _ O
pre-trained, -X- _ O
fine-tuned, -X- _ O
and -X- _ O
served—potentially -X- _ O
months -X- _ O
or -X- _ O
years -X- _ O
apart—its -X- _ O
weights -X- _ O
and -X- _ O
model -X- _ O
implementation -X- _ O
may -X- _ O
be -X- _ O
converted -X- _ O
to -X- _ O
be -X- _ O
compatible -X- _ O
with -X- _ O
another -X- _ O
toolkit, -X- _ O
its -X- _ O
fine-tuning -X- _ O
data -X- _ O
may -X- _ O
be -X- _ O
tokenized -X- _ O
in -X- _ O
a -X- _ O
different -X- _ O
way, -X- _ O
and -X- _ O
the -X- _ O
natural -X- _ O
distribution -X- _ O
of -X- _ O
words -X- _ O
may -X- _ O
be -X- _ O
quite -X- _ O
different. -X- _ O

Embeddings -X- _ O
that -X- _ O
are -X- _ O
rarely -X- _ O
touched -X- _ O
during -X- _ O
pre-training -X- _ O
will -X- _ O
not -X- _ O
be -X- _ O
updated -X- _ O
much -X- _ O
beyond -X- _ O
their -X- _ O
random -X- _ O
initializations. -X- _ O

Hypothetically, -X- _ O
a -X- _ O
model -X- _ O
may -X- _ O
estimate -X- _ O
a -X- _ O
very -X- _ O
good -X- _ O
embedding -X- _ O
for -X- _ O
a -X- _ O
common -X- _ O
vocabulary -X- _ O
element -X- _ O
kitten, -X- _ O
but -X- _ O
a -X- _ O
poor -X- _ O
embedding -X- _ O
for -X- _ O
the -X- _ O
less -X- _ O
frequent -X- _ O
element -X- _ O
kittens -X- _ O
since -X- _ O
the -X- _ O
model -X- _ O
has -X- _ O
no -X- _ O
a -X- _ O
priori -X- _ O
knowledge -X- _ O
that -X- _ O
they -X- _ O
are -X- _ O
related. -X- _ O

Practically, -X- _ O
generalization -X- _ O
is -X- _ O
hindered -X- _ O
for -X- _ O
vocabulary -X- _ O
elements -X- _ O
that -X- _ O
are -X- _ O
slight -X- _ O
orthographic -X- _ O
variations, -X- _ O
where -X- _ O
one -X- _ O
is -X- _ O
very -X- _ O
infrequent. -X- _ O

By -X- _ O
building -X- _ O
a -X- _ O
model -X- _ O
that -X- _ O
directly -X- _ O
engages -X- _ O
with -X- _ O
these -X- _ O
issues -X- _ O
within -X- _ O
the -X- _ O
small -X- _ O
scale -X- _ O
of -X- _ O
word -X- _ O
composition, -X- _ O
we -X- _ O
hope -X- _ O
to -X- _ O
enable -X- _ O
future -X- _ O
work -X- _ O
studying -X- _ O
these -X- _ O
problems -X- _ O
at -X- _ O
larger -X- _ O
scales -X- _ O
such -X- _ O
as -X- _ O
phrasal -X- _ O
constructions. -X- _ O

Large -X- _ O
frequency-derived -X- _ O
vocabularies -X- _ O
partially -X- _ O
mitigate -X- _ O
this -X- _ O
problem -X- _ O
by -X- _ O
simply -X- _ O
memorizing -X- _ O
more, -X- _ O
but -X- _ O
language -X- _ O
inherently -X- _ O
requires -X- _ O
aspects -X- _ O
of -X- _ O
both -X- _ O
memorization -X- _ O
and -X- _ O
composition. -X- _ O

In -X- _ O
terms -X- _ O
of -X- _ O
scientific -X- _ O
inquiry, -X- _ O
we -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
know -X- _ O
whether -X- _ O
we -X- _ O
can -X- _ O
build -X- _ O
models -X- _ O
that -X- _ O
learn -X- _ O
how -X- _ O
to -X- _ O
compose -X- _ O
words -X- _ O
where -X- _ O
appropriate, -X- _ O
and -X- _ O
memorize -X- _ O
them -X- _ O
where -X- _ O
memorization -X- _ O
is -X- _ O
needed. -X- _ O

With -X- _ O
this -X- _ O
in -X- _ O
mind, -X- _ O
we -X- _ O
seek -X- _ O
an -X- _ O
approach -X- _ O
that -X- _ O
can -X- _ O
better -X- _ O
generalize -X- _ O
beyond -X- _ O
the -X- _ O
orthographic -X- _ O
forms -X- _ O
encountered -X- _ O
during -X- _ O
pre-training. -X- _ O

For -X- _ O
instance, -X- _ O
mBERT -X- _ B-MethodName
initially -X- _ O
removed -X- _ O
all -X- _ O
diacritics, -X- _ O
thus -X- _ O
dropping -X- _ O
tense -X- _ O
information -X- _ O
in -X- _ O
Spanish7 -X- _ O
and -X- _ O
conflating -X- _ O
many -X- _ O
unrelated -X- _ O
words -X- _ O
in -X- _ O
Vietnamese.8 -X- _ O
Finally, -X- _ O
using -X- _ O
a -X- _ O
fixed -X- _ O
vocabulary -X- _ O
during -X- _ O
pretraining -X- _ O
also -X- _ O
creates -X- _ O
complications -X- _ O
for -X- _ O
downstream -X- _ O
tasks, -X- _ O
which -X- _ O
are -X- _ O
subsequently -X- _ O
tied -X- _ O
to -X- _ O
the -X- _ O
same -X- _ O
tokenizer -X- _ O
and -X- _ O
vocabulary -X- _ O
used -X- _ O
for -X- _ O
pretraining, -X- _ O
even -X- _ O
if -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
well-suited -X- _ O
for -X- _ O
the -X- _ O
target -X- _ O
domain -X- _ O
and/or -X- _ O
end-task. -X- _ O

6Informal -X- _ O
Twi -X- _ O
uses -X- _ O
a -X- _ O
right -X- _ O
paren -X- _ O
) -X- _ O
to -X- _ O
represent -X- _ O
the -X- _ O
letter -X- _ O
. -X- _ O

5Hawaiian -X- _ O
uses -X- _ O
an -X- _ O
apostrophe -X- _ O
to -X- _ O
indicate -X- _ O
a -X- _ O
glottal -X- _ O
stop. -X- _ O

4For -X- _ O
example, -X- _ O
Spanish -X- _ O
speakers -X- _ O
may -X- _ O
drop -X- _ O
accents -X- _ O
when -X- _ O
typing. -X- _ O

Fixed -X- _ O
vocabulary -X- _ O
can -X- _ O
also -X- _ O
force -X- _ O
modelers -X- _ O
to -X- _ O
choose -X- _ O
between -X- _ O
difficult -X- _ O
preprocessing -X- _ O
tradeoffs: -X- _ O
Should -X- _ O
one -X- _ O
keep -X- _ O
accents, -X- _ O
casing, -X- _ O
and -X- _ O
so -X- _ O
forth, -X- _ O
and -X- _ O
avoid -X- _ O
destructive -X- _ O
preprocessing?—Or -X- _ O
3From -X- _ O
en.wikipedia.org/wiki/Arabic -X- _ O
verbs. -X- _ O

While -X- _ O
SentencePiece -X- _ B-MethodName
does -X- _ O
offer -X- _ O
the -X- _ O
option -X- _ O
to -X- _ O
skip -X- _ O
whitespace -X- _ O
splitting, -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
typically -X- _ O
used -X- _ O
due -X- _ O
to -X- _ O
poor -X- _ O
empirical -X- _ O
performance. -X- _ O

These -X- _ O
algorithms -X- _ O
are -X- _ O
limited -X- _ O
to -X- _ O
only -X- _ O
simple -X- _ O
word-splitting -X- _ O
operations. -X- _ O

This -X- _ O
covers -X- _ O
154 -X- _ O
scripts -X- _ O
and -X- _ O
over -X- _ O
900 -X- _ O
languages. -X- _ O

2Unicode -X- _ O
defines -X- _ O
1,114,112 -X- _ O
total -X- _ O
codepoints, -X- _ O
of -X- _ O
which -X- _ O
only -X- _ O
143,698 -X- _ O
are -X- _ O
assigned -X- _ O
to -X- _ O
characters -X- _ O
as -X- _ O
of -X- _ O
Unicode -X- _ O
13.0. -X- _ O

al., -X- _ O
1We -X- _ O
consider -X- _ O
splitting -X- _ O
on -X- _ O
Unicode -X- _ O
characters -X- _ O
to -X- _ O
be -X- _ O
tokenization-free -X- _ O
because -X- _ O
it -X- _ O
depends -X- _ O
only -X- _ O
on -X- _ O
the -X- _ O
(deterministic) -X- _ O
process -X- _ O
defined -X- _ O
by -X- _ O
the -X- _ O
Unicode -X- _ O
standard, -X- _ O
and -X- _ O
not -X- _ O
on -X- _ O
any -X- _ O
models, -X- _ O
hand-crafted -X- _ O
rules, -X- _ O
or -X- _ O
other -X- _ O
linguistic -X- _ O
knowledge. -X- _ O

In -X- _ O
this -X- _ O
article, -X- _ O
we -X- _ O
contribute: -X- _ O
• -X- _ O
the -X- _ O
first -X- _ O
pre-trained -X- _ O
tokenization-free -X- _ O
deep -X- _ O
encoder; -X- _ O
• -X- _ O
an -X- _ O
efficient -X- _ O
model -X- _ O
architecture -X- _ O
that -X- _ O
directly -X- _ O
encodes -X- _ O
long -X- _ O
sequences -X- _ O
of -X- _ O
characters -X- _ O
with -X- _ O
speed -X- _ O
comparable -X- _ O
to -X- _ O
vanilla -X- _ B-MethodName
BERT; -X- _ I-MethodName
and -X- _ O
• -X- _ O
a -X- _ O
model -X- _ O
that -X- _ O
performs -X- _ O
no -X- _ O
tokenization -X- _ O
on -X- _ O
the -X- _ O
input, -X- _ O
avoiding -X- _ O
the -X- _ O
lossy -X- _ O
information -X- _ O
bottleneck -X- _ O
associated -X- _ O
with -X- _ O
most -X- _ O
pre-processing. -X- _ O

This -X- _ O
effectively -X- _ O
converts -X- _ O
the -X- _ O
hard -X- _ O
constraint -X- _ O
of -X- _ O
token -X- _ O
boundaries -X- _ O
found -X- _ O
in -X- _ O
other -X- _ O
models -X- _ O
into -X- _ O
a -X- _ O
soft -X- _ O
inductive -X- _ O
bias -X- _ O
in -X- _ O
CANINE. -X- _ B-MethodName

Critically, -X- _ O
this -X- _ O
tokenization -X- _ O
is -X- _ O
used -X- _ O
only -X- _ O
for -X- _ O
the -X- _ O
pre-training -X- _ O
loss; -X- _ O
tokens -X- _ O
are -X- _ O
never -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
encoder, -X- _ O
and -X- _ O
the -X- _ O
tokenizer -X- _ O
and -X- _ O
subword -X- _ O
vocabulary -X- _ O
can -X- _ O
be -X- _ O
safely -X- _ O
discarded -X- _ O
after -X- _ O
pretraining. -X- _ O

2. -X- _ O
A -X- _ O
vocabulary-based -X- _ O
loss -X- _ O
that -X- _ O
predicts -X- _ B-TaskName
the -X- _ I-TaskName
identities -X- _ I-TaskName
of -X- _ I-TaskName
masked -X- _ I-TaskName
subword -X- _ I-TaskName
tokens. -X- _ I-TaskName

1. -X- _ O
A -X- _ O
fully -X- _ O
character-level -X- _ O
loss -X- _ O
that -X- _ O
autoregressively -X- _ B-TaskName
predicts -X- _ I-TaskName
characters -X- _ I-TaskName
in -X- _ I-TaskName
masked -X- _ I-TaskName
spans. -X- _ I-TaskName

The -X- _ O
root -X- _ O
is -X- _ O
therefore -X- _ O
not -X- _ O
separable -X- _ O
from -X- _ O
its -X- _ O
inflection -X- _ O
via -X- _ O
any -X- _ O
contiguous -X- _ O
split. -X- _ O

For -X- _ O
the -X- _ O
MLM -X- _ B-TaskName
task, -X- _ O
CANINE -X- _ B-MethodName
offers -X- _ O
two -X- _ O
options: -X- _ O
k-t-b -X- _ O
‘‘write’’ -X- _ O
(root -X- _ O
form) -X- _ O
kataba -X- _ O
‘‘he -X- _ O
wrote’’ -X- _ O
kattaba -X- _ O
‘‘he -X- _ O
made -X- _ O
(someone) -X- _ O
write’’ -X- _ O
iktataba -X- _ O
‘‘he -X- _ O
signed -X- _ O
up’’ -X- _ O
Table -X- _ O
1: -X- _ O
Non-concatenative -X- _ O
morphology -X- _ O
in -X- _ O
Arabic.3 -X- _ O
When -X- _ O
conjugating, -X- _ O
letters -X- _ O
are -X- _ O
interleaved -X- _ O
within -X- _ O
the -X- _ O
root. -X- _ O

Like -X- _ O
BERT, -X- _ B-MethodName
we -X- _ O
pre-train -X- _ O
CANINE -X- _ B-MethodName
on -X- _ O
the -X- _ O
Masked -X- _ B-TaskName
Language -X- _ I-TaskName
Model -X- _ I-TaskName
(MLM) -X- _ B-TaskName
and -X- _ O
Next -X- _ B-TaskName
Sentence -X- _ I-TaskName
Prediction -X- _ I-TaskName
(NSP) -X- _ B-TaskName
tasks. -X- _ O

To -X- _ O
avoid -X- _ O
the -X- _ O
slowdown -X- _ O
from -X- _ O
increasing -X- _ O
the -X- _ O
sequence -X- _ O
length, -X- _ O
CANINE -X- _ B-MethodName
uses -X- _ O
strided -X- _ O
convolutions -X- _ O
to -X- _ O
downsample -X- _ O
input -X- _ O
sequences -X- _ O
to -X- _ O
a -X- _ O
shorter -X- _ O
length -X- _ O
before -X- _ O
the -X- _ O
deep -X- _ O
transformer -X- _ O
stack. -X- _ O

model -X- _ O
are -X- _ O
sequences -X- _ O
of -X- _ O
Unicode -X- _ O
characters.1 -X- _ O
To -X- _ O
represent -X- _ O
the -X- _ O
full -X- _ O
space -X- _ O
of -X- _ O
Unicode -X- _ O
characters2 -X- _ O
without -X- _ O
a -X- _ O
vocabulary, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
hashing -X- _ O
strategy. -X- _ O

Inputs -X- _ O
to -X- _ O
the -X- _ O
Transactions -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics, -X- _ O
vol. -X- _ O
10, -X- _ O
pp. -X- _ O

CANINE -X- _ B-MethodName
is -X- _ O
a -X- _ O
large -X- _ O
language -X- _ O
encoder -X- _ O
with -X- _ O
a -X- _ O
deep -X- _ O
transformer -X- _ O
stack -X- _ O
at -X- _ O
its -X- _ O
core. -X- _ O

In -X- _ O
order -X- _ O
to -X- _ O
enable -X- _ O
tokenization-free -X- _ B-TaskName
modeling -X- _ I-TaskName
that -X- _ O
overcomes -X- _ O
these -X- _ O
obstacles, -X- _ O
we -X- _ O
present -X- _ O
CANINE. -X- _ B-MethodName

Second, -X- _ O
simply -X- _ O
switching -X- _ O
to -X- _ O
a -X- _ O
character -X- _ O
vocabulary -X- _ O
yields -X- _ O
empirically -X- _ O
poor -X- _ O
(§4.2). -X- _ O

Since -X- _ O
standard -X- _ O
subword -X- _ O
models -X- _ O
have -X- _ O
roughly -X- _ O
four -X- _ O
characters -X- _ O
per -X- _ O
subword -X- _ O
on -X- _ O
average, -X- _ O
the -X- _ O
4x -X- _ O
increase -X- _ O
in -X- _ O
input -X- _ O
sequence -X- _ O
length -X- _ O
would -X- _ O
is -X- _ O
a -X- _ O
significantly -X- _ O
slower -X- _ O
model. -X- _ O

First, -X- _ O
the -X- _ O
computational -X- _ O
complexity -X- _ O
of -X- _ O
a -X- _ O
transformer -X- _ O
(Vaswani -X- _ O
et -X- _ O
al., -X- _ O
2017), -X- _ O
the -X- _ O
main -X- _ O
component -X- _ O
in -X- _ O
BERT -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
other -X- _ O
models -X- _ O
such -X- _ O
as -X- _ O
GPT -X- _ O
(Radford -X- _ O
et -X- _ O
al., -X- _ O
2019; -X- _ O
Brown -X- _ O
et -X- _ O
al., -X- _ O
2020) -X- _ O
and -X- _ O
T5 -X- _ O
(Raffel -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
grows -X- _ O
quadratically -X- _ O
with -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
input. -X- _ O

While -X- _ O
this -X- _ O
change -X- _ O
is -X- _ O
conceptually -X- _ O
very -X- _ O
simple—one -X- _ O
could -X- _ O
replace -X- _ O
the -X- _ O
subword -X- _ O
vocabulary -X- _ O
in -X- _ O
a -X- _ O
model -X- _ O
like -X- _ O
BERT -X- _ B-MethodName
(Devlin -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
with -X- _ O
a -X- _ O
vocabulary -X- _ O
made -X- _ O
solely -X- _ O
of -X- _ O
individual -X- _ O
characters—doing -X- _ O
so -X- _ O
leads -X- _ O
to -X- _ O
two -X- _ O
immediate -X- _ O
problems. -X- _ O

In -X- _ O
contrast, -X- _ O
an -X- _ O
end-to-end -X- _ O
model -X- _ O
that -X- _ O
operates -X- _ O
directly -X- _ O
on -X- _ O
raw -X- _ O
text -X- _ O
strings -X- _ O
would -X- _ O
avoid -X- _ O
these -X- _ O
issues, -X- _ O
instead -X- _ O
learning -X- _ O
to -X- _ O
compose -X- _ O
individual -X- _ O
characters -X- _ O
into -X- _ O
its -X- _ O
own -X- _ O
arbitrarily -X- _ O
complex -X- _ O
features, -X- _ O
with -X- _ O
potential -X- _ O
benefits -X- _ O
for -X- _ O
both -X- _ O
accuracy -X- _ O
and -X- _ O
ease -X- _ O
of -X- _ O
use. -X- _ O

The -X- _ O
degree -X- _ O
of -X- _ O
sophistication -X- _ O
required -X- _ O
to -X- _ O
accurately -X- _ O
capture -X- _ O
the -X- _ O
full -X- _ O
breadth -X- _ O
of -X- _ O
linguistic -X- _ O
phenomena, -X- _ O
along -X- _ O
with -X- _ O
the -X- _ O
infeasibility -X- _ O
of -X- _ O
writing -X- _ O
such -X- _ O
rules -X- _ O
by -X- _ O
hand -X- _ O
across -X- _ O
all -X- _ O
languages -X- _ O
and -X- _ O
domains, -X- _ O
suggests -X- _ O
that -X- _ O
explicit -X- _ O
tokenization -X- _ O
itself -X- _ O
is -X- _ O
problematic. -X- _ O

73 -X- _ O
which -X- _ O
are -X- _ O
costly, -X- _ O
requiring -X- _ O
both -X- _ O
manual -X- _ O
feature -X- _ O
engineering -X- _ O
and -X- _ O
linguistic -X- _ O
expertise, -X- _ O
or -X- _ O
datadriven -X- _ O
algorithms -X- _ O
such -X- _ O
as -X- _ O
Byte -X- _ O
Pair -X- _ O
Encoding -X- _ O
(Sennrich -X- _ O
et -X- _ O
al., -X- _ O
2016), -X- _ O
WordPiece -X- _ O
(Wu -X- _ O
et -X- _ O
al., -X- _ O
2016), -X- _ O
or -X- _ O
SentencePiece -X- _ O
(Kudo -X- _ O
and -X- _ O
Richardson, -X- _ O
2018) -X- _ O
that -X- _ O
split -X- _ O
strings -X- _ O
based -X- _ O
on -X- _ O
frequencies -X- _ O
in -X- _ O
a -X- _ O
corpus, -X- _ O
which -X- _ O
are -X- _ O
less -X- _ O
brittle -X- _ O
and -X- _ O
easier -X- _ O
to -X- _ O
scale, -X- _ O
but -X- _ O
are -X- _ O
ultimately -X- _ O
too -X- _ O
simplistic -X- _ O
to -X- _ O
properly -X- _ O
handle -X- _ O
the -X- _ O
wide -X- _ O
range -X- _ O
of -X- _ O
linguistic -X- _ O
phenomena -X- _ O
that -X- _ O
can’t -X- _ O
be -X- _ O
captured -X- _ O
by -X- _ O
mere -X- _ O
string-splitting -X- _ O
(§2.1). -X- _ O

Broadly -X- _ O
speaking, -X- _ O
tokenizers -X- _ O
are -X- _ O
generally -X- _ O
either -X- _ O
carefully -X- _ O
constructed -X- _ O
systems -X- _ O
of -X- _ O
language-specific -X- _ O
rules, -X- _ O
CANINE: -X- _ B-MethodName
Character -X- _ B-MethodName
Architecture -X- _ I-MethodName
with -X- _ I-MethodName
No -X- _ I-MethodName
tokenization -X- _ I-MethodName
In -X- _ I-MethodName
Neural -X- _ I-MethodName
Encoders. -X- _ I-MethodName

Tokenization, -X- _ O
however, -X- _ O
is -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
few -X- _ O
holdovers -X- _ O
from -X- _ O
that -X- _ O
era, -X- _ O
with -X- _ O
nearly -X- _ O
all -X- _ O
commonly -X- _ O
used -X- _ O
models -X- _ O
today -X- _ O
requiring -X- _ O
an -X- _ O
explicit -X- _ O
preprocessing -X- _ O
stage -X- _ O
to -X- _ O
segment -X- _ O
a -X- _ O
raw -X- _ O
text -X- _ O
string -X- _ O
into -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
discrete -X- _ O
model -X- _ O
inputs. -X- _ O

1 -X- _ O
End-to-end -X- _ O
neural -X- _ O
models -X- _ O
have -X- _ O
generally -X- _ O
replaced -X- _ O
the -X- _ O
traditional -X- _ O
NLP -X- _ O
pipeline, -X- _ O
and -X- _ O
with -X- _ O
it, -X- _ O
the -X- _ O
error -X- _ O
cascades -X- _ O
and -X- _ O
feature -X- _ O
engineering -X- _ O
common -X- _ O
to -X- _ O
such -X- _ O
systems, -X- _ O
preferring -X- _ O
instead -X- _ O
to -X- _ O
let -X- _ O
the -X- _ O
model -X- _ O
automatically -X- _ O
induce -X- _ O
its -X- _ O
own -X- _ O
sophisticated -X- _ O
representations. -X- _ O

CANINE -X- _ B-MethodName
outperforms -X- _ O
a -X- _ O
comparable -X- _ O
mBERT -X- _ B-MethodName
model -X- _ O
by -X- _ O
5.7 -X- _ B-MetricValue
F1 -X- _ B-MetricName
on -X- _ O
TYDI -X- _ B-DatasetName
QA, -X- _ I-DatasetName
a -X- _ O
challenging -X- _ O
multilingual -X- _ O
benchmark, -X- _ O
despite -X- _ O
having -X- _ O
fewer -X- _ O
model -X- _ O
parameters. -X- _ O

To -X- _ O
use -X- _ O
its -X- _ O
finer-grained -X- _ O
input -X- _ O
effectively -X- _ O
and -X- _ O
efficiently, -X- _ O
CANINE -X- _ B-MethodName
combines -X- _ O
downsampling, -X- _ O
which -X- _ O
reduces -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
length, -X- _ O
with -X- _ O
a -X- _ O
deep -X- _ O
transformer -X- _ O
stack, -X- _ O
which -X- _ O
encodes -X- _ O
context. -X- _ O

In -X- _ O
this -X- _ O
paper, -X- _ O
we -X- _ O
present -X- _ O
CANINE, -X- _ B-MethodName
a -X- _ O
neural -X- _ O
encoder -X- _ O
that -X- _ O
operates -X- _ O
directly -X- _ O
on -X- _ O
character -X- _ O
sequences—without -X- _ O
explicit -X- _ O
tokenization -X- _ O
or -X- _ O
vocabulary—and -X- _ O
a -X- _ O
pre-training -X- _ O
strategy -X- _ O
that -X- _ O
operates -X- _ O
either -X- _ O
directly -X- _ O
on -X- _ O
characters -X- _ O
or -X- _ O
optionally -X- _ O
uses -X- _ O
subwords -X- _ O
as -X- _ O
a -X- _ O
soft -X- _ O
inductive -X- _ O
bias. -X- _ O

While -X- _ O
recent -X- _ O
tokenization -X- _ O
approaches -X- _ O
based -X- _ O
on -X- _ O
data-derived -X- _ O
subword -X- _ O
lexicons -X- _ O
are -X- _ O
less -X- _ O
brittle -X- _ O
than -X- _ O
manually -X- _ O
engineered -X- _ O
tokenizers, -X- _ O
these -X- _ O
techniques -X- _ O
are -X- _ O
not -X- _ O
equally -X- _ O
suited -X- _ O
to -X- _ O
all -X- _ O
languages, -X- _ O
and -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
any -X- _ O
fixed -X- _ O
vocabulary -X- _ O
may -X- _ O
limit -X- _ O
a -X- _ O
model’s -X- _ O
ability -X- _ O
to -X- _ O
adapt. -X- _ O

CANINE: -X- _ B-MethodName
Pre-training -X- _ O
an -X- _ O
Efficient -X- _ O
Tokenization-Free -X- _ O
Encoder -X- _ O
for -X- _ O
Language -X- _ B-TaskName
Representation -X- _ I-TaskName
Jonathan -X- _ O
H. -X- _ O
Clark, -X- _ O
Dan -X- _ O
Garrette, -X- _ O
Iulia -X- _ O
Turc, -X- _ O
John -X- _ O
Wieting -X- _ O
Google -X- _ O
Research, -X- _ O
USA -X- _ O
{jhclark,dhgarrette,iuliaturc,jwieting}@google.com -X- _ O
Pipelined -X- _ O
NLP -X- _ O
systems -X- _ O
have -X- _ O
largely -X- _ O
been -X- _ O
superseded -X- _ O
by -X- _ O
end-to-end -X- _ O
neural -X- _ O
modeling, -X- _ O
yet -X- _ O
nearly -X- _ O
all -X- _ O
commonly -X- _ O
used -X- _ O
models -X- _ O
still -X- _ O
require -X- _ O
an -X- _ O
explicit -X- _ O
tokenization -X- _ O
step. -X- _ O

This -X- _ O
indicates -X- _ O
that -X- _ O
the -X- _ O
low -X- _ O
performance -X- _ O
of -X- _ O
denotation -X- _ O
predictions -X- _ O
and -X- _ O
the -X- _ O
loss -X- _ O
of -X- _ O
relational -X- _ O
information -X- _ O
between -X- _ O
denotations -X- _ O
lead -X- _ O
to -X- _ O
the -X- _ O
inadequate -X- _ O
performance -X- _ O
of -X- _ O
pipeline -X- _ O
models, -X- _ O
and -X- _ O
it -X- _ O
also -X- _ O
indicates -X- _ O
that -X- _ O
the -X- _ O
table -X- _ O
semantic -X- _ O
parser -X- _ O
has -X- _ O
a -X- _ O
large -X- _ O
space -X- _ O
for -X- _ O
improvement. -X- _ O

A -X- _ O
final -X- _ O
observation -X- _ O
is -X- _ O
that -X- _ O
the -X- _ O
End-to-End -X- _ O
model -X- _ O
is -X- _ O
comparable -X- _ O
to -X- _ O
the -X- _ O
model -X- _ O
that -X- _ O
has -X- _ O
access -X- _ O
to -X- _ O
the -X- _ O
gold -X- _ O
denotations, -X- _ O
suggesting -X- _ O
that -X- _ O
the -X- _ O
End-to-End -X- _ O
model -X- _ O
is -X- _ O
effective -X- _ O
at -X- _ O
extracting -X- _ O
denotations -X- _ O
latently. -X- _ O

Provided -X- _ O
the -X- _ O
full -X- _ O
context -X- _ O
of -X- _ O
ToTTo -X- _ B-DatasetName
instances, -X- _ O
the -X- _ O
annotators -X- _ O
were -X- _ O
asked -X- _ O
to -X- _ O
write -X- _ O
a -X- _ O
question -X- _ O
whose -X- _ O
answer -X- _ O
is -X- _ O
the -X- _ O
provided -X- _ O
ToTTo -X- _ B-DatasetName
sentence. -X- _ O

We -X- _ O
also -X- _ O
processed -X- _ O
merged -X- _ O
cells -X- _ O
by -X- _ O
copying -X- _ O
the -X- _ O
cell -X- _ O
content -X- _ O
and -X- _ O
cell -X- _ O
highlighted -X- _ O
region -X- _ O
to -X- _ O
all -X- _ O
the -X- _ O
individual -X- _ O
cells -X- _ O
that -X- _ O
compose -X- _ O
the -X- _ O
original -X- _ O
merged -X- _ O
cell. -X- _ O

We -X- _ O
adopted -X- _ O
these -X- _ O
table-grounded -X- _ O
sentences -X- _ O
as -X- _ O
the -X- _ O
answers -X- _ O
in -X- _ O
our -X- _ O
new -X- _ O
QA -X- _ B-TaskName
dataset -X- _ O
and -X- _ O
exploited -X- _ O
ToTTo’s -X- _ B-DatasetName
annotations -X- _ O
of -X- _ O
table -X- _ O
cells -X- _ O
(the -X- _ O
highlighted -X- _ O
table -X- _ O
region) -X- _ O
as -X- _ O
the -X- _ O
weak -X- _ O
supervision -X- _ O
labels -X- _ O
(denotations) -X- _ O
for -X- _ O
training -X- _ O
and -X- _ O
evaluating -X- _ O
the -X- _ O
intermediate -X- _ O
semantic -X- _ O
parser. -X- _ O

This -X- _ O
process -X- _ O
gave -X- _ O
us -X- _ O
sufficient -X- _ O
{table, -X- _ O
metadata, -X- _ O
highlighted -X- _ O
region, -X- _ O
sentence} -X- _ O
instances -X- _ O
from -X- _ O
ToTTo, -X- _ B-DatasetName
on -X- _ O
which -X- _ O
we -X- _ O
conducted -X- _ O
the -X- _ O
annotation -X- _ O
procedure -X- _ O
as -X- _ O
described -X- _ O
below. -X- _ O

We -X- _ O
provide -X- _ O
a -X- _ O
flowchart -X- _ O
of -X- _ O
this -X- _ O
sampling -X- _ O
process -X- _ O
in -X- _ O
Figure -X- _ O
7 -X- _ O
in -X- _ O
the -X- _ O
Appendix. -X- _ O

We -X- _ O
further -X- _ O
select -X- _ O
tables -X- _ O
whose -X- _ O
highlighted -X- _ O
cells -X- _ O
span -X- _ O
more -X- _ O
than -X- _ O
a -X- _ O
single -X- _ O
row -X- _ O
or -X- _ O
column -X- _ O
to -X- _ O
ensure -X- _ O
sentences -X- _ O
contain -X- _ O
several -X- _ O
table -X- _ O
entities. -X- _ O

As -X- _ O
shown -X- _ O
by -X- _ O
Figure -X- _ O
9 -X- _ O
and -X- _ O
10 -X- _ O
in -X- _ O
the -X- _ O
Appendix, -X- _ O
we -X- _ O
removed -X- _ O
all -X- _ O
tables -X- _ O
whose -X- _ O
sizes -X- _ O
are -X- _ O
above -X- _ O
the -X- _ O
75th -X- _ O
percentile -X- _ O
of -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
rows -X- _ O
or -X- _ O
columns -X- _ O
of -X- _ O
all -X- _ O
ToTTo -X- _ B-DatasetName
tables, -X- _ O
and -X- _ O
also -X- _ O
removed -X- _ O
tables -X- _ O
with -X- _ O
a -X- _ O
single -X- _ O
row -X- _ O
or -X- _ O
column. -X- _ O

With -X- _ O
this -X- _ O
objective, -X- _ O
we -X- _ O
found -X- _ O
by -X- _ O
probing -X- _ O
ToTTo -X- _ B-DatasetName
that -X- _ O
tables -X- _ O
with -X- _ O
extreme -X- _ O
sizes -X- _ O
(too -X- _ O
large -X- _ O
or -X- _ O
too -X- _ O
small -X- _ O
number -X- _ O
of -X- _ O
rows, -X- _ O
columns -X- _ O
or -X- _ O
both) -X- _ O
are -X- _ O
more -X- _ O
similar -X- _ O
to -X- _ O
attribute–value -X- _ O
pairs -X- _ O
instead -X- _ O
of -X- _ O
tables -X- _ O
with -X- _ O
complicated -X- _ O
structures, -X- _ O
and -X- _ O
they -X- _ O
tend -X- _ O
to -X- _ O
have -X- _ O
a -X- _ O
small -X- _ O
number -X- _ O
of -X- _ O
highlighted -X- _ O
cells, -X- _ O
which -X- _ O
make -X- _ O
them -X- _ O
not -X- _ O
ideal -X- _ O
for -X- _ O
our -X- _ O
dataset. -X- _ O

2.2 -X- _ O
Data -X- _ O
Collection -X- _ O
A -X- _ O
natural -X- _ O
way -X- _ O
to -X- _ O
collect -X- _ O
a -X- _ O
table-based -X- _ O
QA -X- _ O
pair -X- _ O
is -X- _ O
to -X- _ O
ask -X- _ O
annotators -X- _ O
to -X- _ O
first -X- _ O
generate -X- _ O
a -X- _ O
question -X- _ O
given -X- _ O
a -X- _ O
table, -X- _ O
then -X- _ O
provide -X- _ O
the -X- _ O
answer -X- _ O
to -X- _ O
it. -X- _ O

Answers -X- _ O
should -X- _ O
be -X- _ O
well -X- _ O
structured -X- _ O
information -X- _ O
contents, -X- _ O
faithful -X- _ O
to -X- _ O
the -X- _ O
tables, -X- _ O
and -X- _ O
presented -X- _ O
in -X- _ O
natural -X- _ O
utterances. -X- _ O

We -X- _ O
want -X- _ O
to -X- _ O
collect -X- _ O
questions -X- _ O
that -X- _ O
seek -X- _ O
not -X- _ O
just -X- _ O
a -X- _ O
specific -X- _ O
fact, -X- _ O
but -X- _ O
more -X- _ O
structured -X- _ O
information: -X- _ O
Desirably, -X- _ O
they -X- _ O
should -X- _ O
require -X- _ O
retrieving -X- _ O
more -X- _ O
and -X- _ O
different -X- _ O
facts -X- _ O
and -X- _ O
reasoning -X- _ O
with -X- _ O
diverse -X- _ O
aggregations. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
same -X- _ O
Data-to-Text -X- _ B-TaskName
model -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
the -X- _ O
zero-shot -X- _ O
setting. -X- _ O

We -X- _ O
further -X- _ O
fine-tune -X- _ O
the -X- _ O
TAPAS-base -X- _ B-MethodName
checkpoint -X- _ O
(WTQ -X- _ B-DatasetName
fine-tuned) -X- _ O
on -X- _ O
FeTaQA -X- _ B-DatasetName
train -X- _ O
set -X- _ O
and -X- _ O
select -X- _ O
models -X- _ O
based -X- _ O
on -X- _ O
their -X- _ O
performance -X- _ O
on -X- _ O
the -X- _ O
development -X- _ O
set. -X- _ O

Next -X- _ O
we -X- _ O
experiment -X- _ O
with -X- _ O
the -X- _ O
pipeline -X- _ O
model -X- _ O
by -X- _ O
fine-tuning -X- _ O
the -X- _ O
table -X- _ O
semantic -X- _ O
parser -X- _ O
on -X- _ O
FeTaQA. -X- _ B-DatasetName

We -X- _ O
denote -X- _ O
this -X- _ O
setting -X- _ O
as -X- _ O
Pipeline -X- _ O
zeroshot -X- _ O
in -X- _ O
Table -X- _ O
6. -X- _ O

We -X- _ O
then -X- _ O
employ -X- _ O
a -X- _ O
T5-large -X- _ B-MethodName
model -X- _ O
(Raffel -X- _ O
et -X- _ O
al., -X- _ O
2020) -X- _ O
that -X- _ O
goes -X- _ O
through -X- _ O
two -X- _ O
fine-tuning -X- _ O
stages: -X- _ O
in -X- _ O
the -X- _ O
first -X- _ O
stage -X- _ O
it -X- _ O
is -X- _ O
fine-tuned -X- _ O
on -X- _ O
the -X- _ O
downstream -X- _ O
Data-to-Text -X- _ B-TaskName
task -X- _ O
with -X- _ O
DART -X- _ B-DatasetName
(Nan -X- _ O
et -X- _ O
al., -X- _ O
2021); -X- _ O
in -X- _ O
the -X- _ O
second -X- _ O
stage -X- _ O
it -X- _ O
is -X- _ O
further -X- _ O
fine-tuned -X- _ O
on -X- _ O
ToTTo -X- _ B-DatasetName
instances -X- _ O
to -X- _ O
adapt -X- _ O
to -X- _ O
the -X- _ O
triple-set -X- _ O
formulation -X- _ O
we -X- _ O
proposed. -X- _ O

We -X- _ O
use -X- _ O
a -X- _ O
checkpoint -X- _ O
of -X- _ O
TAPASbase -X- _ B-MethodName
that -X- _ O
is -X- _ O
fine-tuned -X- _ O
on -X- _ O
WikiTableQuestions -X- _ B-DatasetName
(Pasupat -X- _ O
and -X- _ O
Liang, -X- _ O
2015) -X- _ O
to -X- _ O
perform -X- _ O
table -X- _ O
semantic -X- _ O
parsing -X- _ O
implicitly -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
produce -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
denotations, -X- _ O
which -X- _ O
is -X- _ O
then -X- _ O
converted -X- _ O
to -X- _ O
a -X- _ O
triple-set -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
§ -X- _ O
3.1. -X- _ O

4.1 -X- _ O
Experiment -X- _ O
Setup -X- _ O
We -X- _ O
first -X- _ O
experiment -X- _ O
with -X- _ O
the -X- _ O
pipeline -X- _ O
model -X- _ O
in -X- _ O
a -X- _ O
zero-shot -X- _ O
setting, -X- _ O
that -X- _ O
is, -X- _ O
without -X- _ O
any -X- _ O
fine-tuning -X- _ O
on -X- _ O
FeTaQA. -X- _ B-DatasetName

Therefore -X- _ O
we -X- _ O
did -X- _ O
not -X- _ O
include -X- _ O
TaBERT -X- _ O
as -X- _ O
a -X- _ O
baseline -X- _ O
end-to-end -X- _ O
model. -X- _ O

Pipeline -X- _ O
- -X- _ O
fine-tuned -X- _ O
Pipeline -X- _ O
- -X- _ O
gold -X- _ O
denotation -X- _ O
End-to-End -X- _ B-MethodName
- -X- _ I-MethodName
T5-small -X- _ I-MethodName
End-to-End -X- _ B-MethodName
- -X- _ I-MethodName
T5-base -X- _ I-MethodName
End-to-End -X- _ B-MethodName
- -X- _ I-MethodName
T5-large -X- _ I-MethodName
9.16 -X- _ O
11.00 -X- _ O
31.63 -X- _ O
21.60 -X- _ O
28.14 -X- _ O
30.54 -X- _ O
0.38 -X- _ O
0.40 -X- _ O
0.67 -X- _ O
0.55 -X- _ O
0.61 -X- _ O
0.63 -X- _ O
0.20 -X- _ O
0.22 -X- _ O
0.43 -X- _ O
0.33 -X- _ O
0.39 -X- _ O
0.41 -X- _ O
0.33 -X- _ O
0.35 -X- _ O
0.53 -X- _ O
0.47 -X- _ O
0.51 -X- _ O
0.53 -X- _ O
0.22 -X- _ O
0.24 -X- _ O
0.50 -X- _ O
0.40 -X- _ O
0.47 -X- _ O
0.49 -X- _ O
0.88 -X- _ O
0.91 -X- _ O
0.91 -X- _ O
0.94 -X- _ O
0.96 -X- _ O
0.96 -X- _ O

The -X- _ O
decoder -X- _ O
that -X- _ O
generates -X- _ O
the -X- _ O
free-form -X- _ O
answer -X- _ O
does -X- _ O
not -X- _ O
have -X- _ O
access -X- _ O
to -X- _ O
any -X- _ O
41 -X- _ O
sacreBLEU2 -X- _ O
ROUGE-1 -X- _ O
ROUGE-2 -X- _ O
ROUGE-L -X- _ O
METEOR -X- _ O
BERTScore -X- _ O

We -X- _ O
considered -X- _ O
an -X- _ O
alternative -X- _ O
option -X- _ O
of -X- _ O
integrating -X- _ O
TaBERT -X- _ O
into -X- _ O
an -X- _ O
end-to-end -X- _ O
model -X- _ O
but -X- _ O
found -X- _ O
it -X- _ O
infeasible, -X- _ O
since -X- _ O
it -X- _ O
provides -X- _ O
contextual -X- _ O
features -X- _ O
for -X- _ O
the -X- _ O
question -X- _ O
and -X- _ O
table -X- _ O
columns -X- _ O
(instead -X- _ O
of -X- _ O
table -X- _ O
cells, -X- _ O
as -X- _ O
in -X- _ O
our -X- _ O
table -X- _ O
linearization). -X- _ O

The -X- _ O
linearization -X- _ O
scheme -X- _ O
is -X- _ O
visualized -X- _ O
in -X- _ O
Figure -X- _ O
6. -X- _ O

We -X- _ O
fine-tune -X- _ O
models -X- _ O
from -X- _ O
the -X- _ O
T5-family -X- _ B-MethodName
on -X- _ O
the -X- _ O
FeTaQA -X- _ B-DatasetName
train -X- _ O
set. -X- _ O

We -X- _ O
prepend -X- _ O
q -X- _ O
to -X- _ O
table -X- _ O
linearization -X- _ O
(cid:4)T -X- _ O
, -X- _ O
and -X- _ O
use -X- _ O
[CLS] -X- _ O
tokens -X- _ O
as -X- _ O
prefixes -X- _ O
for -X- _ O
separation. -X- _ O

We -X- _ O
propose -X- _ O
a -X- _ O
simple -X- _ O
linearization -X- _ O
scheme -X- _ O
as -X- _ O
a -X- _ O
baseline: -X- _ O
table -X- _ O
rows -X- _ O
are -X- _ O
concatenated -X- _ O
with -X- _ O
[SEP] -X- _ O
tokens -X- _ O
in -X- _ O
between, -X- _ O
and -X- _ O
cells -X- _ O
in -X- _ O
each -X- _ O
row -X- _ O
are -X- _ O
separated -X- _ O
by -X- _ O
spaces. -X- _ O

3.2 -X- _ O
End-to-End -X- _ O
Model -X- _ O
In -X- _ O
this -X- _ O
approach, -X- _ O
we -X- _ O
model -X- _ O
the -X- _ O
task -X- _ O
as -X- _ O
a -X- _ O
sequence-to-sequence -X- _ B-TaskName
learning -X- _ I-TaskName
problem -X- _ O
by -X- _ O
linearizing -X- _ O
table -X- _ O
T -X- _ O
appended -X- _ O
to -X- _ O
question -X- _ O
q -X- _ O
as -X- _ O
the -X- _ O
source -X- _ O
sequence, -X- _ O
and -X- _ O
treating -X- _ O
the -X- _ O
free-form -X- _ O
answer -X- _ O
a -X- _ O
as -X- _ O
the -X- _ O
target -X- _ O
sequence. -X- _ O

To -X- _ O
avoid -X- _ O
exposure -X- _ O
to -X- _ O
FeTaQA -X- _ B-DatasetName
test -X- _ O
instances, -X- _ O
we -X- _ O
fine-tune -X- _ O
with -X- _ O
a -X- _ O
sample -X- _ O
of -X- _ O
8K -X- _ O
ToTTo -X- _ B-DatasetName
instances -X- _ O
that -X- _ O
are -X- _ O
not -X- _ O
used -X- _ O
for -X- _ O
creating -X- _ O
FeTaQA. -X- _ B-DatasetName

We -X- _ O
further -X- _ O
fine-tune -X- _ O
the -X- _ O
Data-to-Text -X- _ B-TaskName
model -X- _ O
on -X- _ O
ToTTo -X- _ B-DatasetName
instances -X- _ O
so -X- _ O
that -X- _ O
it -X- _ O
adapts -X- _ O
to -X- _ O
our -X- _ O
formation -X- _ O
of -X- _ O
triple-set -X- _ O
inputs. -X- _ O

We -X- _ O
end -X- _ O
up -X- _ O
with -X- _ O
a -X- _ O
triple-set -X- _ O
containing -X- _ O
all -X- _ O
highlighted -X- _ O
table -X- _ O
cells -X- _ O
and -X- _ O
the -X- _ O
metadata -X- _ O
(table -X- _ O
title -X- _ O
and -X- _ O
title -X- _ O
of -X- _ O
the -X- _ O
Wikipedia -X- _ O
page -X- _ O
that -X- _ O
includes -X- _ O
the -X- _ O
table). -X- _ O

We -X- _ O
then -X- _ O
incorporate -X- _ O
the -X- _ O
metadata -X- _ O
into -X- _ O
triples -X- _ O
by -X- _ O
replacing -X- _ O
column -X- _ O
header -X- _ O
with -X- _ O
the -X- _ O
field -X- _ O
name -X- _ O
(TABLE -X- _ O
TITLE, -X- _ O
PAGE -X- _ O
TITLE) -X- _ O
and -X- _ O
cell -X- _ O
value -X- _ O
with -X- _ O
the -X- _ O
metadata -X- _ O
content -X- _ O
(table -X- _ O
title -X- _ O
text, -X- _ O
page -X- _ O
title -X- _ O
text). -X- _ O

Similar -X- _ O
to -X- _ B-DatasetName
DART, -X- _ I-DatasetName
we -X- _ O
use -X- _ O
[TABLECONTEXT] -X- _ O
as -X- _ O
a -X- _ O
special -X- _ O
into -X- _ O
a -X- _ O
triple. -X- _ O

(cid:3) -X- _ O
(cid:2) -X- _ O
token -X- _ O
for -X- _ O
converting -X- _ O
a -X- _ O
table -X- _ O
cell -X- _ O
the -X- _ O
highlighted -X- _ O
region, -X- _ O
we -X- _ O
generate -X- _ O
the -X- _ O
following -X- _ O
[TABLECONTEXT], -X- _ O
column -X- _ O
header, -X- _ O
triple: -X- _ O
cell -X- _ O
value -X- _ O
, -X- _ O
where -X- _ O
column -X- _ O
header -X- _ O
is -X- _ O
the -X- _ O
cell’s -X- _ O
corresponding -X- _ O
column -X- _ O
name. -X- _ O

We -X- _ O
first -X- _ O
convert -X- _ O
the -X- _ O
denotation -X- _ O
prediction -X- _ O
into -X- _ O
the -X- _ O
triple-set -X- _ O
format -X- _ O
with -X- _ O
the -X- _ O
following -X- _ O
scheme: -X- _ O
for -X- _ O
each -X- _ O
table -X- _ O
cell -X- _ O
in -X- _ O
Figure -X- _ O
6: -X- _ O
Table -X- _ O
linearization -X- _ O
in -X- _ O
end-to-end -X- _ O
model. -X- _ O

Data-to-Text -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
5, -X- _ O
we -X- _ O
finetune -X- _ O
T5 -X- _ B-MethodName
(Raffel -X- _ O
et -X- _ O
al., -X- _ O
2020) -X- _ O
on -X- _ O
DART -X- _ B-DatasetName
(Nan -X- _ O
et -X- _ O
al., -X- _ O
2021) -X- _ O
to -X- _ O
obtain -X- _ O
a -X- _ O
Data-to-Text -X- _ B-TaskName
model -X- _ O
as -X- _ O
the -X- _ O
second -X- _ O
module -X- _ O
of -X- _ O
the -X- _ O
pipeline -X- _ O
to -X- _ O
perform -X- _ O
inference -X- _ O
of -X- _ O
aggregation -X- _ O
and -X- _ O
surface -X- _ O
realization -X- _ O
of -X- _ O
table -X- _ O
cells -X- _ O
(denotations -X- _ O
in -X- _ O
our -X- _ O
case). -X- _ O

Instead, -X- _ O
we -X- _ O
use -X- _ O
NONE -X- _ O
as -X- _ O
the -X- _ O
aggregation -X- _ O
operation -X- _ O
label -X- _ O
for -X- _ O
fine-tuning -X- _ O
TAPAS, -X- _ B-MethodName
and -X- _ O
let -X- _ O
the -X- _ O
second -X- _ O
module -X- _ O
(described -X- _ O
next) -X- _ O
produce -X- _ O
latent -X- _ O
aggregations -X- _ O
inferred -X- _ O
from -X- _ O
the -X- _ O
question -X- _ O
and -X- _ O
the -X- _ O
denotation -X- _ O
predictions -X- _ O
for -X- _ O
generating -X- _ O
the -X- _ O
answer -X- _ O
sentence. -X- _ O

However, -X- _ O
we -X- _ O
argue -X- _ O
that -X- _ O
the -X- _ O
aggregations -X- _ O
required -X- _ O
to -X- _ O
solve -X- _ O
FeTaQA -X- _ B-DatasetName
instances -X- _ O
are -X- _ O
diverse -X- _ O
and -X- _ O
they -X- _ O
are -X- _ O
not -X- _ O
covered -X- _ O
by -X- _ O
a -X- _ O
small -X- _ O
list -X- _ O
of -X- _ O
atomic -X- _ O
operations -X- _ O
pre-defined -X- _ O
by -X- _ O
humans. -X- _ O

Note -X- _ O
that -X- _ O
besides -X- _ O
denotations, -X- _ O
TAPAS -X- _ B-MethodName
was -X- _ O
pre-trained -X- _ O
to -X- _ O
explicitly -X- _ O
predict -X- _ O
an -X- _ O
aggregation -X- _ O
operation -X- _ O
(choose -X- _ O
from -X- _ O
COUNT, -X- _ O
SUM, -X- _ O
AVG, -X- _ O
NONE) -X- _ O
applied -X- _ O
to -X- _ O
the -X- _ O
predicted -X- _ O
denotations -X- _ O
to -X- _ O
obtain -X- _ O
the -X- _ O
final -X- _ O
answer. -X- _ O

We -X- _ O
believe -X- _ O
fine-tuning -X- _ O
is -X- _ O
crucial -X- _ O
for -X- _ O
our -X- _ O
task -X- _ O
because -X- _ O
TAPAS -X- _ B-MethodName
is -X- _ O
pre-trained -X- _ O
on -X- _ O
questions -X- _ O
that -X- _ O
require -X- _ O
retrieval -X- _ O
of -X- _ O
limited -X- _ O
denotations -X- _ O
(single -X- _ O
entity -X- _ O
or -X- _ O
homogeneous -X- _ O
entities -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
aggregated -X- _ O
with -X- _ O
COUNT, -X- _ O
SUM, -X- _ O
or -X- _ O
AVG -X- _ O
operation), -X- _ O
while -X- _ O
FeTaQA -X- _ B-DatasetName
questions -X- _ O
require -X- _ O
retrieval -X- _ O
of -X- _ O
multiple -X- _ O
entities -X- _ O
and -X- _ O
complex -X- _ O
aggregations. -X- _ O

We -X- _ O
fine-tune -X- _ O
TAPAS -X- _ B-MethodName
with -X- _ O
FeTaQA’s -X- _ B-DatasetName
label -X- _ O
denotations -X- _ O
(highlighted -X- _ O
table -X- _ O
regions). -X- _ O

After -X- _ O
fine-tuning, -X- _ O
the -X- _ O
table -X- _ O
semantic -X- _ O
parser -X- _ O
predicts -X- _ O
denotations, -X- _ O
which -X- _ O
are -X- _ O
then -X- _ O
converted -X- _ O
to -X- _ O
triples -X- _ O
and -X- _ O
sent -X- _ O
to -X- _ O
the -X- _ O
Data-to-Text -X- _ O
module. -X- _ O

We -X- _ O
choose -X- _ O
a -X- _ O
checkpoint -X- _ O
of -X- _ O
TAPAS-base -X- _ B-MethodName
fine-tuned -X- _ O
on -X- _ O
WikiTableQuestions -X- _ B-DatasetName
to -X- _ O
start -X- _ O
with. -X- _ O

40 -X- _ O
Figure -X- _ O
5: -X- _ O
Weakly -X- _ O
supervised -X- _ O
fine-tuning -X- _ O
of -X- _ O
table -X- _ O
semantic -X- _ O
parser -X- _ O
on -X- _ B-DatasetName
FeTaQA. -X- _ I-DatasetName

In -X- _ O
contrast, -X- _ O
TAPAS -X- _ B-MethodName
provides -X- _ O
representations -X- _ O
for -X- _ O
all -X- _ O
table -X- _ O
cells -X- _ O
that -X- _ O
help -X- _ O
weakly -X- _ O
supervised -X- _ O
semantic -X- _ O
parsers -X- _ O
directly -X- _ O
predict -X- _ O
denotations -X- _ O
in -X- _ O
an -X- _ O
end-to-end -X- _ O
fashion, -X- _ O
so -X- _ O
it’s -X- _ O
easier -X- _ O
to -X- _ O
perform -X- _ O
analysis -X- _ O
for -X- _ O
our -X- _ O
pipeline -X- _ O
models -X- _ O
without -X- _ O
considering -X- _ O
any -X- _ O
propagating -X- _ O
error. -X- _ O

These -X- _ O
representations -X- _ O
are -X- _ O
designed -X- _ O
to -X- _ O
help -X- _ O
weakly -X- _ O
supervised -X- _ O
semantic -X- _ O
parsers -X- _ O
generate -X- _ O
better -X- _ O
database-like -X- _ O
queries, -X- _ O
therefore -X- _ O
this -X- _ O
also -X- _ O
depends -X- _ O
on -X- _ O
a -X- _ O
reasonably -X- _ O
designed -X- _ O
domain-specific -X- _ O
query -X- _ O
language, -X- _ O
as -X- _ O
shown -X- _ O
by -X- _ O
TaBERT’s -X- _ O
use -X- _ O
case -X- _ O
of -X- _ O
MAPO -X- _ O
(Liang -X- _ O
et -X- _ O
al., -X- _ O
2018). -X- _ O

Two -X- _ O
recently -X- _ O
proposed -X- _ O
pre-trained -X- _ O
models -X- _ O
could -X- _ O
help -X- _ O
achieve -X- _ O
this: -X- _ O
TAPAS -X- _ B-MethodName
(Herzig -X- _ O
et -X- _ O
al., -X- _ O
2020) -X- _ O
and -X- _ O
TaBERT -X- _ O
(Yin -X- _ O
et -X- _ O
al., -X- _ O
2020a). -X- _ O

However, -X- _ O
we -X- _ O
did -X- _ O
not -X- _ O
include -X- _ O
TaBERT -X- _ O
in -X- _ O
our -X- _ O
experiment -X- _ O
because -X- _ O
it -X- _ O
provides -X- _ O
table -X- _ O
column -X- _ O
representations -X- _ O
based -X- _ O
on -X- _ O
no -X- _ O
more -X- _ O
than -X- _ O
3 -X- _ O
rows -X- _ O
of -X- _ O
the -X- _ O
table, -X- _ O
which -X- _ O
are -X- _ O
selected -X- _ O
based -X- _ O
on -X- _ O
their -X- _ O
n-gram -X- _ O
overlap -X- _ O
with -X- _ O
the -X- _ O
question. -X- _ O

They -X- _ O
are -X- _ O
both -X- _ O
pre-trained -X- _ O
models -X- _ O
for -X- _ O
joint -X- _ O
understanding -X- _ O
of -X- _ O
text -X- _ O
and -X- _ O
tabular -X- _ O
data, -X- _ O
and -X- _ O
can -X- _ O
be -X- _ O
integrated -X- _ O
into -X- _ O
semantic -X- _ O
parsers -X- _ O
for -X- _ O
solving -X- _ O
table-based -X- _ B-TaskName
QA -X- _ I-TaskName
tasks. -X- _ O

Therefore, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
pipeline -X- _ O
model -X- _ O
with -X- _ O
two -X- _ O
separately -X- _ O
trained -X- _ O
modules, -X- _ O
described -X- _ O
below. -X- _ O

However, -X- _ O
in -X- _ O
our -X- _ O
task, -X- _ O
targets -X- _ O
are -X- _ O
generated -X- _ O
texts -X- _ O
instead -X- _ O
of -X- _ O
retrieved -X- _ O
denotations, -X- _ O
suggesting -X- _ O
that -X- _ O
we -X- _ O
also -X- _ O
need -X- _ O
a -X- _ O
generator -X- _ O
to -X- _ O
integrate -X- _ O
the -X- _ O
retrieved -X- _ O
information -X- _ O
into -X- _ O
a -X- _ O
cogent -X- _ O
sentence. -X- _ O

But -X- _ O
due -X- _ O
to -X- _ O
their -X- _ O
high -X- _ O
annotation -X- _ O
costs, -X- _ O
people -X- _ O
usually -X- _ O
train -X- _ O
semantic -X- _ O
parsers -X- _ O
with -X- _ O
the -X- _ O
latter: -X- _ O
a -X- _ O
weakly -X- _ O
supervised -X- _ O
setting, -X- _ O
which -X- _ O
requires -X- _ O
label -X- _ O
denotations, -X- _ O
and -X- _ O
semantic -X- _ O
parsers -X- _ O
learn -X- _ O
to -X- _ O
predict -X- _ O
which -X- _ O
table -X- _ O
cells -X- _ O
constitute -X- _ O
the -X- _ O
final -X- _ O
answer -X- _ O
(Note -X- _ O
that -X- _ O
we -X- _ O
use -X- _ O
ToTTo’s -X- _ B-DatasetName
highlighted -X- _ O
table -X- _ O
cells -X- _ O
as -X- _ O
these -X- _ O
labels). -X- _ O

The -X- _ O
first -X- _ O
one -X- _ O
is -X- _ O
the -X- _ O
supervised -X- _ O
learning -X- _ O
setting, -X- _ O
which -X- _ O
requires -X- _ O
annotations -X- _ O
of -X- _ O
database-like -X- _ O
queries. -X- _ O

There -X- _ O
are -X- _ O
two -X- _ O
possible -X- _ O
settings -X- _ O
for -X- _ O
training -X- _ O
or -X- _ O
fine-tuning -X- _ O
a -X- _ O
table -X- _ O
semantic -X- _ O
parser, -X- _ O
as -X- _ O
shown -X- _ O
by -X- _ O
the -X- _ O
two -X- _ O
diagrams -X- _ O
on -X- _ O
the -X- _ O
left -X- _ O
in -X- _ O
Figure -X- _ O
4. -X- _ O

These -X- _ O
generated -X- _ O
queries -X- _ O
then -X- _ O
get -X- _ O
executed -X- _ O
to -X- _ O
give -X- _ O
the -X- _ O
final -X- _ O
denotation(s), -X- _ O
which -X- _ O
are -X- _ O
sufficient -X- _ O
for -X- _ O
answering -X- _ O
the -X- _ O
questions -X- _ O
in -X- _ O
the -X- _ O
previous -X- _ O
datasets. -X- _ O

3 -X- _ O
Models -X- _ O
To -X- _ O
quantify -X- _ O
the -X- _ O
challenge -X- _ O
posed -X- _ O
by -X- _ O
FeTaQA -X- _ B-DatasetName
for -X- _ O
state-of-the-art -X- _ O
models, -X- _ O
we -X- _ O
used -X- _ O
two -X- _ O
modeling -X- _ O
approaches -X- _ O
that -X- _ O
have -X- _ O
shown -X- _ O
to -X- _ O
be -X- _ O
effective -X- _ O
for -X- _ O
the -X- _ O
39 -X- _ O
Figure -X- _ O
3: -X- _ O
FeTaQA -X- _ B-DatasetName
questions -X- _ O
by -X- _ O
most -X- _ O
frequent -X- _ O
starting -X- _ O
words. -X- _ O

There -X- _ O
are -X- _ O
also -X- _ O
abundant -X- _ O
instances -X- _ O
related -X- _ O
to -X- _ O
media, -X- _ O
politics, -X- _ O
and -X- _ O
government. -X- _ O

Topics -X- _ O
Similar -X- _ O
to -X- _ O
ToTTo, -X- _ B-DatasetName
we -X- _ O
use -X- _ O
Wikimedia -X- _ B-MethodName
Foundation’s -X- _ I-MethodName
topic -X- _ I-MethodName
categorization -X- _ I-MethodName
model -X- _ O
(Asthana -X- _ O
and -X- _ O
Halfaker, -X- _ O
2018) -X- _ O
to -X- _ O
investigate -X- _ O
the -X- _ O
topic -X- _ O
distribution -X- _ O
of -X- _ O
FeTaQA, -X- _ B-DatasetName
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
2. -X- _ O

We -X- _ O
found -X- _ O
that -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
instances -X- _ O
are -X- _ O
related -X- _ O
to -X- _ O
biography, -X- _ O
sports, -X- _ O
and -X- _ O
geographical -X- _ O
regions. -X- _ O

The -X- _ O
free-form -X- _ O
answers -X- _ O
have -X- _ O
a -X- _ O
median -X- _ O
of -X- _ O
18 -X- _ O
tokens -X- _ O
in -X- _ O
length, -X- _ O
and -X- _ O
are -X- _ O
grounded -X- _ O
to -X- _ O
the -X- _ O
table -X- _ O
and -X- _ O
the -X- _ O
denotations, -X- _ O
also -X- _ O
suggested -X- _ O
by -X- _ O
the -X- _ O
high -X- _ O
evaluation -X- _ O
scores. -X- _ O

These -X- _ O
denotations -X- _ O
are -X- _ O
correct -X- _ O
and -X- _ O
adequate -X- _ O
as -X- _ O
indicated -X- _ O
by -X- _ O
the -X- _ O
corresponding -X- _ O
high -X- _ O
evaluation -X- _ O
scores. -X- _ O

The -X- _ O
median -X- _ O
number -X- _ O
of -X- _ O
highlighted -X- _ O
cells -X- _ O
(denotations) -X- _ O
is -X- _ O
6, -X- _ O
which -X- _ O
is -X- _ O
twice -X- _ O
as -X- _ O
much -X- _ O
as -X- _ O
the -X- _ O
corresponding -X- _ O
number -X- _ O
for -X- _ O
ToTTo, -X- _ B-DatasetName
indicating -X- _ O
that -X- _ O
FeTaQA -X- _ B-DatasetName
requires -X- _ O
retrieval -X- _ O
of -X- _ O
multiple -X- _ O
entities -X- _ O
in -X- _ O
the -X- _ O
table. -X- _ O

this -X- _ O
is -X- _ O
a -X- _ O
time-consuming -X- _ O
process, -X- _ O
we -X- _ O
simply -X- _ O
asked -X- _ O
the -X- _ O
evaluators -X- _ O
to -X- _ O
score -X- _ O
based -X- _ O
on -X- _ O
their -X- _ O
subjective -X- _ O
judgement, -X- _ O
which -X- _ O
could -X- _ O
have -X- _ O
caused -X- _ O
the -X- _ O
relatively -X- _ O
low -X- _ O
agreement. -X- _ O

Note -X- _ O
that -X- _ O
an -X- _ O
ideal -X- _ O
measurement -X- _ O
of -X- _ O
the -X- _ O
question -X- _ B-MetricName
complexity -X- _ I-MetricName
is -X- _ O
to -X- _ O
quantify -X- _ O
the -X- _ O
structural -X- _ O
complexity -X- _ O
of -X- _ O
the -X- _ O
information -X- _ O
contained -X- _ O
in -X- _ O
the -X- _ O
answer, -X- _ O
but -X- _ O
since -X- _ O
Figure -X- _ O
2: -X- _ O
FeTaQA -X- _ B-DatasetName
topics -X- _ O
distribution. -X- _ O

A -X- _ O
quantitative -X- _ O
and -X- _ O
qualitative -X- _ O
analysis -X- _ O
of -X- _ O
FeTaQA -X- _ B-DatasetName
shows -X- _ O
it -X- _ O
contains -X- _ O
lots -X- _ O
of -X- _ O
complex -X- _ O
questions -X- _ O
judged -X- _ O
by -X- _ O
human -X- _ O
evaluators. -X- _ O

Evaluation -X- _ O
scores -X- _ O
and -X- _ O
inter-evaluator -X- _ B-MetricName
agreements -X- _ I-MetricName
are -X- _ O
reported -X- _ O
in -X- _ O
Table -X- _ O
5. -X- _ O

This -X- _ O
in -X- _ O
7,326/1,001/2,003 -X- _ O
instances -X- _ O
in -X- _ O
the -X- _ O
train/dev/test -X- _ O
splits, -X- _ O
respectively. -X- _ O

When -X- _ O
this -X- _ O
seed -X- _ O
set -X- _ O
grows -X- _ O
to -X- _ O
take -X- _ O
up -X- _ O
70% -X- _ O
of -X- _ O
all -X- _ O
the -X- _ O
instances, -X- _ O
the -X- _ O
remaining -X- _ O
30% -X- _ O
instances -X- _ O
are -X- _ O
less -X- _ O
similar -X- _ O
to -X- _ O
any -X- _ O
instance -X- _ O
in -X- _ O
the -X- _ O
seed -X- _ O
set. -X- _ O

The -X- _ O
seed -X- _ O
set -X- _ O
then -X- _ O
becomes -X- _ O
the -X- _ O
training -X- _ O
set -X- _ O
and -X- _ O
the -X- _ O
remaining -X- _ O
instances -X- _ O
are -X- _ O
divided -X- _ O
to -X- _ O
form -X- _ O
the -X- _ O
development -X- _ O
and -X- _ O
test -X- _ O
sets. -X- _ O

We -X- _ O
first -X- _ O
sampled -X- _ O
800 -X- _ O
instances -X- _ O
randomly -X- _ O
as -X- _ O
a -X- _ O
seed -X- _ O
set, -X- _ O
then -X- _ O
gradually -X- _ O
add -X- _ O
instances -X- _ O
to -X- _ O
it -X- _ O
if -X- _ O
an -X- _ O
instance -X- _ O
is -X- _ O
similar -X- _ O
to -X- _ O
any -X- _ O
instance -X- _ O
in -X- _ O
the -X- _ O
seed -X- _ O
set. -X- _ O

We -X- _ O
report -X- _ O
% -X- _ O
of -X- _ O
samples -X- _ O
that -X- _ O
have -X- _ O
score -X- _ O
≥ -X- _ O
4 -X- _ O
to -X- _ O
show -X- _ O
high -X- _ O
quality -X- _ O
of -X- _ O
FeTaQA, -X- _ B-DatasetName
and -X- _ O
report -X- _ O
percent -X- _ O
agreement -X- _ O
and -X- _ O
Randolph’s -X- _ B-MetricName
Kappa -X- _ I-MetricName
(Randolph, -X- _ O
2010) -X- _ O
(with -X- _ O
95% -X- _ O
CI) -X- _ O
to -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
human -X- _ O
evaluation -X- _ O
has -X- _ O
high -X- _ O
inter-annotator -X- _ B-MetricName
agreement. -X- _ I-MetricName

In -X- _ O
total, -X- _ O
our -X- _ O
dataset -X- _ O
contains -X- _ O
10,330 -X- _ O
instances. -X- _ O

The -X- _ O
annotator -X- _ O
contributions -X- _ O
to -X- _ O
the -X- _ O
final -X- _ O
dataset -X- _ O
are -X- _ O
distributed -X- _ O
as -X- _ O
follows: -X- _ O
We -X- _ O
have -X- _ O
3,039 -X- _ O
(30%) -X- _ O
instances -X- _ O
from -X- _ O
internal -X- _ O
annotators -X- _ O
and -X- _ O
7,291 -X- _ O
(70%) -X- _ O
from -X- _ O
MTurk -X- _ O
workers. -X- _ O

First -X- _ O
we -X- _ O
found -X- _ O
that -X- _ O
ToTTo’s -X- _ B-DatasetName
annotation -X- _ O
of -X- _ O
highlighted -X- _ O
cells -X- _ O
is -X- _ O
a -X- _ O
reasonable -X- _ O
indicator -X- _ O
of -X- _ O
how -X- _ O
much -X- _ O
information -X- _ O
is -X- _ O
required -X- _ O
from -X- _ O
the -X- _ O
table -X- _ O
to -X- _ O
give -X- _ O
the -X- _ O
answer, -X- _ O
which -X- _ O
we -X- _ O
aim -X- _ O
to -X- _ O
maximize. -X- _ O

We -X- _ O
sample -X- _ O
ToTTo -X- _ B-DatasetName
instances -X- _ O
with -X- _ O
the -X- _ O
following -X- _ O
considerations. -X- _ O

More -X- _ O
importantly, -X- _ O
such -X- _ O
sentences -X- _ O
contain -X- _ O
noteworthy -X- _ O
information -X- _ O
that -X- _ O
users -X- _ O
are -X- _ O
more -X- _ O
interested -X- _ O
in -X- _ O
and -X- _ O
likely -X- _ O
to -X- _ O
ask -X- _ O
given -X- _ O
a -X- _ O
table -X- _ O
from -X- _ O
Wikipedia. -X- _ O

We -X- _ O
want -X- _ O
to -X- _ O
first -X- _ O
sample -X- _ O
a -X- _ O
subset -X- _ O
of -X- _ O
these -X- _ O
sentences -X- _ O
that -X- _ O
already -X- _ O
provide -X- _ O
aggregation -X- _ O
and -X- _ O
reasoning -X- _ O
on -X- _ O
multiple -X- _ O
facts -X- _ O
in -X- _ O
the -X- _ O
table, -X- _ O
which -X- _ O
is -X- _ O
the -X- _ O
target -X- _ O
content -X- _ O
that -X- _ O
annotators -X- _ O
spend -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
time -X- _ O
trying -X- _ O
to -X- _ O
come -X- _ O
up -X- _ O
with, -X- _ O
so -X- _ O
that -X- _ O
we -X- _ O
could -X- _ O
largely -X- _ O
reduce -X- _ O
the -X- _ O
time -X- _ O
spent -X- _ O
on -X- _ O
annotation. -X- _ O

ToTTo -X- _ B-DatasetName
applied -X- _ O
several -X- _ O
heuristics -X- _ O
to -X- _ O
sample -X- _ O
the -X- _ O
tables -X- _ O
and -X- _ O
the -X- _ O
candidate -X- _ O
sentences -X- _ O
from -X- _ O
Wikipedia -X- _ O
pages, -X- _ O
and -X- _ O
their -X- _ O
annotators -X- _ O
are -X- _ O
asked -X- _ O
to -X- _ O
revise -X- _ O
sentences -X- _ O
and -X- _ O
highlight -X- _ O
the -X- _ O
corresponding -X- _ O
table -X- _ O
regions -X- _ O
so -X- _ O
that -X- _ O
the -X- _ O
sentences -X- _ O
still -X- _ O
have -X- _ O
the -X- _ O
varied -X- _ O
language -X- _ O
and -X- _ O
structure -X- _ O
found -X- _ O
in -X- _ O
natural -X- _ O
sentences. -X- _ O

Additionally, -X- _ O
ToTTo -X- _ B-DatasetName
comes -X- _ O
with -X- _ O
annotations -X- _ O
of -X- _ O
table -X- _ O
cells -X- _ O
that -X- _ O
support -X- _ O
the -X- _ O
sentences: -X- _ O
A -X- _ O
sentence -X- _ O
is -X- _ O
supported -X- _ O
by -X- _ O
the -X- _ O
cell -X- _ O
contents -X- _ O
if -X- _ O
it -X- _ O
is -X- _ O
directly -X- _ O
stated -X- _ O
or -X- _ O
can -X- _ O
be -X- _ O
logically -X- _ O
inferred -X- _ O
by -X- _ O
them. -X- _ O

It -X- _ O
contains -X- _ O
textual -X- _ O
descriptions -X- _ O
that -X- _ O
are -X- _ O
naturally -X- _ O
written -X- _ O
and -X- _ O
fully -X- _ O
grounded -X- _ O
in -X- _ O
Wikipedia -X- _ O
tables. -X- _ O

We -X- _ O
found -X- _ O
that -X- _ O
ToTTo -X- _ B-DatasetName
(Parikh -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
a -X- _ O
recently -X- _ O
proposed -X- _ O
large-scale -X- _ O
Table-to-Text -X- _ O
dataset, -X- _ O
is -X- _ O
a -X- _ O
desirable -X- _ O
resource -X- _ O
to -X- _ O
start -X- _ O
with. -X- _ O

However, -X- _ O
we -X- _ O
found -X- _ O
that -X- _ O
it -X- _ O
usually -X- _ O
takes -X- _ O
more -X- _ O
effort -X- _ O
to -X- _ O
ask -X- _ O
about -X- _ O
how -X- _ O
multiple -X- _ O
facts -X- _ O
are -X- _ O
related -X- _ O
or -X- _ O
share -X- _ O
something -X- _ O
in -X- _ O
common -X- _ O
than -X- _ O
to -X- _ O
ask -X- _ O
about -X- _ O
a -X- _ O
specific -X- _ O
fact -X- _ O
in -X- _ O
the -X- _ O
table; -X- _ O
annotators -X- _ O
spend -X- _ O
much -X- _ O
more -X- _ O
time -X- _ O
finding -X- _ O
out -X- _ O
the -X- _ O
relations -X- _ O
between -X- _ O
cell -X- _ O
contents -X- _ O
for -X- _ O
question -X- _ O
generation, -X- _ O
and -X- _ O
they -X- _ O
also -X- _ O
need -X- _ O
to -X- _ O
spend -X- _ O
time -X- _ O
writing -X- _ O
an -X- _ O
answer. -X- _ O

We -X- _ O
make -X- _ O
the -X- _ O
dataset -X- _ O
and -X- _ O
code -X- _ O
available -X- _ O
online.1 -X- _ O
2 -X- _ O
Dataset -X- _ O
Here -X- _ O
we -X- _ O
introduce -X- _ O
FeTaQA -X- _ B-DatasetName
and -X- _ O
describe -X- _ O
the -X- _ O
process -X- _ O
and -X- _ O
criteria -X- _ O
for -X- _ O
collecting -X- _ O
the -X- _ O
tables, -X- _ O
questions, -X- _ O
and -X- _ O
answers. -X- _ O

The -X- _ O
indicate -X- _ O
the -X- _ O
challenging -X- _ O
nature -X- _ O
of -X- _ O
FeTaQA -X- _ B-DatasetName
and -X- _ O
that -X- _ O
there -X- _ O
is -X- _ O
much -X- _ O
room -X- _ O
for -X- _ O
improvement -X- _ O
in -X- _ O
QA -X- _ O
systems. -X- _ O

Through -X- _ O
human -X- _ O
studies, -X- _ O
we -X- _ O
evaluate -X- _ O
answers -X- _ O
generated -X- _ O
by -X- _ O
our -X- _ O
proposed -X- _ O
models -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
reference -X- _ O
answer -X- _ O
based -X- _ O
on -X- _ O
fluency, -X- _ O
correctness, -X- _ O
adequacy -X- _ O
(informativeness), -X- _ O
and -X- _ O
faithfulness. -X- _ O

The -X- _ O
other -X- _ O
is -X- _ O
a -X- _ O
pipeline -X- _ B-MethodName
model -X- _ I-MethodName
that -X- _ O
achieves -X- _ O
content -X- _ B-TaskName
selection -X- _ I-TaskName
and -X- _ O
surface -X- _ B-TaskName
realization -X- _ I-TaskName
in -X- _ O
separate -X- _ O
modules -X- _ O
involving -X- _ O
TAPAS -X- _ B-MethodName
(Herzig -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
which -X- _ O
is -X- _ O
a -X- _ O
recently -X- _ O
proposed -X- _ O
pre-trained -X- _ O
model -X- _ O
that -X- _ O
jointly -X- _ O
processes -X- _ O
text -X- _ O
and -X- _ O
tabular -X- _ O
data -X- _ O
for -X- _ O
the -X- _ O
usage -X- _ O
of -X- _ O
semantic -X- _ B-TaskName
parsing. -X- _ I-TaskName

The -X- _ O
first -X- _ O
one -X- _ O
is -X- _ O
an -X- _ O
end-to-end -X- _ B-MethodName
model -X- _ I-MethodName
that -X- _ O
integrates -X- _ O
query -X- _ O
and -X- _ O
table -X- _ O
comprehension, -X- _ O
reasoning, -X- _ O
and -X- _ O
language -X- _ O
generation -X- _ O
by -X- _ O
adapting -X- _ O
T5 -X- _ O
(Raffel -X- _ O
et -X- _ O
al., -X- _ O
2020). -X- _ O

We -X- _ O
propose -X- _ O
two -X- _ O
benchmark -X- _ O
and -X- _ O
provide -X- _ O
experimental -X- _ O
for -X- _ O
them. -X- _ O

We -X- _ O
formulate -X- _ O
generative -X- _ B-TaskName
table -X- _ I-TaskName
question -X- _ I-TaskName
answering -X- _ I-TaskName
as -X- _ O
a -X- _ O
Sequence-to-Sequence -X- _ O
learning -X- _ O
problem. -X- _ O

FeTaQA -X- _ B-DatasetName
examples -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
and -X- _ O
differences -X- _ O
between -X- _ O
FeTaQA -X- _ O
and -X- _ O
other -X- _ O
QA -X- _ O
datasets -X- _ O
are -X- _ O
described -X- _ O
in -X- _ O
Table -X- _ O
1. -X- _ O

In -X- _ O
addition, -X- _ O
the -X- _ O
FeTaQA -X- _ B-DatasetName
tables -X- _ O
cover -X- _ O
a -X- _ O
diverse -X- _ O
set -X- _ O
of -X- _ O
topics -X- _ O
and -X- _ O
contain -X- _ O
un-normalized -X- _ O
text, -X- _ O
including -X- _ O
numbers, -X- _ O
dates, -X- _ O
and -X- _ O
phrases. -X- _ O

We -X- _ O
annotate -X- _ O
questions -X- _ O
that -X- _ O
elicit -X- _ O
such -X- _ O
descriptions, -X- _ O
and -X- _ O
we -X- _ O
make -X- _ O
efforts -X- _ O
to -X- _ O
ensure -X- _ O
that -X- _ O
the -X- _ O
QA -X- _ O
interaction -X- _ O
is -X- _ O
compatible, -X- _ O
and -X- _ O
question -X- _ O
annotations -X- _ O
are -X- _ O
not -X- _ O
contrived. -X- _ O

We -X- _ O
collect -X- _ O
question–answer -X- _ O
pairs -X- _ O
from -X- _ O
noteworthy -X- _ O
descriptions -X- _ O
of -X- _ O
Wikipedia -X- _ O
tables -X- _ O
that -X- _ O
are -X- _ O
high -X- _ O
quality -X- _ O
sentences -X- _ O
rich -X- _ O
in -X- _ O
structured -X- _ O
information -X- _ O
contents. -X- _ O

FeTaQA -X- _ B-DatasetName
reveals -X- _ O
the -X- _ O
challenging -X- _ O
nature -X- _ O
of -X- _ O
the -X- _ O
table -X- _ B-TaskName
QA -X- _ I-TaskName
task: -X- _ O
1) -X- _ O
retrieving -X- _ O
multiple -X- _ O
entities -X- _ O
from -X- _ O
tables -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
query; -X- _ O
2) -X- _ O
aggregating -X- _ O
and -X- _ O
reasoning -X- _ O
over -X- _ O
relations -X- _ O
of -X- _ O
these -X- _ O
entities; -X- _ O
and -X- _ O
3) -X- _ O
structuring -X- _ O
surface -X- _ O
information -X- _ O
and -X- _ O
inferences -X- _ O
into -X- _ O
a -X- _ O
coherent -X- _ O
answer -X- _ O
that -X- _ O
is -X- _ O
faithful -X- _ O
to -X- _ O
the -X- _ O
table. -X- _ O

To -X- _ O
complement -X- _ O
the -X- _ O
existing -X- _ O
datasets -X- _ O
with -X- _ O
the -X- _ O
absent -X- _ O
QA -X- _ O
interactions, -X- _ O
we -X- _ O
present -X- _ O
FeTaQA, -X- _ B-DatasetName
a -X- _ O
Free-form -X- _ B-DatasetName
Table -X- _ I-DatasetName
Question -X- _ I-DatasetName
Answering -X- _ I-DatasetName
dataset -X- _ O
that -X- _ O
includes -X- _ O
long, -X- _ O
informative, -X- _ O
and -X- _ O
free-form -X- _ O
answers. -X- _ O

Therefore -X- _ O
a -X- _ O
QA -X- _ O
system -X- _ O
should -X- _ O
also -X- _ O
possess -X- _ O
such -X- _ O
structuring -X- _ O
capability, -X- _ O
evaluated -X- _ O
by -X- _ O
text -X- _ O
generation -X- _ O
tasks. -X- _ O

Nevertheless, -X- _ O
in -X- _ O
many -X- _ O
cases, -X- _ O
people -X- _ O
tend -X- _ O
to -X- _ O
seek -X- _ O
more -X- _ O
structured -X- _ O
information -X- _ O
content, -X- _ O
such -X- _ O
as -X- _ O
‘‘how’’, -X- _ O
‘‘why’’, -X- _ O
and -X- _ O
some -X- _ O
of -X- _ O
the -X- _ O
‘‘what’’ -X- _ O
questions -X- _ O
that -X- _ O
ask -X- _ O
for -X- _ O
general -X- _ O
concepts. -X- _ O

The -X- _ O
exchange -X- _ O
of -X- _ O
information -X- _ O
between -X- _ O
humans -X- _ O
through -X- _ O
interactions -X- _ O
with -X- _ O
questions -X- _ O
and -X- _ O
answers -X- _ O
is -X- _ O
different -X- _ O
from -X- _ O
the -X- _ O
interactions -X- _ O
presented -X- _ O
in -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
existing -X- _ O
QA -X- _ O
datasets, -X- _ O
in -X- _ O
which -X- _ O
questions -X- _ O
are -X- _ O
specific -X- _ O
(sometimes -X- _ O
contrived -X- _ O
for -X- _ O
testing -X- _ O
multi-hop -X- _ O
reasoning) -X- _ O
and -X- _ O
provide -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
information, -X- _ O
while -X- _ O
answers -X- _ O
are -X- _ O
in -X- _ O
short-form -X- _ O
and -X- _ O
fill -X- _ O
in -X- _ O
the -X- _ O
missing -X- _ O
information -X- _ O
piece. -X- _ O

Though -X- _ O
existing -X- _ O
datasets -X- _ O
have -X- _ O
enabled -X- _ O
significant -X- _ O
progress -X- _ O
for -X- _ O
table -X- _ B-TaskName
QA, -X- _ I-TaskName
their -X- _ O
limitations -X- _ O
prevent -X- _ O
them -X- _ O
from -X- _ O
reflecting -X- _ O
the -X- _ O
challenging -X- _ O
nature -X- _ O
of -X- _ O
the -X- _ O
task. -X- _ O

The -X- _ O
answers -X- _ O
are -X- _ O
extracted -X- _ O
facts/entities -X- _ O
in -X- _ O
the -X- _ O
table, -X- _ O
therefore -X- _ O
usually -X- _ O
in -X- _ O
short-form. -X- _ O

For -X- _ O
QA -X- _ B-TaskName
over -X- _ I-TaskName
table, -X- _ I-TaskName
a -X- _ O
common -X- _ O
approach -X- _ O
is -X- _ O
to -X- _ O
apply -X- _ O
semantic -X- _ B-MethodName
parsing -X- _ I-MethodName
on -X- _ O
the -X- _ O
query -X- _ O
and -X- _ O
the -X- _ O
table -X- _ O
schema -X- _ O
to -X- _ O
generate -X- _ O
a -X- _ O
logical -X- _ O
form -X- _ O
(e.g., -X- _ O
a -X- _ O
SQL-like -X- _ O
database -X- _ O
query) -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
executed -X- _ O
to -X- _ O
retrieve -X- _ O
the -X- _ O
answer -X- _ O
from -X- _ O
the -X- _ O
relevant -X- _ O
portion -X- _ O
of -X- _ O
the -X- _ O
table -X- _ O
(Pasupat -X- _ O
and -X- _ O
Liang, -X- _ O
2015; -X- _ O
Iyyer -X- _ O
et -X- _ O
al., -X- _ O
2017; -X- _ O
Zhong -X- _ O
et -X- _ O

For -X- _ O
QA -X- _ B-TaskName
over -X- _ I-TaskName
text, -X- _ I-TaskName
a -X- _ O
sequence -X- _ O
modeling -X- _ O
approach -X- _ O
is -X- _ O
usually -X- _ O
adopted -X- _ O
to -X- _ O
encode -X- _ O
the -X- _ O
query -X- _ O
and -X- _ O
the -X- _ O
context, -X- _ O
and -X- _ O
answers -X- _ O
are -X- _ O
either -X- _ O
categorical -X- _ O
(Lai -X- _ O
et -X- _ O
al., -X- _ O
2017), -X- _ O
extractive -X- _ O
(Rajpurkar -X- _ O
et -X- _ O
al., -X- _ O
2016; -X- _ O

In -X- _ O
the -X- _ O
real -X- _ O
world, -X- _ O
knowledge -X- _ O
sources -X- _ O
take -X- _ O
a -X- _ O
variety -X- _ O
of -X- _ O
forms, -X- _ O
including -X- _ O
unstructured -X- _ O
texts -X- _ O
(documents, -X- _ O
passages, -X- _ O
or -X- _ O
conversations), -X- _ O
structured -X- _ O
knowledge -X- _ O
bases, -X- _ O
and -X- _ O
semi-structured -X- _ O
tables, -X- _ O
each -X- _ O
requiring -X- _ O
dedicated -X- _ O
modeling -X- _ O
approaches. -X- _ O

One -X- _ O
of -X- _ O
the -X- _ O
primary -X- _ O
goals -X- _ O
of -X- _ O
QA -X- _ B-TaskName
is -X- _ O
to -X- _ O
allow -X- _ O
users -X- _ O
to -X- _ O
directly -X- _ O
and -X- _ O
efficiently -X- _ O
interact -X- _ O
with -X- _ O
large-scale -X- _ O
and -X- _ O
het∗Now -X- _ O
at -X- _ O
Facebook -X- _ O
AI. -X- _ O

We -X- _ O
provide -X- _ O
two -X- _ O
benchmark -X- _ O
for -X- _ O
the -X- _ O
proposed -X- _ O
task: -X- _ O
a -X- _ O
pipeline -X- _ O
based -X- _ O
on -X- _ O
semantic -X- _ B-TaskName
parsing-based -X- _ I-TaskName
QA -X- _ I-TaskName
systems -X- _ O
and -X- _ O
an -X- _ O
end-to-end -X- _ B-TaskName
based -X- _ I-TaskName
on -X- _ I-TaskName
large -X- _ I-TaskName
pretrained -X- _ I-TaskName
text -X- _ I-TaskName
generation -X- _ I-TaskName
models, -X- _ O
and -X- _ O
show -X- _ O
that -X- _ O
FeTaQA -X- _ B-DatasetName
poses -X- _ O
a -X- _ O
challenge -X- _ O
for -X- _ O
both -X- _ O
methods. -X- _ O
infer, -X- _ O
and -X- _ O
conduct -X- _ O
1 -X- _ O
Question -X- _ B-TaskName
Answering -X- _ I-TaskName
(QA) -X- _ O
is -X- _ O
the -X- _ O
task -X- _ O
of -X- _ O
producing -X- _ O
answers -X- _ O
to -X- _ O
natural -X- _ O
language -X- _ O
questions -X- _ O
based -X- _ O
on -X- _ O
knowledge -X- _ O
resources -X- _ O
(Burke -X- _ O
et -X- _ O
al., -X- _ O
1997; -X- _ O
Yao -X- _ O
and -X- _ O
Van -X- _ O
Durme, -X- _ O
2014; -X- _ O
Chen -X- _ O
et -X- _ O
al., -X- _ O
2017). -X- _ O

FeTaQA -X- _ B-DatasetName
is -X- _ O
collected -X- _ O
from -X- _ O
noteworthy -X- _ O
descriptions -X- _ O
of -X- _ O
Wikipedia -X- _ O
tables -X- _ O
that -X- _ O
contain -X- _ O
information -X- _ O
people -X- _ O
tend -X- _ O
to -X- _ O
seek; -X- _ O
generation -X- _ O
of -X- _ O
these -X- _ O
descriptions -X- _ O
requires -X- _ O
advanced -X- _ O
processing -X- _ O
that -X- _ O
humans -X- _ O
perform -X- _ O
on -X- _ O
a -X- _ O
daily -X- _ O
basis: -X- _ O
Understand -X- _ O
the -X- _ O
question -X- _ O
and -X- _ O
table, -X- _ O
retrieve, -X- _ O
text -X- _ O
planning -X- _ O
integrate, -X- _ O
and -X- _ O
surface -X- _ O
realization -X- _ O
to -X- _ O
generate -X- _ O
an -X- _ O
answer. -X- _ O

To -X- _ O
complement -X- _ O
the -X- _ O
existing -X- _ O
datasets -X- _ O
and -X- _ O
to -X- _ O
reveal -X- _ O
the -X- _ O
challenging -X- _ O
nature -X- _ O
of -X- _ O
the -X- _ O
table-based -X- _ O
question -X- _ O
answering -X- _ O
task, -X- _ O
we -X- _ O
introduce -X- _ O
FeTaQA, -X- _ B-DatasetName
a -X- _ O
new -X- _ O
dataset -X- _ O
with -X- _ O
10K -X- _ O
Wikipedia-based -X- _ O
{table, -X- _ O
question, -X- _ O
free-form -X- _ O
answer, -X- _ O
supporting -X- _ O
table -X- _ O
cells} -X- _ O
pairs. -X- _ O

However, -X- _ O
restricted -X- _ O
by -X- _ O
their -X- _ O
short-form -X- _ O
answers, -X- _ O
these -X- _ O
datasets -X- _ O
fail -X- _ O
to -X- _ O
include -X- _ O
question–answer -X- _ B-TaskName
interactions -X- _ O
that -X- _ O
represent -X- _ O
more -X- _ O
advanced -X- _ O
and -X- _ O
naturally -X- _ O
occurring -X- _ O
information -X- _ O
needs: -X- _ O
questions -X- _ O
that -X- _ O
ask -X- _ O
for -X- _ O
reasoning -X- _ O
and -X- _ O
integration -X- _ O
of -X- _ O
information -X- _ O
pieces -X- _ O
retrieved -X- _ O
from -X- _ O
a -X- _ O
structured -X- _ O
knowledge -X- _ O
source. -X- _ O

Xiangru -X- _ O
Tang1 -X- _ O
Mutethia -X- _ O
Mutuma1 -X- _ O
Ben -X- _ O
Rosand1 -X- _ O
Isabel -X- _ O
Trindade1 -X- _ O
Renusree -X- _ O
Bandaru4 -X- _ O
Jacob -X- _ O
Cunningham4 -X- _ O
Caiming -X- _ O
Xiong2 -X- _ O
Dragomir -X- _ O
Radev1,2 -X- _ O
1 -X- _ O
Yale -X- _ O
University, -X- _ O
USA -X- _ O
2 -X- _ O
Salesforce -X- _ O
Research, -X- _ O
USA -X- _ O
3 -X- _ O
The -X- _ O
University -X- _ O
of -X- _ O
Hong -X- _ O
Kong, -X- _ O
China -X- _ O
4 -X- _ O
Penn -X- _ O
State -X- _ O
University, -X- _ O
USA -X- _ O
5 -X- _ O
Archbishop -X- _ O
Mitty -X- _ O
High -X- _ O
School, -X- _ O
USA -X- _ O
{linyong.nan, -X- _ O
ziming.mao}@yale.edu, -X- _ O
hsiehcc@connect.hku.hk -X- _ O
Existing -X- _ O
table -X- _ B-TaskName
question -X- _ I-TaskName
answering -X- _ I-TaskName
datasets -X- _ O
contain -X- _ O
abundant -X- _ O
factual -X- _ O
questions -X- _ O
that -X- _ O
primarily -X- _ O
evaluate -X- _ O
a -X- _ O
QA -X- _ B-TaskName
system’s -X- _ O
comprehension -X- _ O
of -X- _ O
query -X- _ O
and -X- _ O
tabular -X- _ O
data. -X- _ O

FeTaQA: -X- _ B-DatasetName
Free-form -X- _ O
Table -X- _ O
Question -X- _ B-TaskName
Answering -X- _ I-TaskName
Linyong -X- _ O
Nan1 -X- _ O
Chiachun -X- _ O
Hsieh3 -X- _ O
Ziming -X- _ O
Mao1 -X- _ O
Xi -X- _ O
Victoria -X- _ O

However, -X- _ O
the -X- _ O
existing -X- _ O
annotations -X- _ O
fail -X- _ O
to -X- _ O
capture -X- _ O
a -X- _ O
number -X- _ O
of -X- _ O
the -X- _ O
core -X- _ O
distinctions -X- _ O
above—a -X- _ O
lacuna -X- _ O
this -X- _ O
work -X- _ O
aims -X- _ O
to -X- _ O
fill. -X- _ O

Two -X- _ O
argument -X- _ O
subspaces -X- _ O
with -X- _ O
four -X- _ O
properties: -X- _ O
• -X- _ O
GENERICITY -X- _ O
(Govindarajan -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
particular: -X- _ O
is -X- _ O
the -X- _ O
entity -X- _ O
a -X- _ O
particular? -X- _ O

Bolded -X- _ O
properties -X- _ O
are -X- _ O
ones -X- _ O
we -X- _ O
collect -X- _ O
in -X- _ O
this -X- _ O
paper, -X- _ O
and -X- _ O
our -X- _ O
new -X- _ O
document-level -X- _ O
graph -X- _ O
is -X- _ O
also -X- _ O
shown -X- _ O
in -X- _ O
purple. -X- _ O

UDS -X- _ O
comprises -X- _ O
two -X- _ O
layers -X- _ O
of -X- _ O
annotations -X- _ O
on -X- _ O
top -X- _ O
of -X- _ O
the -X- _ O
Universal -X- _ O
Dependencies -X- _ O
(UD) -X- _ O
syntactic -X- _ O
graphs -X- _ O
in -X- _ O
the -X- _ O
English -X- _ B-DatasetName
Web -X- _ I-DatasetName
Treebank -X- _ I-DatasetName
(EWT): -X- _ B-DatasetName
(i) -X- _ O
predicate-argument -X- _ O
graphs -X- _ O
with -X- _ O
mappings -X- _ O
into -X- _ O
the -X- _ O
syntactic -X- _ O
graphs, -X- _ O
derived -X- _ O
using -X- _ O
the -X- _ O
PredPatt -X- _ O
tool -X- _ O
(White -X- _ O
et -X- _ O
al., -X- _ O
2016; -X- _ O
Zhang -X- _ O
et -X- _ O
al., -X- _ O
2017); -X- _ O
and -X- _ O
(ii) -X- _ O
crowd-sourced -X- _ O
annotations -X- _ O
for -X- _ O
properties -X- _ O
of -X- _ O
events -X- _ O
(on -X- _ O
the -X- _ O
predicate -X- _ O
nodes -X- _ O
of -X- _ O
the -X- _ O
predicate-argument -X- _ O
graph), -X- _ O
entities -X- _ O
(on -X- _ O
the -X- _ O
19 -X- _ O
Figure -X- _ O
1: -X- _ O
Example -X- _ O
UDS -X- _ O
semantics -X- _ O
and -X- _ O
syntax -X- _ O
graphs -X- _ O
with -X- _ O
select -X- _ O
properties -X- _ O
(see -X- _ O
Footnote -X- _ O
2 -X- _ O
on -X- _ O
the -X- _ O
property -X- _ O
values). -X- _ O

(i) -X- _ O
UDS -X- _ O
is -X- _ O
a -X- _ O
semantic -X- _ O
annotation -X- _ O
framework -X- _ O
and -X- _ O
the -X- _ O
dataset -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
principles -X- _ O
that -X- _ O
semantics -X- _ O
of -X- _ O
words -X- _ O
or -X- _ O
phrases -X- _ O
can -X- _ O
be -X- _ O
decomposed -X- _ O
into -X- _ O
sets -X- _ O
of -X- _ O
simpler -X- _ O
semantic -X- _ O
properties -X- _ O
and -X- _ O
(ii) -X- _ O
these -X- _ O
properties -X- _ O
can -X- _ O
be -X- _ O
annotated -X- _ O
by -X- _ O
asking -X- _ O
straightforward -X- _ O
questions -X- _ O
intelligible -X- _ O
to -X- _ O
non-experts. -X- _ O

In -X- _ O
(12), -X- _ O
the -X- _ O
meeting -X- _ O
presumably -X- _ O
has -X- _ O
some -X- _ O
structure, -X- _ O
but -X- _ O
there -X- _ O
is -X- _ O
no -X- _ O
bijection -X- _ O
from -X- _ O
members -X- _ O
to -X- _ O
subevents—meet -X- _ O
is -X- _ O
collective -X- _ O
(see -X- _ O
Champollion, -X- _ O
2010, -X- _ O
for -X- _ O
a -X- _ O
review). -X- _ O

In -X- _ O
the -X- _ O
case -X- _ O
where -X- _ O
the -X- _ O
subevents -X- _ O
are -X- _ O
similar -X- _ O
and -X- _ O
a -X- _ O
participant -X- _ O
itself -X- _ O
has -X- _ O
subparts -X- _ O
(e.g., -X- _ O
when -X- _ O
the -X- _ O
participant -X- _ O
is -X- _ O
a -X- _ O
group), -X- _ O
there -X- _ O
may -X- _ O
be -X- _ O
a -X- _ O
bijection -X- _ O
from -X- _ O
participant -X- _ O
subparts -X- _ O
to -X- _ O
subevents. -X- _ O

As -X- _ O
theoretical -X- _ O
constructs, -X- _ O
these -X- _ O
classes -X- _ O
are -X- _ O
used -X- _ O
to -X- _ O
explain -X- _ O
both -X- _ O
the -X- _ O
distributional -X- _ O
characteristics -X- _ O
of -X- _ O
event -X- _ O
descriptions -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
inferences -X- _ O
about -X- _ O
how -X- _ O
an -X- _ O
event -X- _ O
progresses -X- _ O
over -X- _ O
time. -X- _ O

Theoretical -X- _ O
Approaches -X- _ O
Vendler -X- _ O
categorizes -X- _ O
event -X- _ O
descriptions -X- _ O
into -X- _ O
four -X- _ O
classes: -X- _ O
statives -X- _ O
(3), -X- _ O
activities -X- _ O
(4), -X- _ O
achievements -X- _ O
(5), -X- _ O
and -X- _ O
accomplishments -X- _ O
(6). -X- _ O

We -X- _ O
briefly -X- _ O
discuss -X- _ O
this -X- _ O
classification -X- _ O
and -X- _ O
elaborations -X- _ O
thereon -X- _ O
before -X- _ O
turning -X- _ O
to -X- _ O
other -X- _ O
event -X- _ O
structure -X- _ O
classifications -X- _ O
developed -X- _ O
for -X- _ O
annotating -X- _ O
corpora.1 -X- _ O
We -X- _ O
then -X- _ O
contrast -X- _ O
these -X- _ O
with -X- _ O
the -X- _ O
fully -X- _ O
decompositional -X- _ O
approach -X- _ O
we -X- _ O
take -X- _ O
in -X- _ O
this -X- _ O
paper. -X- _ O

2 -X- _ O
Background -X- _ O
Contemporary -X- _ O
theoretical -X- _ O
treatments -X- _ O
of -X- _ O
event -X- _ O
structure -X- _ O
tend -X- _ O
to -X- _ O
take -X- _ O
as -X- _ O
their -X- _ O
starting -X- _ O
point -X- _ O
Vendler’s -X- _ O
(1957) -X- _ O
seminal -X- _ O
four-way -X- _ O
classification. -X- _ O

We -X- _ O
validate -X- _ O
our -X- _ O
protocols -X- _ O
(§4) -X- _ O
and -X- _ O
use -X- _ O
them -X- _ O
to -X- _ O
collect -X- _ O
annotations -X- _ O
for -X- _ O
the -X- _ O
entire -X- _ O
Universal -X- _ O
Dependencies -X- _ O
(Nivre -X- _ O
et -X- _ O
al., -X- _ O
2016) -X- _ O

After -X- _ O
motivating -X- _ O
the -X- _ O
need -X- _ O
for -X- _ O
these -X- _ O
additional -X- _ O
properties -X- _ O
(§2), -X- _ O
we -X- _ O
develop -X- _ O
annotation -X- _ O
protocols -X- _ O
for -X- _ O
them -X- _ O
(§3). -X- _ O

UDS -X- _ O
annotates -X- _ O
for -X- _ O
a -X- _ O
subset -X- _ O
of -X- _ O
key -X- _ O
event -X- _ O
structure -X- _ O
properties, -X- _ O
Transactions -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics, -X- _ O
vol. -X- _ O

We -X- _ O
induce -X- _ O
this -X- _ O
classification -X- _ O
jointly -X- _ O
with -X- _ O
semantic -X- _ B-TaskName
role, -X- _ I-TaskName
entity, -X- _ B-TaskName
and -X- _ O
event-event -X- _ B-TaskName
relation -X- _ I-TaskName
classifications -X- _ I-TaskName
using -X- _ O
a -X- _ O
document-level -X- _ B-MethodName
generative -X- _ I-MethodName
model -X- _ I-MethodName
structured -X- _ O
by -X- _ O
these -X- _ O
graphs. -X- _ O

We -X- _ O
present -X- _ O
an -X- _ O
event -X- _ B-TaskName
structure -X- _ I-TaskName
classification -X- _ I-TaskName
empirically -X- _ O
derived -X- _ O
from -X- _ O
inferential -X- _ O
properties -X- _ O
annotated -X- _ O
on -X- _ O
sentence- -X- _ O
and -X- _ O
document-level -X- _ O
Universal -X- _ B-DatasetName
Decompositional -X- _ I-DatasetName
Semantics -X- _ I-DatasetName
(UDS) -X- _ I-DatasetName
graphs. -X- _ I-DatasetName

Similar -X- _ O
in -X- _ O
spirit -X- _ O
to -X- _ O
this -X- _ O
prior -X- _ O
work, -X- _ O
but -X- _ O
different -X- _ O
in -X- _ O
method, -X- _ O
our -X- _ O
work -X- _ O
aims -X- _ O
to -X- _ O
develop -X- _ O
an -X- _ O
empirically -X- _ O
derived -X- _ O
event -X- _ O
structure -X- _ O
classification. -X- _ O

al., -X- _ O
2005), -X- _ O
Meaning -X- _ B-DatasetName
Representation -X- _ I-DatasetName
(Banarescu -X- _ O
et -X- _ O
al., -X- _ O
2013), -X- _ O
and -X- _ O
Universal -X- _ B-DatasetName
Conceptual -X- _ I-DatasetName
Cognitive -X- _ I-DatasetName
Annotation -X- _ I-DatasetName
(Abend -X- _ O
and -X- _ O
Rappoport, -X- _ O
2013), -X- _ O
among -X- _ O
others. -X- _ O

A -X- _ O
number -X- _ O
of -X- _ O
such -X- _ O
classifications -X- _ O
and -X- _ O
annotated -X- _ O
corpora -X- _ O
exist: -X- _ O
FrameNet -X- _ B-DatasetName
(Baker -X- _ O
et -X- _ O
al., -X- _ O
1998), -X- _ O
VerbNet -X- _ B-DatasetName
(Kipper -X- _ O
Schuler, -X- _ O
2005), -X- _ O
PropBank -X- _ B-DatasetName
(Palmer -X- _ O
et -X- _ O

Determining -X- _ O
this -X- _ O
structure -X- _ O
requires -X- _ O
an -X- _ O
event -X- _ O
classification -X- _ O
whose -X- _ O
elements -X- _ O
are -X- _ O
associated -X- _ O
with -X- _ O
event -X- _ O
structure -X- _ O
representations. -X- _ O

Consequently, -X- _ O
extracting -X- _ O
knowledge -X- _ O
about -X- _ O
complex -X- _ O
events -X- _ O
from -X- _ O
text -X- _ O
involves -X- _ O
determining -X- _ O
the -X- _ O
structure -X- _ O
of -X- _ O
the -X- _ O
events -X- _ O
being -X- _ O
referred -X- _ O
to: -X- _ O
what -X- _ O
their -X- _ O
parts -X- _ O
are, -X- _ O
how -X- _ O
those -X- _ O
parts -X- _ O
are -X- _ O
laid -X- _ O
out -X- _ O
in -X- _ O
time, -X- _ O
17 -X- _ O
who -X- _ O
participates -X- _ O
in -X- _ O
them -X- _ O
and -X- _ O
how, -X- _ O
and -X- _ O
so -X- _ O
forth. -X- _ O

Decomposing -X- _ B-TaskName
and -X- _ I-TaskName
Recomposing -X- _ I-TaskName
Event -X- _ I-TaskName
Structure -X- _ I-TaskName
William -X- _ O
Gantt -X- _ O
University -X- _ O
of -X- _ O
Rochester, -X- _ O
USA -X- _ O
wgantt@cs.rochester.edu -X- _ O

[...] -X- _ O
Further, -X- _ O
descriptions -X- _ O
of -X- _ O
the -X- _ O
same -X- _ O
event -X- _ O
at -X- _ O
different -X- _ O
granularities -X- _ O
can -X- _ O
be -X- _ O
interleaved -X- _ O
within -X- _ O
the -X- _ O
same -X- _ O
document—for -X- _ O
example, -X- _ O
(2) -X- _ O
might -X- _ O
well -X- _ O
directly -X- _ O
follow -X- _ O
(1) -X- _ O
as -X- _ O
an -X- _ O
elaboration -X- _ O
on -X- _ O
the -X- _ O
housebuilding -X- _ O
process. -X- _ O

They -X- _ O
then -X- _ O
framed -X- _ O
the -X- _ O
house -X- _ O
before -X- _ O
installing -X- _ O
the -X- _ O
plumbing. -X- _ O

(2) -X- _ O
They -X- _ O
started -X- _ O
by -X- _ O
laying -X- _ O
the -X- _ O
house’s -X- _ O
foundation. -X- _ O

(1) -X- _ O
The -X- _ O
contractors -X- _ O
built -X- _ O
the -X- _ O
house. -X- _ O

For -X- _ O
instance, -X- _ O
one -X- _ O
and -X- _ O
the -X- _ O
same -X- _ O
event -X- _ O
can -X- _ O
be -X- _ O
described -X- _ O
at -X- _ O
a -X- _ O
coarse -X- _ O
grain, -X- _ O
using -X- _ O
a -X- _ O
single -X- _ O
clause -X- _ O
(1), -X- _ O
or -X- _ O
at -X- _ O
a -X- _ O
finer -X- _ O
grain, -X- _ O
using -X- _ O
an -X- _ O
entire -X- _ O
document -X- _ O
(2). -X- _ O

1 -X- _ O
Natural -X- _ O
language -X- _ O
provides -X- _ O
myriad -X- _ O
ways -X- _ O
of -X- _ O
communicating -X- _ O
about -X- _ O
complex -X- _ O
events. -X- _ O

The -X- _ O
resulting -X- _ O
dataset -X- _ O
(available -X- _ O
at -X- _ O
decomp.io) -X- _ O
is -X- _ O
the -X- _ O
largest -X- _ O
annotation -X- _ O
of -X- _ O
event -X- _ O
structure -X- _ O
and -X- _ O
(partial) -X- _ O
event -X- _ O
coreference -X- _ O
to -X- _ O
date. -X- _ O

To -X- _ O
support -X- _ O
this -X- _ O
induction, -X- _ O
we -X- _ O
augment -X- _ O
existing -X- _ O
annotations -X- _ O
found -X- _ O
in -X- _ O
the -X- _ O
UDS1.0 -X- _ B-DatasetName
dataset, -X- _ O
which -X- _ O
covers -X- _ O
the -X- _ O
entirety -X- _ O
of -X- _ O
the -X- _ O
English -X- _ B-DatasetName
Web -X- _ I-DatasetName
Treebank, -X- _ I-DatasetName
with -X- _ O
an -X- _ O
array -X- _ O
of -X- _ O
inferential -X- _ O
properties -X- _ O
capturing -X- _ O
fine-grained -X- _ O
aspects -X- _ O
of -X- _ O
the -X- _ O
temporal -X- _ O
and -X- _ O
aspectual -X- _ O
structure -X- _ O
of -X- _ O
events. -X- _ O

Furthermore, -X- _ O
our -X- _ O
combines -X- _ O
with -X- _ O
P´erez -X- _ O
et -X- _ O
al.’s -X- _ O
(2019) -X- _ B-MethodName
AHAT -X- _ I-MethodName
implementation -X- _ O
of -X- _ O
the -X- _ O
MAJORITY -X- _ B-DatasetName
language -X- _ I-DatasetName
and -X- _ O
Bhattamishra -X- _ O
et -X- _ O

We -X- _ O
also -X- _ O
show -X- _ O
that -X- _ O
every -X- _ O
UHAT -X- _ B-MethodName
can -X- _ O
be -X- _ O
simulated -X- _ O
by -X- _ O
an -X- _ O
AHAT, -X- _ B-MethodName
establishing -X- _ O
UHAT -X- _ B-MethodName
as -X- _ O
a -X- _ O
subclass -X- _ O
of -X- _ O
AHAT. -X- _ B-MethodName

More -X- _ O
formally, -X- _ O
we -X- _ O
prove -X- _ O
that -X- _ O
any -X- _ O
formal -X- _ O
language -X- _ O
recognized -X- _ O
using -X- _ O
a -X- _ O
GUHAT -X- _ B-MethodName
is -X- _ O
also -X- _ O
recognized -X- _ O
by -X- _ O
a -X- _ O
family -X- _ O
of -X- _ O
Boolean -X- _ O
circuits -X- _ O
of -X- _ O
constant -X- _ O
depth -X- _ O
and -X- _ O
polynomial -X- _ O
size, -X- _ O
establishing -X- _ O
AC0 -X- _ O
as -X- _ O
an -X- _ O
upper -X- _ O
bound -X- _ O
on -X- _ O
the -X- _ O
expressiveness -X- _ B-MetricName
of -X- _ O
UHAT -X- _ B-MethodName
and -X- _ O
GUHAT. -X- _ B-MethodName

(2022).1 -X- _ O
Our -X- _ O
main -X- _ O
contribution -X- _ O
is -X- _ O
to -X- _ O
prove -X- _ O
that -X- _ O
GUHAT -X- _ B-MethodName
and -X- _ O
UHAT -X- _ B-MethodName
can -X- _ O
only -X- _ O
recognize -X- _ O
formal -X- _ O
languages -X- _ O
in -X- _ O
AC0, -X- _ O
the -X- _ O
class -X- _ O
of -X- _ O
formal -X- _ O
languages -X- _ O
recognized -X- _ O
by -X- _ O
a -X- _ O
family -X- _ O
of -X- _ O
Boolean -X- _ O
circuits -X- _ O
of -X- _ O
constant -X- _ O
depth -X- _ O
and -X- _ O
polynomial -X- _ O
size, -X- _ O
whereas -X- _ O
AHAT -X- _ B-MethodName
can -X- _ O
recognize -X- _ O
formal -X- _ O
languages -X- _ O
outside -X- _ O
of -X- _ O
AC0. -X- _ O

This -X- _ O
is -X- _ O
the -X- _ O
definition -X- _ O
of -X- _ O
hard -X- _ O
attention -X- _ O
used -X- _ O
by -X- _ O
P´erez -X- _ O
et -X- _ O
al. -X- _ O

In -X- _ O
the -X- _ O
third -X- _ O
model, -X- _ O
which -X- _ O
we -X- _ O
call -X- _ O
averaging -X- _ B-MethodName
hard -X- _ I-MethodName
attention -X- _ I-MethodName
Transformers -X- _ I-MethodName
(AHAT), -X- _ B-MethodName
the -X- _ O
attention -X- _ O
mechanism -X- _ O
returns -X- _ O
the -X- _ O
uniform -X- _ O
average -X- _ O
of -X- _ O
the -X- _ O
values -X- _ O
at -X- _ O
positions -X- _ O
with -X- _ O
the -X- _ O
maximum -X- _ O
attention -X- _ O
value. -X- _ O

(2021) -X- _ O
and -X- _ O
is -X- _ O
a -X- _ O
more -X- _ O
concrete -X- _ O
version -X- _ O
of -X- _ O
GUHAT -X- _ B-MethodName
that -X- _ O
incorporates -X- _ O
restrictions -X- _ O
on -X- _ O
the -X- _ O
nature -X- _ O
of -X- _ O
activation -X- _ O
values -X- _ O
and -X- _ O
computations. -X- _ O

The -X- _ O
second -X- _ O
model, -X- _ O
unique -X- _ B-MethodName
hard -X- _ I-MethodName
attention -X- _ I-MethodName
Transformers -X- _ I-MethodName
(UHAT), -X- _ B-MethodName
was -X- _ O
defined -X- _ O
and -X- _ O
studied -X- _ O
by -X- _ O
Yao -X- _ O
et -X- _ O
al. -X- _ O

The -X- _ O
first -X- _ O
such -X- _ O
model, -X- _ O
which -X- _ O
we -X- _ O
call -X- _ O
generalized -X- _ B-MethodName
unique -X- _ I-MethodName
hard -X- _ I-MethodName
attention -X- _ I-MethodName
Transformers -X- _ I-MethodName
(GUHAT) -X- _ B-MethodName
and -X- _ O
was -X- _ O
defined -X- _ O
by -X- _ O
Hahn -X- _ O
(2020), -X- _ O
imposes -X- _ O
no -X- _ O
restrictions -X- _ O
on -X- _ O
the -X- _ O
nature -X- _ O
of -X- _ O
activation -X- _ O
values -X- _ O
or -X- _ O
the -X- _ O
functions -X- _ O
the -X- _ O
network -X- _ O
uses -X- _ O
to -X- _ O
compute -X- _ O
them. -X- _ O

In -X- _ O
the -X- _ O
first -X- _ O
two -X- _ O
models -X- _ O
we -X- _ O
study, -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
returns -X- _ O
the -X- _ O
value -X- _ O
at -X- _ O
exactly -X- _ O
one -X- _ O
position -X- _ O
(for -X- _ O
example, -X- _ O
the -X- _ O
leftmost) -X- _ O
in -X- _ O
case -X- _ O
several -X- _ O
positions -X- _ O
tie -X- _ O
for -X- _ O
the -X- _ O
maximum -X- _ O
attention -X- _ O
value. -X- _ O

All -X- _ O
three -X- _ O
models -X- _ O
use -X- _ O
hard -X- _ O
attention—meaning -X- _ O
that -X- _ O
each -X- _ O
attention -X- _ O
head -X- _ O
attends -X- _ O
only -X- _ O
to -X- _ O
the -X- _ O
position -X- _ O
or -X- _ O
positions -X- _ O
with -X- _ O
the -X- _ O
800 -X- _ O
highest -X- _ O
attention -X- _ O
score, -X- _ O
with -X- _ O
no -X- _ O
attention -X- _ O
paid -X- _ O
to -X- _ O
any -X- _ O
of -X- _ O
the -X- _ O
other -X- _ O
positions—but -X- _ O
differ -X- _ O
in -X- _ O
how -X- _ O
they -X- _ O
behave -X- _ O
in -X- _ O
the -X- _ O
case -X- _ O
of -X- _ O
ties -X- _ O
in -X- _ O
the -X- _ O
maximum -X- _ O
attention -X- _ O
value. -X- _ O

insights -X- _ O
about -X- _ O
In -X- _ O
this -X- _ O
work, -X- _ O
we -X- _ O
analyze -X- _ O
three -X- _ O
restricted -X- _ O
models -X- _ O
of -X- _ O
self-attention -X- _ O
based -X- _ O
on -X- _ O
their -X- _ O
ability -X- _ O
to -X- _ O
recognize -X- _ O
formal -X- _ O
languages. -X- _ O

(2022) -X- _ O
have -X- _ O
uncovered -X- _ O
meaningful -X- _ O
the -X- _ O
expressive -X- _ O
power -X- _ O
of -X- _ O
Transformers -X- _ O
by -X- _ O
formulating -X- _ O
models -X- _ O
of -X- _ O
the -X- _ O
self-attention -X- _ O
mechanism -X- _ O
and -X- _ O
analyzing -X- _ O
their -X- _ O
computational -X- _ O
power. -X- _ O

While -X- _ O
Transformer -X- _ O
networks -X- _ O
are -X- _ O
extremely -X- _ O
complex -X- _ O
when -X- _ O
deployed -X- _ O
at -X- _ O
scale, -X- _ O
theoretical -X- _ O
studies -X- _ O
such -X- _ O
as -X- _ O
those -X- _ O
of -X- _ O
P´erez -X- _ O
et -X- _ O
al. -X- _ O

These -X- _ O
advances -X- _ O
have -X- _ O
spurred -X- _ O
considerable -X- _ O
interest -X- _ O
in -X- _ O
understanding -X- _ O
the -X- _ O
capabilities -X- _ O
and -X- _ O
limitations -X- _ O
of -X- _ O
the -X- _ O
Transformer -X- _ O
architecture. -X- _ O

The -X- _ O
Transformer -X- _ O
architecture -X- _ O
for -X- _ O
neural -X- _ O
networks -X- _ O
(Vaswani -X- _ O
et -X- _ O
al., -X- _ O
2017) -X- _ O
has -X- _ O
yielded -X- _ O
remarkable -X- _ O
advances -X- _ O
in -X- _ O
performance -X- _ O
on -X- _ O
a -X- _ O
variety -X- _ O
of -X- _ O
benchmark -X- _ O
tasks -X- _ O
in -X- _ O
natural -X- _ O
language -X- _ O
processing. -X- _ O

We -X- _ O
show -X- _ O
that -X- _ O
UHAT -X- _ B-MethodName
and -X- _ O
GUHAT -X- _ B-MethodName
Transformers, -X- _ O
viewed -X- _ O
as -X- _ O
string -X- _ O
acceptors, -X- _ O
can -X- _ O
only -X- _ O
recognize -X- _ O
formal -X- _ O
languages -X- _ O
in -X- _ O
the -X- _ O
complexity -X- _ O
class -X- _ O
AC0, -X- _ O
the -X- _ O
class -X- _ O
of -X- _ O
languages -X- _ O
recognizable -X- _ O
by -X- _ O
families -X- _ O
of -X- _ O
Boolean -X- _ O
circuits -X- _ O
of -X- _ O
constant -X- _ O
depth -X- _ O
and -X- _ O
polynomial -X- _ O
size. -X- _ O

In -X- _ O
contrast, -X- _ O
the -X- _ O
non-AC0 -X- _ O
languages -X- _ O
MAJORITY -X- _ O
and -X- _ O
DYCK-1 -X- _ O
are -X- _ O
recognizable -X- _ O
by -X- _ O
AHAT -X- _ B-MethodName
networks, -X- _ O
implying -X- _ O
that -X- _ O
AHAT -X- _ B-MethodName
can -X- _ O
recognize -X- _ O
languages -X- _ O
that -X- _ O
UHAT -X- _ B-MethodName
and -X- _ O
GUHAT -X- _ B-MethodName
cannot. -X- _ O

This -X- _ O
upper -X- _ O
bound -X- _ O
subsumes -X- _ O
Hahn’s -X- _ O
(2020) -X- _ O
that -X- _ O
GUHAT -X- _ B-MethodName
cannot -X- _ O
recognize -X- _ O
the -X- _ O
DYCK -X- _ O
languages -X- _ O
or -X- _ O
the -X- _ O
PARITY -X- _ O
language, -X- _ O
since -X- _ O
those -X- _ O
languages -X- _ O
are -X- _ O
outside -X- _ O
AC0 -X- _ O
(Furst -X- _ O
et -X- _ O
al., -X- _ O
1984). -X- _ O

Formal -X- _ B-TaskName
Language -X- _ I-TaskName
Recognition -X- _ I-TaskName
by -X- _ O
Hard -X- _ B-MethodName
Attention -X- _ I-MethodName
Transformers: -X- _ I-MethodName
Perspectives -X- _ O
from -X- _ O
Circuit -X- _ B-MetricName
Complexity -X- _ I-MetricName
Yiding -X- _ O
Hao, -X- _ O
Dana -X- _ O
Angluin, -X- _ O
and -X- _ O
Robert -X- _ O
Frank -X- _ O
Yale -X- _ O
University -X- _ O
New -X- _ O
Haven, -X- _ O
CT, -X- _ O
USA -X- _ O
{yiding.hao, -X- _ O
robert.frank, -X- _ O
dana.angluin}@yale.edu -X- _ O
This -X- _ O
paper -X- _ O
analyzes -X- _ O
three -X- _ O
formal -X- _ O
models -X- _ O
of -X- _ O
Transformer -X- _ O
encoders -X- _ O
that -X- _ O
differ -X- _ O
in -X- _ O
the -X- _ O
form -X- _ O
of -X- _ O
their -X- _ O
self-attention -X- _ O
mechanism: -X- _ O
unique -X- _ B-MethodName
hard -X- _ I-MethodName
attention -X- _ I-MethodName
(UHAT); -X- _ B-MethodName
generalized -X- _ B-MethodName
unique -X- _ I-MethodName
hard -X- _ I-MethodName
attention -X- _ I-MethodName
(GUHAT), -X- _ B-MethodName
which -X- _ O
generalizes -X- _ O
UHAT; -X- _ B-MethodName
and -X- _ O
averaging -X- _ B-MethodName
hard -X- _ I-MethodName
attention -X- _ I-MethodName
(AHAT). -X- _ B-MethodName

We -X- _ O
hypothesize -X- _ O
that -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
n-chars -X- _ O
in -X- _ O
language -X- _ O
models -X- _ O
may -X- _ O
be -X- _ O
driven -X- _ O
by -X- _ O
polysemy, -X- _ O
which -X- _ O
is -X- _ O
not -X- _ O
accounted -X- _ O
for -X- _ O
in -X- _ O
our -X- _ O
regressions. -X- _ O

This -X- _ O
result -X- _ O
is -X- _ O
particularly -X- _ O
interesting -X- _ O
because -X- _ O
the -X- _ O
language -X- _ O
models -X- _ O
we -X- _ O
used -X- _ O
have -X- _ O
no -X- _ O
information -X- _ O
about -X- _ O
word -X- _ O
length. -X- _ O

This -X- _ O
contrasts -X- _ O
with -X- _ O
children, -X- _ O
who -X- _ O
acquire -X- _ O
shorter -X- _ O
words -X- _ O
earlier. -X- _ O

n-chars -X- _ O
There -X- _ O
was -X- _ O
a -X- _ O
negative -X- _ O
effect -X- _ O
of -X- _ O
n-chars -X- _ O
on -X- _ O
age -X- _ O
of -X- _ O
acquisition -X- _ O
in -X- _ O
all -X- _ O
four -X- _ O
language -X- _ O
models; -X- _ O
longer -X- _ O
words -X- _ O
were -X- _ O
learned -X- _ O
earlier. -X- _ O

The -X- _ O
positive -X- _ O
effect -X- _ O
of -X- _ O
MLU -X- _ O
in -X- _ O
other -X- _ O
models -X- _ O
suggests -X- _ O
that -X- _ O
complex -X- _ O
syntactic -X- _ O
contexts -X- _ O
may -X- _ O
be -X- _ O
more -X- _ O
difficult -X- _ O
to -X- _ O
learn -X- _ O
through -X- _ O
distributional -X- _ O
learning -X- _ O
alone, -X- _ O
which -X- _ O
might -X- _ O
partly -X- _ O
explain -X- _ O
why -X- _ O
children -X- _ O
learn -X- _ O
words -X- _ O
in -X- _ O
longer -X- _ O
utterances -X- _ O
more -X- _ O
slowly. -X- _ O

The -X- _ O
lack -X- _ O
of -X- _ O
effect -X- _ O
in -X- _ O
unidirectional -X- _ O
LSTMs -X- _ B-MethodName
could -X- _ O
simply -X- _ O
be -X- _ O
due -X- _ O
to -X- _ O
LSTMs -X- _ B-MethodName
being -X- _ O
the -X- _ O
least -X- _ O
sensitive -X- _ O
to -X- _ O
contextual -X- _ O
information -X- _ O
of -X- _ O
the -X- _ O
models -X- _ O
under -X- _ O
consideration. -X- _ O

Instead, -X- _ O
our -X- _ O
results -X- _ O
are -X- _ O
consistent -X- _ O
with -X- _ O
effects -X- _ O
of -X- _ O
MLU -X- _ B-MetricName
in -X- _ O
children; -X- _ B-MethodName
words -X- _ O
in -X- _ O
longer -X- _ O
utterances -X- _ O
are -X- _ O
learned -X- _ O
later, -X- _ O
even -X- _ O
after -X- _ O
accounting -X- _ O
for -X- _ O
other -X- _ O
variables. -X- _ O

Interestingly, -X- _ O
we -X- _ O
might -X- _ O
have -X- _ O
expected -X- _ O
the -X- _ O
opposite -X- _ O
effect -X- _ O
(particularly -X- _ O
in -X- _ O
Transformers) -X- _ O
if -X- _ O
additional -X- _ O
context -X- _ O
(longer -X- _ O
utterances) -X- _ O
facilitated -X- _ O
word -X- _ O
learning. -X- _ O

MLU -X- _ O
Except -X- _ O
in -X- _ O
unidirectional -X- _ O
LSTMs, -X- _ B-MethodName
MLU -X- _ B-MetricName
had -X- _ O
a -X- _ O
positive -X- _ O
effect -X- _ O
on -X- _ O
a -X- _ O
word’s -X- _ O
age -X- _ B-MetricName
of -X- _ I-MetricName
acquisition -X- _ I-MetricName
in -X- _ O
language -X- _ O
models. -X- _ O

The -X- _ O
sizeable -X- _ O
difference -X- _ O
in -X- _ O
log-frequency -X- _ O
predictivity -X- _ O
emphasizes -X- _ O
the -X- _ O
fact -X- _ O
that -X- _ O
language -X- _ O
models -X- _ O
learn -X- _ O
exclusively -X- _ O
from -X- _ O
distributional -X- _ O
statistics -X- _ O
over -X- _ O
words, -X- _ O
while -X- _ O
children -X- _ O
have -X- _ O
access -X- _ O
to -X- _ O
additional -X- _ O
social -X- _ O
and -X- _ O
sensorimotor -X- _ O
cues. -X- _ O

The -X- _ O
BiLSTM -X- _ B-MethodName
was -X- _ O
the -X- _ O
language -X- _ O
model -X- _ O
architecture -X- _ O
with -X- _ O
the -X- _ O
largest -X- _ O
effect -X- _ O
of -X- _ O
log-frequency -X- _ O
(adjusted -X- _ O
R2 -X- _ B-MetricName
= -X- _ O
0.94). -X- _ B-MetricValue

Figure -X- _ O
3: -X- _ O
Effects -X- _ O
of -X- _ O
log-frequency -X- _ O
on -X- _ O
words’ -X- _ O
ages -X- _ B-MetricName
of -X- _ I-MetricName
acquisition -X- _ I-MetricName
(AoA) -X- _ O
in -X- _ O
the -X- _ O
BiLSTM -X- _ B-MethodName
and -X- _ O
children. -X- _ B-MethodName

For -X- _ O
instance, -X- _ O
Korean -X- _ O
and -X- _ O
Mandarin-speaking -X- _ O
children -X- _ O
have -X- _ O
been -X- _ O
found -X- _ O
to -X- _ O
acquire -X- _ O
verbs -X- _ O
earlier -X- _ O
than -X- _ O
nouns, -X- _ O
although -X- _ O
this -X- _ O
effect -X- _ O
appears -X- _ O
sensitive -X- _ O
to -X- _ O
context -X- _ O
and -X- _ O
the -X- _ O
measure -X- _ O
of -X- _ O
vocabulary -X- _ B-MetricName
acquisition -X- _ I-MetricName
(Choi -X- _ O
and -X- _ O
Gopnik, -X- _ O
1995; -X- _ O
Tardif -X- _ O
et -X- _ O

7There -X- _ O
is -X- _ O
ongoing -X- _ O
debate -X- _ O
around -X- _ O
the -X- _ O
existence -X- _ O
of -X- _ O
a -X- _ O
universal -X- _ O
‘‘noun -X- _ O
bias’’ -X- _ O
in -X- _ O
early -X- _ O
word -X- _ O
acquisition. -X- _ O

Consistent -X- _ O
with -X- _ O
these -X- _ O
hypotheses, -X- _ O
our -X- _ O
results -X- _ O
suggest -X- _ O
that -X- _ O
English -X- _ O
verbs -X- _ O
and -X- _ O
adjectives -X- _ O
may -X- _ O
be -X- _ O
easier -X- _ O
to -X- _ O
learn -X- _ O
from -X- _ O
a -X- _ O
purely -X- _ O
distributional -X- _ O
perspective, -X- _ O
but -X- _ O
children -X- _ O
acquire -X- _ O
nouns -X- _ O
earlier -X- _ O
based -X- _ O
on -X- _ O
sensorimotor, -X- _ O
social, -X- _ O
or -X- _ O
cognitive -X- _ O
factors. -X- _ O

It -X- _ O
has -X- _ O
also -X- _ O
been -X- _ O
argued -X- _ O
that -X- _ O
children -X- _ O
might -X- _ O
have -X- _ O
an -X- _ O
innate -X- _ O
bias -X- _ O
to -X- _ O
learn -X- _ O
objects -X- _ O
earlier -X- _ O
than -X- _ O
relations -X- _ O
and -X- _ O
traits -X- _ O
(Markman, -X- _ O
1994). -X- _ O

This -X- _ O
result -X- _ O
is -X- _ O
compatible -X- _ O
with -X- _ O
the -X- _ O
hypothesis -X- _ O
that -X- _ O
nouns -X- _ O
are -X- _ O
acquired -X- _ O
earlier -X- _ O
because -X- _ O
they -X- _ O
often -X- _ O
map -X- _ O
to -X- _ O
real -X- _ O
world -X- _ O
objects; -X- _ O
function -X- _ O
words -X- _ O
might -X- _ O
be -X- _ O
acquired -X- _ O
later -X- _ O
because -X- _ O
their -X- _ O
meanings -X- _ O
are -X- _ O
less -X- _ O
grounded -X- _ O
in -X- _ O
sensorimotor -X- _ O
experience. -X- _ O

Thus, -X- _ O
children’s -X- _ O
early -X- _ O
acquisition -X- _ O
of -X- _ O
nouns -X- _ O
cannot -X- _ O
be -X- _ O
explained -X- _ O
by -X- _ O
distributional -X- _ O
properties -X- _ O
of -X- _ O
English -X- _ O
nouns, -X- _ O
which -X- _ O
are -X- _ O
acquired -X- _ O
later -X- _ O
by -X- _ O
unidirectional -X- _ O
language -X- _ O
models. -X- _ O

This -X- _ O
contrasts -X- _ O
with -X- _ O
children -X- _ O
learning -X- _ O
English, -X- _ O
who -X- _ O
on -X- _ O
average -X- _ O
acquired -X- _ O
nouns -X- _ O
earlier -X- _ O
than -X- _ O
adjectives -X- _ O
and -X- _ O
verbs, -X- _ O
acquiring -X- _ O
function -X- _ O
words -X- _ O
last.7 -X- _ O

However, -X- _ O
in -X- _ O
the -X- _ O
unidirectional -X- _ O
language -X- _ O
models -X- _ O
(GPT-2 -X- _ B-MethodName
and -X- _ O
the -X- _ O
LSTM), -X- _ B-MethodName
nouns -X- _ O
and -X- _ O
function -X- _ O
words -X- _ O
were -X- _ O
acquired -X- _ O
later -X- _ O
than -X- _ O
adjectives -X- _ O
and -X- _ O
verbs.6 -X- _ O

In -X- _ O
other -X- _ O
words, -X- _ O
the -X- _ O
differences -X- _ O
between -X- _ O
lexical -X- _ O
classes -X- _ O
were -X- _ O
sufficiently -X- _ O
accounted -X- _ O
for -X- _ O
by -X- _ O
the -X- _ O
other -X- _ O
predictors -X- _ O
for -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
the -X- _ O
BiLSTM. -X- _ B-MethodName

Lexical -X- _ O
Class -X- _ O
The -X- _ O
bidirectional -X- _ O
language -X- _ O
models -X- _ O
showed -X- _ O
no -X- _ O
significant -X- _ O
effects -X- _ O
of -X- _ O
lexical -X- _ O
class -X- _ O
on -X- _ O
age -X- _ O
of -X- _ O
acquisition. -X- _ O

Again, -X- _ O
this -X- _ O
highlights -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
sensorimotor -X- _ O
experience -X- _ O
and -X- _ O
conceptual -X- _ O
development -X- _ O
in -X- _ O
explaining -X- _ O
the -X- _ O
course -X- _ O
of -X- _ O
child -X- _ B-MethodName
language -X- _ B-TaskName
acquisition. -X- _ I-TaskName

This -X- _ O
entails -X- _ O
that -X- _ O
the -X- _ O
effects -X- _ O
in -X- _ O
children -X- _ O
cannot -X- _ O
be -X- _ O
explained -X- _ O
by -X- _ O
correlations -X- _ O
between -X- _ O
concrete -X- _ O
words -X- _ O
and -X- _ O
easier -X- _ O
distributional -X- _ O
learning -X- _ O
contexts. -X- _ O

5 -X- _ O
Concreteness -X- _ O
Although -X- _ O
children -X- _ O
overall -X- _ O
learn -X- _ O
more -X- _ O
concrete -X- _ O
words -X- _ O
earlier, -X- _ O
the -X- _ O
language -X- _ O
models -X- _ O
showed -X- _ O
no -X- _ O
significant -X- _ O
effects -X- _ O
of -X- _ O
concreteness -X- _ O
on -X- _ O
age -X- _ O
of -X- _ O
acquisition. -X- _ O

Regardless, -X- _ O
significant -X- _ O
effects -X- _ O
of -X- _ O
other -X- _ O
predictors -X- _ O
remained -X- _ O
the -X- _ O
same -X- _ O
when -X- _ O
using -X- _ O
a -X- _ O
quadratic -X- _ O
model -X- _ O
for -X- _ O
log-frequency. -X- _ O

A -X- _ O
quadratic -X- _ O
model -X- _ O
of -X- _ O
log-frequency -X- _ O
also -X- _ O
provided -X- _ O
a -X- _ O
slightly -X- _ O
better -X- _ O
fit -X- _ O
for -X- _ O
unidirectional -X- _ O
language -X- _ O
models -X- _ O
(R2 -X- _ B-MetricName
= -X- _ O
0.93 -X- _ B-MetricValue
to -X- _ O
0.94), -X- _ B-MetricValue
particularly -X- _ O
for -X- _ O
high-frequency -X- _ O
words; -X- _ O
in -X- _ O
language -X- _ O
models, -X- _ O
this -X- _ O
could -X- _ O
be -X- _ O
due -X- _ O
either -X- _ O
to -X- _ O
a -X- _ O
floor -X- _ O
effect -X- _ O
on -X- _ O
age -X- _ O
of -X- _ O
acquisition -X- _ O
for -X- _ O
high-frequency -X- _ O
words -X- _ O
or -X- _ O
to -X- _ O
slower -X- _ O
learning -X- _ O
of -X- _ O
function -X- _ O
words. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
3, -X- _ O
this -X- _ O
effect -X- _ O
was -X- _ O
much -X- _ O
more -X- _ O
pronounced -X- _ O
in -X- _ O
language -X- _ O
models -X- _ O
(adjusted -X- _ O
R2 -X- _ O
= -X- _ O
0.91 -X- _ O
to -X- _ O
0.94) -X- _ O
than -X- _ O
in -X- _ O
children -X- _ O
(adjusted -X- _ O
R2 -X- _ O
= -X- _ O
0.01).5 -X- _ O
5Because -X- _ O
function -X- _ O
words -X- _ O
are -X- _ O
frequent -X- _ O
but -X- _ O
acquired -X- _ O
later -X- _ O
by -X- _ O
children, -X- _ O
a -X- _ O
quadratic -X- _ O
model -X- _ O
of -X- _ O
log-frequency -X- _ O
on -X- _ O
age -X- _ O
of -X- _ O
acquisition -X- _ O
in -X- _ O
children -X- _ O
provided -X- _ O
a -X- _ O
slightly -X- _ O
better -X- _ O
fit -X- _ O
(R2 -X- _ O
= -X- _ O
0.03) -X- _ O
if -X- _ O
not -X- _ O
accounting -X- _ O
for -X- _ O
lexical -X- _ O
class. -X- _ O

Log-frequency -X- _ B-MetricName
In -X- _ O
children -X- _ O
and -X- _ O
all -X- _ O
four -X- _ O
language -X- _ O
models, -X- _ O
more -X- _ O
frequent -X- _ O
words -X- _ O
were -X- _ O
learned -X- _ O
earlier -X- _ O
(a -X- _ O
negative -X- _ O
effect -X- _ O
on -X- _ O
age -X- _ B-MetricName
of -X- _ I-MetricName
acquisition). -X- _ I-MetricName

4 -X- _ O
Results -X- _ O
Significant -X- _ O
predictors -X- _ O
of -X- _ O
age -X- _ O
of -X- _ O
acquisition -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
2, -X- _ O
comparing -X- _ O
children -X- _ O
and -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
four -X- _ O
language -X- _ O
model -X- _ O
architectures. -X- _ O

Instead, -X- _ O
the -X- _ O
models -X- _ O
serve -X- _ O
as -X- _ O
examples -X- _ O
of -X- _ O
relatively -X- _ O
successful -X- _ O
distributional -X- _ O
learners, -X- _ O
establishing -X- _ O
how -X- _ O
one -X- _ O
might -X- _ O
expect -X- _ O
word -X- _ O
acquisition -X- _ O
to -X- _ O
progress -X- _ O
according -X- _ O
to -X- _ O
effective -X- _ O
distributional -X- _ O
mechanisms. -X- _ O

Because -X- _ O
contemporary -X- _ O
language -X- _ O
models -X- _ O
require -X- _ O
much -X- _ O
more -X- _ O
data -X- _ O
than -X- _ O
children -X- _ O
hear, -X- _ O
the -X- _ O
models -X- _ O
do -X- _ O
not -X- _ O
necessarily -X- _ O
reflect -X- _ O
how -X- _ O
children -X- _ O
would -X- _ O
learn -X- _ O
if -X- _ O
restricted -X- _ O
solely -X- _ O
to -X- _ O
linguistic -X- _ O
input. -X- _ O

CHILDES -X- _ B-DatasetName
contained -X- _ O
7.5M -X- _ O
tokens, -X- _ O
while -X- _ O
the -X- _ O
language -X- _ O
model -X- _ O
corpus -X- _ O
contained -X- _ O
852.1M -X- _ O
tokens. -X- _ O

We -X- _ O
also -X- _ O
note -X- _ O
that -X- _ O
the -X- _ O
language -X- _ B-DatasetName
model -X- _ I-DatasetName
training -X- _ I-DatasetName
corpus -X- _ I-DatasetName
was -X- _ O
much -X- _ O
larger -X- _ O
overall -X- _ O
than -X- _ O
the -X- _ O
CHILDES -X- _ B-DatasetName
corpus. -X- _ O

We -X- _ O
computed -X- _ O
word -X- _ B-MetricName
frequencies -X- _ I-MetricName
and -X- _ O
MLUs -X- _ B-MetricName
separately -X- _ O
over -X- _ O
the -X- _ O
two -X- _ O
corpora -X- _ O
to -X- _ O
ensure -X- _ O
that -X- _ O
our -X- _ O
predictors -X- _ O
accurately -X- _ O
reflected -X- _ O
the -X- _ O
learning -X- _ O
environments -X- _ O
of -X- _ O
children -X- _ B-MethodName
and -X- _ O
the -X- _ O
language -X- _ O
models. -X- _ O

These -X- _ O
differences -X- _ O
were -X- _ O
likely -X- _ O
compounded -X- _ O
by -X- _ O
differences -X- _ O
between -X- _ O
spoken -X- _ O
language -X- _ O
in -X- _ O
the -X- _ O
CHILDES -X- _ B-DatasetName
corpus -X- _ O
and -X- _ O
written -X- _ O
language -X- _ O
in -X- _ O
the -X- _ O
language -X- _ O
model -X- _ O
corpus. -X- _ O

This -X- _ O
aligns -X- _ O
with -X- _ O
previous -X- _ O
work -X- _ O
finding -X- _ O
that -X- _ O
child-directed -X- _ O
speech -X- _ O
contains -X- _ O
on -X- _ O
average -X- _ O
fewer -X- _ O
words -X- _ O
per -X- _ O
utterance, -X- _ O
smaller -X- _ O
vocabularies, -X- _ O
and -X- _ O
simpler -X- _ O
syntactic -X- _ O
structures -X- _ O
than -X- _ O
adult-directed -X- _ O
speech -X- _ O
(Soderstrom, -X- _ O
2007). -X- _ O

CDI -X- _ O
word -X- _ O
log-frequencies -X- _ O
were -X- _ O
only -X- _ O
moderately -X- _ O
correlated -X- _ O
between -X- _ O
the -X- _ O
two -X- _ O
corpora -X- _ O
(r -X- _ O
= -X- _ O
0.78). -X- _ O

Notably, -X- _ O
CHILDES -X- _ B-DatasetName
contained -X- _ O
much -X- _ O
shorter -X- _ O
sentences -X- _ O
on -X- _ O
average -X- _ O
than -X- _ O
the -X- _ O
language -X- _ O
model -X- _ O
training -X- _ O
corpus -X- _ O
(mean -X- _ O
sentence -X- _ O
length -X- _ O
4.50 -X- _ O
tokens -X- _ O
compared -X- _ O
to -X- _ O
15.14 -X- _ O
tokens). -X- _ O

When -X- _ O
predicting -X- _ O
ages -X- _ B-MetricName
of -X- _ I-MetricName
acquisition -X- _ I-MetricName
in -X- _ O
children, -X- _ B-MethodName
we -X- _ O
computed -X- _ O
word -X- _ B-MetricName
frequencies -X- _ I-MetricName
and -X- _ B-MetricName
utterance -X- _ I-MetricName
lengths -X- _ I-MetricName
over -X- _ O
the -X- _ O
North -X- _ B-DatasetName
American -X- _ I-DatasetName
English -X- _ I-DatasetName
CHILDES -X- _ I-DatasetName
corpus -X- _ I-DatasetName
of -X- _ O
child-directed -X- _ O
speech -X- _ O
(MacWhinney, -X- _ O
2000). -X- _ O

3.4 -X- _ O
Age -X- _ B-MetricName
of -X- _ I-MetricName
Acquisition -X- _ I-MetricName
in -X- _ O
Children -X- _ B-MethodName
For -X- _ O
comparison, -X- _ O
we -X- _ O
used -X- _ O
the -X- _ O
same -X- _ O
variables -X- _ O
to -X- _ O
predict -X- _ O
words’ -X- _ O
ages -X- _ B-MetricName
of -X- _ I-MetricName
acquisition -X- _ I-MetricName
in -X- _ O
children, -X- _ B-MethodName
as -X- _ O
in -X- _ O
Braginsky -X- _ O
et -X- _ O

We -X- _ O
used -X- _ O
Tukey’s -X- _ O
honestly -X- _ O
significant -X- _ O
difference -X- _ O
(HSD) -X- _ O
test -X- _ O
to -X- _ O
assess -X- _ O
pairwise -X- _ O
differences -X- _ O
between -X- _ O
lexical -X- _ O
classes. -X- _ O

The -X- _ O
ANCOVA -X- _ O
ran -X- _ O
a -X- _ O
standard -X- _ O
ANOVA -X- _ O
on -X- _ O
the -X- _ O
age -X- _ O
of -X- _ O
acquisition -X- _ O
residuals -X- _ O
after -X- _ O
regressing -X- _ O
over -X- _ O
log-frequency. -X- _ O

When -X- _ O
lexical -X- _ O
class -X- _ O
(the -X- _ O
sole -X- _ O
categorical -X- _ O
predictor) -X- _ O
reached -X- _ O
significance -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
likelihood -X- _ O
ratio -X- _ O
test, -X- _ O
we -X- _ O
ran -X- _ O
a -X- _ O
one-way -X- _ O
analysis -X- _ O
of -X- _ O
covariance -X- _ O
(ANCOVA) -X- _ O
with -X- _ O
log-frequency -X- _ O
as -X- _ O
a -X- _ O
covariate. -X- _ O

The -X- _ O
R2 -X- _ B-MetricName
denotes -X- _ O
the -X- _ O
adjusted -X- _ B-MetricName
R2 -X- _ I-MetricName
in -X- _ O
a -X- _ O
regression -X- _ O
using -X- _ O
all -X- _ O
five -X- _ O
predictors. -X- _ O

Signs -X- _ O
of -X- _ O
coefficients -X- _ O
are -X- _ O
notated -X- _ O
in -X- _ O
parentheses. -X- _ O

4 -X- _ O
Log-frequency -X- _ O
MLU -X- _ O
n-chars -X- _ O
Concreteness -X- _ B-MetricName
Lexical -X- _ O
class -X- _ O
R2 -X- _ O
LSTM -X- _ B-MethodName
∗∗∗(−) -X- _ O
∗∗∗(−) -X- _ O
GPT-2 -X- _ B-MethodName
∗∗∗(−) -X- _ O
∗∗(+) -X- _ O
∗∗∗(−) -X- _ O
BiLSTM -X- _ B-MethodName
∗∗∗(−) -X- _ O
∗∗∗(+) -X- _ O
∗∗∗(−) -X- _ O
BERT -X- _ B-MethodName
∗∗∗(−) -X- _ O
∗∗∗(+) -X- _ O
∗∗∗(−) -X- _ O
∗∗∗ -X- _ O
∗∗∗ -X- _ B-MethodName
Children -X- _ I-MethodName
∗∗∗(−) -X- _ O
∗∗∗(+) -X- _ O
∗∗(+) -X- _ O
∗∗∗(−) -X- _ O
∗∗∗ -X- _ O
0.93 -X- _ B-MetricValue
0.92 -X- _ B-MetricValue
0.95 -X- _ B-MetricValue
0.94 -X- _ B-MetricValue
0.43 -X- _ B-MetricValue
Table -X- _ O
2: -X- _ O
Significant -X- _ O
predictors -X- _ O
for -X- _ O
a -X- _ O
word’s -X- _ O
age -X- _ O
of -X- _ O
acquisition -X- _ O
are -X- _ O
marked -X- _ O
by -X- _ O
asterisks -X- _ O
(p -X- _ O
< -X- _ O
0.05∗; -X- _ O
p -X- _ O
< -X- _ O
0.01∗∗; -X- _ O
p -X- _ O
< -X- _ O
0.001∗∗∗). -X- _ O

In -X- _ O
all -X- _ O
cases, -X- _ O
the -X- _ O
coefficient -X- _ O
sign -X- _ O
in -X- _ O
the -X- _ O
adjusted -X- _ O
single -X- _ O
predictor -X- _ O
regression -X- _ O
was -X- _ O
consistent -X- _ O
with -X- _ O
the -X- _ O
sign -X- _ O
of -X- _ O
the -X- _ O
coefficient -X- _ O
in -X- _ O
the -X- _ O
overall -X- _ O
regression. -X- _ O

To -X- _ O
ease -X- _ O
collinearity -X- _ O
concerns, -X- _ O
we -X- _ O
considered -X- _ O
singlepredictor -X- _ O
regressions -X- _ O
for -X- _ O
each -X- _ O
predictor, -X- _ O
using -X- _ O
adjusted -X- _ O
predictor -X- _ O
values -X- _ O
after -X- _ O
accounting -X- _ O
for -X- _ O
logfrequency -X- _ O
(residuals -X- _ O
after -X- _ O
regressing -X- _ O
the -X- _ O
predictor -X- _ O
over -X- _ O
log-frequency). -X- _ O

No -X- _ O
VIF -X- _ O
exceeded -X- _ O
5.0,4 -X- _ O
although -X- _ O
we -X- _ O
did -X- _ O
observe -X- _ O
moderate -X- _ O
correlations -X- _ O
between -X- _ O
log-frequency -X- _ O
and -X- _ O
n-chars -X- _ O
(r -X- _ O
= -X- _ O
−0.49), -X- _ O
and -X- _ O
between -X- _ O
log-frequency -X- _ O
and -X- _ O
concreteness -X- _ O
(r -X- _ O
= -X- _ O
−0.64). -X- _ O

As -X- _ O
a -X- _ O
potential -X- _ O
concern -X- _ O
for -X- _ O
interpreting -X- _ O
regression -X- _ O
coefficient -X- _ O
signs, -X- _ O
we -X- _ O
assessed -X- _ O
collinearities -X- _ O
between -X- _ O
predictors -X- _ O
by -X- _ O
computing -X- _ O
the -X- _ O
variance -X- _ O
inflation -X- _ O
factor -X- _ O
(VIF) -X- _ O
for -X- _ O
each -X- _ O
predictor. -X- _ O

We -X- _ O
ran -X- _ O
linear -X- _ O
regressions -X- _ O
with -X- _ O
linear -X- _ O
terms -X- _ O
for -X- _ O
each -X- _ O
predictor. -X- _ O

To -X- _ O
determine -X- _ O
statistical -X- _ O
significance -X- _ O
for -X- _ O
each -X- _ O
predictor, -X- _ O
we -X- _ O
ran -X- _ O
likelihood -X- _ O
ratio -X- _ O
tests, -X- _ O
comparing -X- _ O
the -X- _ O
overall -X- _ O
regression -X- _ O
(including -X- _ O
the -X- _ O
target -X- _ O
predictor) -X- _ O
with -X- _ O
a -X- _ O
regression -X- _ O
including -X- _ O
all -X- _ O
predictors -X- _ O
except -X- _ O
the -X- _ O
target. -X- _ O

To -X- _ O
determine -X- _ O
the -X- _ O
direction -X- _ O
of -X- _ O
effect -X- _ O
for -X- _ O
each -X- _ O
continuous -X- _ O
predictor, -X- _ O
we -X- _ O
used -X- _ O
the -X- _ O
sign -X- _ O
of -X- _ O
the -X- _ O
coefficient -X- _ O
in -X- _ O
the -X- _ O
overall -X- _ O
regression. -X- _ O

When -X- _ O
predicting -X- _ O
ages -X- _ O
of -X- _ O
acquisition -X- _ O
in -X- _ O
language -X- _ O
models, -X- _ O
we -X- _ O
computed -X- _ O
word -X- _ B-MetricName
frequencies -X- _ I-MetricName
and -X- _ O
utterance -X- _ B-MetricName
lengths -X- _ I-MetricName
over -X- _ O
the -X- _ O
language -X- _ O
model -X- _ O
training -X- _ O
corpus. -X- _ O

We -X- _ O
defined -X- _ O
age -X- _ B-MetricName
of -X- _ I-MetricName
acquisition -X- _ I-MetricName
for -X- _ O
a -X- _ O
language -X- _ O
model -X- _ O
as -X- _ O
the -X- _ O
corresponding -X- _ O
training -X- _ O
step, -X- _ O
on -X- _ O
a -X- _ O
log10 -X- _ O
scale. -X- _ O

For -X- _ O
each -X- _ O
learning -X- _ O
curve, -X- _ O
we -X- _ O
found -X- _ O
the -X- _ O
intersection -X- _ O
between -X- _ O
the -X- _ O
fitted -X- _ O
sigmoid -X- _ O
and -X- _ O
the -X- _ O
cutoff -X- _ O
surprisal -X- _ O
value. -X- _ O

We -X- _ O
selected -X- _ O
minimum -X- _ O
surprisal -X- _ O
as -X- _ O
our -X- _ O
other -X- _ O
bound -X- _ O
to -X- _ O
reflect -X- _ O
how -X- _ O
well -X- _ O
a -X- _ O
particular -X- _ O
word -X- _ O
can -X- _ O
eventually -X- _ O
be -X- _ O
learned -X- _ O
by -X- _ O
a -X- _ O
particular -X- _ O
language -X- _ O
model, -X- _ O
analogous -X- _ O
to -X- _ O
an -X- _ O
adult’s -X- _ O
understanding -X- _ O
of -X- _ O
a -X- _ O
given -X- _ O
word. -X- _ O

Following -X- _ O
this -X- _ O
precedent, -X- _ O
we -X- _ O
determined -X- _ O
our -X- _ O
cutoff -X- _ B-HyperparameterName
to -X- _ O
be -X- _ O
50% -X- _ B-HyperparameterValue
between -X- _ O
a -X- _ O
baseline -X- _ O
surprisal -X- _ O
(predicting -X- _ O
words -X- _ O
based -X- _ O
on -X- _ O
random -X- _ O
chance) -X- _ O
and -X- _ O
the -X- _ O
minimum -X- _ O
surprisal -X- _ O
attained -X- _ O
by -X- _ O
the -X- _ O
model -X- _ O
for -X- _ O
word -X- _ O
w. -X- _ O
We -X- _ O
selected -X- _ O
the -X- _ O
random -X- _ O
chance -X- _ O
baseline -X- _ O
to -X- _ O
best -X- _ O
reflect -X- _ O
a -X- _ O
language -X- _ O
model’s -X- _ O
ability -X- _ O
to -X- _ O
predict -X- _ O
a -X- _ O
word -X- _ O
with -X- _ O
no -X- _ O
access -X- _ O
to -X- _ O
any -X- _ O
training -X- _ O
data, -X- _ O
similar -X- _ O
3 -X- _ O
Figure -X- _ O
2: -X- _ O
Learning -X- _ B-MetricName
curves -X- _ I-MetricName
for -X- _ O
the -X- _ O
word -X- _ O
‘‘eat’’ -X- _ O
for -X- _ O
all -X- _ O
four -X- _ O
language -X- _ O
model -X- _ O
architectures. -X- _ O

Age -X- _ B-MetricName
of -X- _ I-MetricName
Acquisition -X- _ I-MetricName
To -X- _ O
extract -X- _ O
age -X- _ B-MetricName
of -X- _ I-MetricName
acquisition -X- _ I-MetricName
from -X- _ O
a -X- _ O
learning -X- _ B-MetricName
curve, -X- _ I-MetricName
we -X- _ O
established -X- _ O
a -X- _ O
cutoff -X- _ B-HyperparameterName
surprisal -X- _ I-HyperparameterName
where -X- _ O
we -X- _ O
considered -X- _ O
a -X- _ O
given -X- _ O
word -X- _ O
‘‘learned.’’ -X- _ O

For -X- _ O
each -X- _ O
learning -X- _ B-MetricName
curve -X- _ I-MetricName
(4 -X- _ O
language -X- _ O
model -X- _ O
architectures -X- _ O
× -X- _ O
611 -X- _ O
words), -X- _ O
we -X- _ O
fitted -X- _ O
a -X- _ O
sigmoid -X- _ O
function -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
smoothed -X- _ O
acquisition -X- _ O
of -X- _ O
word -X- _ O
w. -X- _ O
Sample -X- _ O
learning -X- _ O
curves -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Figures -X- _ O
1 -X- _ O
and -X- _ O
2. -X- _ O

By -X- _ O
plotting -X- _ O
surprisals -X- _ B-MetricName
over -X- _ O
the -X- _ O
course -X- _ O
of -X- _ O
training, -X- _ O
we -X- _ O
obtained -X- _ O
a -X- _ O
learning -X- _ B-MetricName
curve -X- _ I-MetricName
for -X- _ O
each -X- _ O
word, -X- _ O
generally -X- _ O
moving -X- _ O
from -X- _ O
high -X- _ O
surprisal -X- _ B-MetricName
to -X- _ O
low -X- _ O
surprisal. -X- _ B-MetricName

We -X- _ O
computed -X- _ O
this -X- _ O
average -X- _ B-MetricName
surprisal -X- _ I-MetricName
for -X- _ O
each -X- _ O
target -X- _ O
word -X- _ O
at -X- _ O
approximately -X- _ O
200 -X- _ O
different -X- _ O
steps -X- _ O
during -X- _ O
language -X- _ O
model -X- _ O
training, -X- _ O
sampling -X- _ O
more -X- _ O
heavily -X- _ O
from -X- _ O
earlier -X- _ O
training -X- _ O
steps, -X- _ O
prior -X- _ O
to -X- _ O
model -X- _ O
convergence. -X- _ O

The -X- _ O
blue -X- _ O
curve -X- _ O
represents -X- _ O
the -X- _ O
fitted -X- _ O
sigmoid -X- _ O
function -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
language -X- _ O
model -X- _ O
surprisals -X- _ O
during -X- _ O
training -X- _ O
(black). -X- _ O

Blue -X- _ O
horizontal -X- _ O
lines -X- _ O
indicate -X- _ O
age -X- _ B-MetricName
of -X- _ I-MetricName
acquisition -X- _ I-MetricName
cutoffs. -X- _ I-MetricName

Figure -X- _ O
1: -X- _ B-MetricName
Learning -X- _ I-MetricName
curves -X- _ I-MetricName
for -X- _ O
the -X- _ O
word -X- _ O
‘‘walk’’ -X- _ O
in -X- _ O
a -X- _ O
BERT -X- _ B-MethodName
language -X- _ O
model -X- _ O
and -X- _ O
human -X- _ B-MethodName
children. -X- _ I-MethodName

Most -X- _ O
tokens -X- _ O
(92.3%) -X- _ O
had -X- _ O
the -X- _ O
maximum -X- _ O
of -X- _ O
512 -X- _ O
samples -X- _ O
both -X- _ O
unidirectionally -X- _ O
and -X- _ O
bidirectionally, -X- _ O
and -X- _ O
all -X- _ O
tokens -X- _ O
had -X- _ O
at -X- _ O
least -X- _ O
100 -X- _ O
samples -X- _ O
in -X- _ O
both -X- _ O
cases. -X- _ O

Thus, -X- _ O
the -X- _ O
unidirectional -X- _ O
and -X- _ O
bidirectional -X- _ O
samples -X- _ O
differed -X- _ O
slightly. -X- _ O

For -X- _ O
each -X- _ O
such -X- _ O
token -X- _ O
w, -X- _ O
we -X- _ O
identified -X- _ O
up -X- _ O
to -X- _ O
512 -X- _ O
occurrences -X- _ O
of -X- _ O
w -X- _ O
in -X- _ O
the -X- _ O
held-out -X- _ O
portion -X- _ O
of -X- _ O
the -X- _ O
language -X- _ O
modeling -X- _ O
dataset.2 -X- _ O
To -X- _ O
evaluate -X- _ O
a -X- _ O
language -X- _ O
model -X- _ O
at -X- _ O
training -X- _ O
step -X- _ O
s, -X- _ O
we -X- _ O
fed -X- _ O
each -X- _ O
sentence -X- _ O
pair -X- _ O
into -X- _ O
the -X- _ O
model, -X- _ O
attempting -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
masked -X- _ O
token -X- _ O
w. -X- _ O
We -X- _ O
computed -X- _ O
the -X- _ O
surprisal: -X- _ O
2We -X- _ O
only -X- _ O
selected -X- _ O
sentence -X- _ O
pairs -X- _ O
with -X- _ O
at -X- _ O
least -X- _ O
eight -X- _ B-HyperparameterValue
tokens -X- _ O
of -X- _ O
context, -X- _ O
unidirectionally -X- _ O
or -X- _ O
bidirectionally -X- _ O
depending -X- _ O
on -X- _ O
model -X- _ O
architecture. -X- _ O

We -X- _ O
considered -X- _ O
all -X- _ O
words -X- _ O
in -X- _ O
the -X- _ O
CDI -X- _ B-DatasetName
that -X- _ O
were -X- _ O
considered -X- _ O
one -X- _ O
token -X- _ O
by -X- _ O
the -X- _ O
language -X- _ O
models -X- _ O
(611 -X- _ O
out -X- _ O
of -X- _ O
651 -X- _ O
words). -X- _ O

3.2 -X- _ O
Learning -X- _ B-MetricName
Curves -X- _ I-MetricName
and -X- _ O
Ages -X- _ B-MetricName
of -X- _ I-MetricName
Acquisition -X- _ I-MetricName
We -X- _ O
sought -X- _ O
to -X- _ O
quantify -X- _ O
each -X- _ O
language -X- _ O
model’s -X- _ O
ability -X- _ O
to -X- _ O
predict -X- _ O
individual -X- _ O
words -X- _ O
over -X- _ O
the -X- _ O
course -X- _ O
of -X- _ O
training. -X- _ O

Similar -X- _ O
to -X- _ O
GPT-2, -X- _ B-MethodName
the -X- _ O
unidirectional -X- _ B-MethodName
LSTM -X- _ I-MethodName
predicted -X- _ O
the -X- _ O
token -X- _ O
at -X- _ O
time -X- _ O
t -X- _ O
from -X- _ O
the -X- _ O
hidden -X- _ O
state -X- _ O
at -X- _ O
time -X- _ O
t -X- _ O
− -X- _ O
1. -X- _ O

LSTMs -X- _ B-MethodName
We -X- _ O
also -X- _ O
trained -X- _ O
both -X- _ O
a -X- _ O
unidirectional -X- _ B-MethodName
and -X- _ O
bidirectional -X- _ B-MethodName
LSTM -X- _ I-MethodName
language -X- _ O
model, -X- _ O
each -X- _ O
with -X- _ O
three -X- _ B-HyperparameterValue
stacked -X- _ B-HyperparameterName
LSTM -X- _ I-HyperparameterName
layers. -X- _ I-HyperparameterName

where -X- _ O
masked -X- _ O
tokens -X- _ O
are -X- _ O
predicted -X- _ O
from -X- _ O
surrounding -X- _ O
tokens -X- _ O
in -X- _ O
both -X- _ O
directions. -X- _ O

Additional -X- _ O
perplexity -X- _ B-MetricName
comparisons -X- _ O
with -X- _ O
comparable -X- _ O
models -X- _ O
are -X- _ O
included -X- _ O
in -X- _ O
Appendix -X- _ O
A.1. -X- _ O

For -X- _ O
reference, -X- _ O
the -X- _ O
pre-trained -X- _ O
BERT -X- _ B-MethodName
base -X- _ O
model -X- _ O
from -X- _ O
Huggingface -X- _ O
reached -X- _ O
a -X- _ O
perplexity -X- _ B-MetricName
of -X- _ O
9.4 -X- _ B-MetricValue
on -X- _ O
our -X- _ O
evaluation -X- _ O
set. -X- _ O

In -X- _ O
contrast, -X- _ O
BERT -X- _ B-MethodName
used -X- _ O
the -X- _ O
masked -X- _ O
language -X- _ O
modeling -X- _ O
objective, -X- _ O
1Code -X- _ O
and -X- _ O
data -X- _ O
are -X- _ O
available -X- _ O
at -X- _ O
https://github.com -X- _ O
/tylerachang/word-acquisition-language-models. -X- _ O

GPT-2 -X- _ B-MethodName
was -X- _ O
trained -X- _ O
with -X- _ O
the -X- _ O
causal -X- _ O
language -X- _ O
modeling -X- _ O
objective, -X- _ O
where -X- _ O
each -X- _ O
token -X- _ O
representation -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
next -X- _ O
token; -X- _ O
the -X- _ O
masked -X- _ O
self-attention -X- _ O
mechanism -X- _ O
allows -X- _ O
tokens -X- _ O
to -X- _ O
attend -X- _ O
only -X- _ O
to -X- _ O
previous -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
sequence. -X- _ O

We -X- _ O
include -X- _ O
evaluation -X- _ O
loss -X- _ O
curves, -X- _ O
full -X- _ O
training -X- _ O
details, -X- _ O
and -X- _ O
hyperparameters -X- _ O
in -X- _ O
Appendix -X- _ O
A.1. -X- _ O

As -X- _ O
a -X- _ O
metric -X- _ O
for -X- _ O
overall -X- _ O
language -X- _ O
model -X- _ O
performance, -X- _ O
we -X- _ O
report -X- _ O
evaluation -X- _ B-MetricName
perplexity -X- _ I-MetricName
scores -X- _ O
in -X- _ O
Table -X- _ O
1. -X- _ O

Models -X- _ O
were -X- _ O
trained -X- _ O
for -X- _ O
1M -X- _ O
steps, -X- _ O
with -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
128 -X- _ B-HyperparameterValue
and -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
0.0001. -X- _ B-HyperparameterValue

Sentences -X- _ O
were -X- _ O
tokenized -X- _ O
using -X- _ O
the -X- _ O
unigram -X- _ O
language -X- _ O
model -X- _ O
tokenizer -X- _ O
implemented -X- _ O
in -X- _ O
SentencePiece -X- _ O
(Kudo -X- _ O
and -X- _ O
Richardson, -X- _ O
2018). -X- _ O

The -X- _ O
remaining -X- _ O
sentences -X- _ O
(5.8M -X- _ O
pairs) -X- _ O
were -X- _ O
used -X- _ O
for -X- _ O
evaluation -X- _ O
and -X- _ O
to -X- _ O
generate -X- _ O
word -X- _ B-MetricName
learning -X- _ I-MetricName
curves. -X- _ I-MetricName

(2019), -X- _ O
each -X- _ O
input -X- _ O
sequence -X- _ O
was -X- _ O
a -X- _ O
sentence -X- _ O
pair; -X- _ O
the -X- _ O
training -X- _ O
dataset -X- _ O
consisted -X- _ O
of -X- _ O
25.6M -X- _ O
sentence -X- _ O
pairs. -X- _ O

3 -X- _ O
Method -X- _ O
We -X- _ O
trained -X- _ O
unidirectional -X- _ B-MethodName
and -X- _ O
bidirectional -X- _ B-MethodName
language -X- _ I-MethodName
models -X- _ I-MethodName
with -X- _ O
LSTM -X- _ B-MethodName
and -X- _ O
Transformer -X- _ B-MethodName
architectures. -X- _ O

Our -X- _ O
work -X- _ O
seeks -X- _ O
to -X- _ O
bridge -X- _ O
this -X- _ O
gap. -X- _ O

That -X- _ O
said, -X- _ O
previous -X- _ O
language -X- _ O
model -X- _ O
evaluation -X- _ O
studies -X- _ O
have -X- _ O
focused -X- _ O
on -X- _ O
fully-trained -X- _ O
models, -X- _ O
progressing -X- _ O
largely -X- _ O
independently -X- _ O
from -X- _ O
human -X- _ B-TaskName
language -X- _ I-TaskName
acquisition -X- _ I-TaskName
literature. -X- _ O

Notably, -X- _ O
because -X- _ O
these -X- _ O
approaches -X- _ O
rely -X- _ O
only -X- _ O
on -X- _ O
output -X- _ O
token -X- _ O
probabilities -X- _ O
from -X- _ O
a -X- _ O
given -X- _ O
language -X- _ O
model, -X- _ O
they -X- _ O
are -X- _ O
well -X- _ O
suited -X- _ O
to -X- _ O
evaluations -X- _ O
early -X- _ O
in -X- _ O
training, -X- _ O
when -X- _ O
fine-tuning -X- _ O
on -X- _ O
downstream -X- _ O
tasks -X- _ O
is -X- _ O
unfruitful. -X- _ O

These -X- _ O
psycholinguistic -X- _ O
methodologies -X- _ O
do -X- _ O
not -X- _ O
rely -X- _ O
on -X- _ O
specific -X- _ O
language -X- _ O
model -X- _ O
architectures -X- _ O
or -X- _ O
fine-tuning -X- _ O
on -X- _ O
a -X- _ O
probe -X- _ O
task. -X- _ O

However, -X- _ O
no -X- _ O
studies -X- _ O
have -X- _ O
evaluated -X- _ O
ages -X- _ O
of -X- _ O
acquisition -X- _ O
in -X- _ O
language -X- _ O
models -X- _ O
themselves. -X- _ O

For -X- _ O
instance, -X- _ O
Ettinger -X- _ O
(2020) -X- _ O
used -X- _ O
the -X- _ O
output -X- _ O
token -X- _ O
probabilities -X- _ O
from -X- _ O
BERT -X- _ B-MethodName
in -X- _ O
carefully -X- _ O
constructed -X- _ O
sentences, -X- _ O
finding -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
learns -X- _ O
commonsense -X- _ B-MetricName
and -X- _ O
semantic -X- _ B-MetricName
relations -X- _ I-MetricName
to -X- _ O
some -X- _ O
degree, -X- _ O
although -X- _ O
it -X- _ O
struggles -X- _ O
with -X- _ O
negation. -X- _ B-MetricName

2.1 -X- _ O
Child -X- _ B-MethodName
Word -X- _ O
Acquisition -X- _ O
Child -X- _ O
development -X- _ O
researchers -X- _ O
have -X- _ O
previously -X- _ O
studied -X- _ O
word -X- _ B-TaskName
acquisition -X- _ I-TaskName
in -X- _ O
children, -X- _ B-MethodName
identifying -X- _ O
variables -X- _ O
that -X- _ O
help -X- _ O
predict -X- _ O
words’ -X- _ B-TaskName
ages -X- _ I-TaskName
of -X- _ I-TaskName
acquisition -X- _ I-TaskName
in -X- _ O
children. -X- _ B-MethodName

In -X- _ O
this -X- _ O
section, -X- _ O
we -X- _ O
briefly -X- _ O
outline -X- _ O
both -X- _ O
lines -X- _ O
of -X- _ O
research. -X- _ O

Understanding -X- _ O
how -X- _ O
language -X- _ O
models -X- _ O
acquire -X- _ O
language -X- _ O
can -X- _ O
lead -X- _ O
to -X- _ O
better -X- _ O
architectures -X- _ O
and -X- _ O
task -X- _ O
designs -X- _ O
for -X- _ O
future -X- _ O
models, -X- _ O
while -X- _ O
also -X- _ O
providing -X- _ O
insights -X- _ O
into -X- _ O
distributional -X- _ O
learning -X- _ O
mechanisms -X- _ O
in -X- _ O
people. -X- _ O

Finally, -X- _ O
to -X- _ O
better -X- _ O
understand -X- _ O
how -X- _ O
computational -X- _ O
models -X- _ O
acquire -X- _ O
language, -X- _ O
we -X- _ O
identify -X- _ O
consistent -X- _ O
patterns -X- _ O
in -X- _ O
language -X- _ O
model -X- _ O
training -X- _ O
across -X- _ O
architectures. -X- _ O

Each -X- _ O
of -X- _ O
our -X- _ O
selected -X- _ O
variables -X- _ O
has -X- _ O
effects -X- _ O
on -X- _ O
words’ -X- _ O
ages -X- _ O
of -X- _ O
acquisition -X- _ O
in -X- _ O
children; -X- _ O
our -X- _ O
language -X- _ O
model -X- _ O
results -X- _ O
allow -X- _ O
us -X- _ O
to -X- _ O
identify -X- _ O
the -X- _ O
extent -X- _ O
to -X- _ O
which -X- _ O
each -X- _ O
effect -X- _ O
in -X- _ O
children -X- _ O
can -X- _ O
or -X- _ O
cannot -X- _ O
be -X- _ O
attributed -X- _ O
in -X- _ O
principle -X- _ O
to -X- _ O
distributional -X- _ O
learning -X- _ O
mechanisms. -X- _ O

We -X- _ O
consider -X- _ O
how -X- _ O
variables -X- _ O
such -X- _ O
as -X- _ O
word -X- _ B-HyperparameterName
frequency, -X- _ I-HyperparameterName
concreteness, -X- _ B-HyperparameterName
and -X- _ O
lexical -X- _ B-HyperparameterName
class -X- _ I-HyperparameterName
contribute -X- _ O
to -X- _ O
words’ -X- _ B-TaskName
ages -X- _ I-TaskName
of -X- _ I-TaskName
acquisition -X- _ I-TaskName
in -X- _ O
language -X- _ O
models. -X- _ O

As -X- _ O
an -X- _ O
initial -X- _ O
step -X- _ O
towards -X- _ O
bridging -X- _ O
the -X- _ O
gap -X- _ O
between -X- _ O
language -X- _ B-TaskName
acquisition -X- _ I-TaskName
and -X- _ O
language -X- _ B-TaskName
modeling, -X- _ I-TaskName
we -X- _ O
present -X- _ O
an -X- _ O
empirical -X- _ O
study -X- _ O
of -X- _ O
word -X- _ B-TaskName
acquisition -X- _ I-TaskName
during -X- _ O
training -X- _ O
in -X- _ O
contemporary -X- _ O
language -X- _ O
models, -X- _ O
including -X- _ O
LSTMs, -X- _ B-MethodName
GPT-2, -X- _ B-MethodName
and -X- _ O
BERT. -X- _ B-MethodName

There -X- _ O
remains -X- _ O
a -X- _ O
lack -X- _ O
of -X- _ O
research -X- _ O
on -X- _ O
language -X- _ B-TaskName
acquisition -X- _ I-TaskName
in -X- _ O
contemporary -X- _ O
language -X- _ O
models, -X- _ O
which -X- _ O
encode -X- _ O
higher -X- _ O
level -X- _ O
features -X- _ O
such -X- _ O
as -X- _ O
syntax -X- _ O
and -X- _ O
semantics. -X- _ O

There -X- _ O
are -X- _ O
limited -X- _ O
exceptions. -X- _ O

However, -X- _ O
these -X- _ O
studies -X- _ O
focused -X- _ O
only -X- _ O
on -X- _ O
sub-word -X- _ O
features. -X- _ O

Rumelhart -X- _ O
and -X- _ O
McClelland -X- _ O
(1986) -X- _ O
famously -X- _ O
studied -X- _ O
past -X- _ O
tense -X- _ O
verb -X- _ O
form -X- _ O
learning -X- _ O
in -X- _ O
phoneme-level -X- _ O
neural -X- _ O
networks -X- _ O
during -X- _ O
training, -X- _ O
a -X- _ O
study -X- _ O
which -X- _ O
was -X- _ O
replicated -X- _ O
in -X- _ O
more -X- _ O
modern -X- _ O
character-level -X- _ B-MethodName
recurrent -X- _ I-MethodName
neural -X- _ I-MethodName
networks -X- _ I-MethodName
(Kirov -X- _ O
and -X- _ O
Cotterell, -X- _ O
2018). -X- _ O

However, -X- _ O
previous -X- _ O
psycholinguistic -X- _ O
studies -X- _ O
of -X- _ O
language -X- _ O
models -X- _ O
have -X- _ O
nearly -X- _ O
always -X- _ O
focused -X- _ O
on -X- _ O
fully-trained -X- _ O
models, -X- _ O
precluding -X- _ O
comparisons -X- _ O
to -X- _ O
the -X- _ O
wealth -X- _ O
of -X- _ O
literature -X- _ O
on -X- _ O
human -X- _ B-TaskName
language -X- _ I-TaskName
acquisition. -X- _ I-TaskName

From -X- _ O
a -X- _ O
cognitive -X- _ O
perspective, -X- _ O
language -X- _ O
models -X- _ O
are -X- _ O
of -X- _ O
theoretical -X- _ O
interest -X- _ O
as -X- _ O
distributional -X- _ O
models -X- _ O
of -X- _ O
language, -X- _ O
agents -X- _ O
that -X- _ O
learn -X- _ O
exclusively -X- _ O
from -X- _ O
statistics -X- _ O
over -X- _ O
language -X- _ O
(Boleda, -X- _ O
2020; -X- _ O
Lenci, -X- _ O
2018). -X- _ O

These -X- _ O
results -X- _ O
shed -X- _ O
light -X- _ O
on -X- _ O
the -X- _ O
role -X- _ O
of -X- _ O
distributional -X- _ O
learning -X- _ O
mechanisms -X- _ O
in -X- _ O
children, -X- _ B-MethodName
while -X- _ O
also -X- _ O
providing -X- _ O
insights -X- _ O
for -X- _ O
more -X- _ O
human-like -X- _ O
language -X- _ B-TaskName
acquisition -X- _ I-TaskName
in -X- _ O
language -X- _ O
models. -X- _ O

Models -X- _ O
predict -X- _ O
based -X- _ O
on -X- _ O
unigram -X- _ O
token -X- _ O
frequencies -X- _ O
early -X- _ O
in -X- _ O
training, -X- _ O
before -X- _ O
transitioning -X- _ O
loosely -X- _ O
to -X- _ O
bigram -X- _ O
probabilities, -X- _ O
eventually -X- _ O
converging -X- _ O
on -X- _ O
more -X- _ O
nuanced -X- _ O
predictions. -X- _ O

Interestingly, -X- _ O
models -X- _ O
follow -X- _ O
consistent -X- _ O
patterns -X- _ O
during -X- _ O
training -X- _ O
for -X- _ O
both -X- _ O
unidirectional -X- _ O
and -X- _ O
bidirectional -X- _ O
models, -X- _ O
and -X- _ O
for -X- _ O
both -X- _ O
LSTM -X- _ B-MethodName
and -X- _ O
Transformer -X- _ B-MethodName
architectures. -X- _ O

Language -X- _ O
models -X- _ O
rely -X- _ O
far -X- _ O
more -X- _ O
on -X- _ O
word -X- _ B-HyperparameterName
frequency -X- _ I-HyperparameterName
than -X- _ O
children, -X- _ B-MethodName
but, -X- _ O
like -X- _ O
children, -X- _ B-MethodName
they -X- _ O
exhibit -X- _ O
slower -X- _ O
learning -X- _ O
of -X- _ O
words -X- _ O
in -X- _ O
longer -X- _ O
utterances. -X- _ O

We -X- _ O
find -X- _ O
that -X- _ O
the -X- _ O
effects -X- _ O
of -X- _ O
concreteness, -X- _ B-HyperparameterName
word -X- _ B-HyperparameterName
length, -X- _ I-HyperparameterName
and -X- _ O
lexical -X- _ B-HyperparameterName
class -X- _ I-HyperparameterName
are -X- _ O
pointedly -X- _ O
different -X- _ O
in -X- _ O
children -X- _ B-MethodName
and -X- _ O
language -X- _ B-MethodName
models, -X- _ I-MethodName
reinforcing -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
interaction -X- _ O
and -X- _ O
sensorimotor -X- _ O
experience -X- _ O
in -X- _ O
child -X- _ O
language -X- _ O
acquisition. -X- _ O

Drawing -X- _ O
on -X- _ O
studies -X- _ O
of -X- _ O
word -X- _ B-TaskName
acquisition -X- _ I-TaskName
in -X- _ O
children, -X- _ O
we -X- _ O
evaluate -X- _ O
multiple -X- _ O
predictors -X- _ O
for -X- _ O
words’ -X- _ B-TaskName
ages -X- _ I-TaskName
of -X- _ I-TaskName
acquisition -X- _ I-TaskName
in -X- _ O
LSTMs, -X- _ B-MethodName
BERT, -X- _ B-MethodName
and -X- _ O
GPT-2. -X- _ B-MethodName

